{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to merfish3d-analysis Documentation","text":"<p>GPU accelerated post-processing for 2D or 3D iterative barcoded FISH data. This package currently Nvidia only and Linux only due to RAPIDS.AI package availabilty.</p> <p>WARNING: alpha software. We are sharing this early in case it is useful to other groups. Please expect breaking changes.</p>"},{"location":"#motivation","title":"Motivation","text":"<p>Iterative multiplexing experiments, such as MERFISH, typically involve 6D data. These dimensions are <code>[rounds,tile,channel,z,y,x]</code> and require significant processing across each dimension to go from raw data to quality controlled transcript 3D localizations and 3D cell outlines.</p> <p>Additionally, our laboratory, the Quantiative Imaging and Inference Laboratory (qi2lab), specializes in high-throughput 3D microscopy using custom microscopes. This includes purpose built high numerical aperture widefield and oblique plane microscopy platforms. While increased sampling provides more information on the sample, it introduces new challenges due to the increase in data density and more complicated MERFISH decoding inverse problem.</p> <p>To efficiently perform 3D MERFISH processing, we created this <code>merfish3d-analysis</code> package. The goal of the package is to aid researchers in rapidly and robustly turning gigabyte to petabyte level MERFISH data into decoded transcripts using chunked, compressed file formats and GPU-accelerated processing.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Decode both 2D and 3D iterative barcoded experiments that use a codebook. Our focus on 3D MERFISH, but this library can be extended to any iterative imaging and barcoded RNA imaging approach.</li> <li>Datastore optimized for large-scale imaging data.<ul> <li>Read and write compressed Zarr v2 using Tensorstore library for performance.</li> </ul> </li> <li>Processing capabilities for widefield, standard light-sheet, and skewed light-sheet data.</li> <li>Rigid, affine, and deformable local tile registration.<ul> <li>GPU-accelerated registration estimation combined with ITK for image warping.</li> </ul> </li> <li>Rigid and affine global registration using multiview-stitcher</li> <li>GPU-accelerated image processing and decoding.<ul> <li>Nearly all image processing functions utilize GPU acceleration through CuPy, CuCIM, CuVS, and custom CUDA kernels. All non-GPU accelerated functions are Numba accelerated.</li> <li>Larger-than-GPU-memory block computations are handled using Ryomen, a lightweight solution that avoids many issues with other distribution computing solutions.</li> </ul> </li> <li>Iterative estimation of background and normalization vectors across codebook bits to remove subjective normalization by user that often leads to non-optimal decoding solutions.</li> <li>Integrated functionality to leverage machine learning tools such as Cellpose, Baysor, and U-FISH.</li> </ul>"},{"location":"#examples","title":"Examples","text":"<p>Multiple examples are provided with the library, including qi2lab data, Zhuang laboratory data, and synthetic data.</p>"},{"location":"#api-reference","title":"API reference","text":"<p>For more information, check out the API Reference.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>This is an early project and we welcome all contributions! The easiest way to get started is to open an issue. We are especially interested in open-source iterative multiplexing datasets that we can setup as examples for this library.</p>"},{"location":"datastore/","title":"qi2lab DataStore overview","text":""},{"location":"datastore/#philosophy","title":"Philosophy","text":"<p>To help efficiently handle the data complexity of 3D MERFISH experiments, we have created a dedicated Zarr based datastore, using TensorStore for efficient reading and writing of image data and Parquet for tabular data. </p>"},{"location":"datastore/#important-considerations","title":"Important considerations","text":"<p>To create a <code>qi2labDataStore</code>, we need to know the following metadata:</p> <ul> <li>the effective xy pixel size and z step</li> <li>the objective numerical aperture</li> <li>the immersion media refractive index</li> <li>the global stage zyx position at each tile</li> <li>the camera orientation with respect to the stage orientation</li> <li>the direction of stage motion with respect the camera view</li> <li>the bits that were collected in each round</li> <li>the acquisition order in each tile (channel,z) or (z,channel)</li> <li>the excitation and emission wavelengths for each channel</li> </ul> <p>Most of these are straightforward to obtain. The camera orientation and stage direction can be the trickiest. In our experience, one way to figure this out is to load a few tiles of the data in napari and explore different orientations of the images and stage direction.</p> <p>Because there are so many different microscopes and microscope acquisition software, we rely on the user to provide the images in the correct orientaton such that a positive displacement in the global stage coordinates corresponds to a positive displacement in the image and vice-versa. In the Zhuang lab examples, we show how to determine the camera and stage orientations when the metadata is not available.</p>"},{"location":"datastore/#codebook-and-experiment-order-files","title":"Codebook and Experiment Order files","text":"<p>For iterative multiplexing, we need to know the codebook, which connects genes and codewords, and the experiment order, which connects rounds and bits.</p> <p>We expect these to be in <code>.csv</code> or <code>.tsv</code> format. </p> <p>For example, a 16-bit codebook <code>codebook.tsv</code> should have the following structure:</p> codeword bit01 bit02 bit03 bit04 bit05 bit06 bit07 bit08 bit09 bit10 bit11 bit12 bit13 bit14 bit15 bit16 word 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 word 2 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 word 3 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 word 4 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 -------- - - - - - - - - - - - - - - - - word N 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 <p><code>exp_order</code> should have N columns. The first column is the round, starting from <code>1</code>. The remaining columns are the readout bits in the codebook, in order of acquisition. Important: we assume that each tile has a fiducial channel. If there is not, this software package will not work for your experiment.</p> <p>For a 16-bit codebook, where we acquire the bits in sequential order within rounds and across rounds, the <code>exp_order.tsv</code> will look like:</p> round readout 1 readout 2 1 1 2 2 3 4 3 5 6 4 7 8 5 9 10 6 11 12 7 13 14 8 15 16"},{"location":"datastore/#general-use","title":"General use","text":"<p>Here, we use a hypothetical dataset that only has one round with two bits. We assume the data is already gain, offset, and hot pixel corrected.</p> <p>Tiles and rounds are indexed from 0, keeping with python conventions. However, we always index rounds and codebook bits from 1, to avoid confusion.</p> <pre><code>from merfish3danalysis.qi2labDataStore import qi2labDataStore\nfrom pathlib import Path\n\n# define the datastore directory and create the datastore\nroot_path = Path(r\"/path/to/dataset/\")\ndatastore = qi2labDataStore(root_path / Path(\"qi2labdatastore\"))\n\n# required metadata\ndatastore.channels_in_data = [\"alexa488\",\"alexa561\",\"alexa647\"]\ndatastore.baysor_path = Path(r\"/path/to/Baysor/bin/baysor/bin/./baysor\")\ndatastore.baysor_options = Path(r\"/path/to/baysor_options.toml\")\ndatastore.julia_threads = 8\ndatastore.num_rounds = 1\ndatastore.codebook = Path(r\"/path/to/dataset/raw_data/codebook.csv\")\ndatastore.experiment_order = Path(r\"/path/to/dataset/raw_data/exp_order.csv\")\ndatastore.num_tiles = 1\ndatastore.microscope_type = \"3D\"\ndatastore.tile_overlap = 0.2\ndatastore.e_per_ADU = 0.51\ndatastore.na = 1.35\ndatastore.ri = 1.51\ndatastore.binning = 1\ndatastore.noise_map = None\ndatastore.channel_psfs = channel_psfs # either experimental or theoretical PSFs\ndatastore.voxel_size_zyx_um = [.31,.098,.098]\n\n# Update datastore state that Calibration are created\ndatastore_state = datastore.datastore_state\ndatastore_state.update({\"Calibrations\": True})\ndatastore.datastore_state = datastore_state\n\n# initialize the tile\ndatastore.initialize_tile(tile_idx)\n\n# code to read image tile here\n# Assume the images are of shape [n_channels,nz,nx,ny]\npolyDT_data = imread(\"/path/to/dataset/raw_data/tile001/image.tif\")[0,:]\n\n# save image data for tile = 0, round = 0, polyDT\ndatastore.save_local_corrected_image(\n    polyDT_data,\n    tile=0,\n    psf_idx=0,\n    gain_correction=True,\n    hotpixel_correction=True,\n    shading_correction=False,\n    round=0,\n)\n\n# save stage position for tile = 0, round = 0, polyDT\ndatastore.save_local_stage_position_zyx_um(\n    [1000., 200., 500.], tile=0, round=0\n)\n\n# save excitation and emission wavelengths for tile = 0, round = 0, polyDT\n# this position is used for any bits linked to this round\ndatastore.save_local_wavelengths_um(\n    (.488, .520),\n    tile=0,\n    round=0,\n)\n\n# code to read image tile here\n# Assume the images are of shape [n_channels,nz,nx,ny]\nbit001_data = imread(\"/path/to/dataset/raw_data/tile001/image.tif\")[1,:]\n\n# save first readout channel for tile = 0, bit_idx = 1\ndatastore.save_local_corrected_image(\n    bit001_data,\n    tile=0,\n    psf_idx=1,\n    gain_correction=True,\n    hotpixel_correction=True,\n    shading_correction=False,\n    bit=1,\n)\n\n# save excitation and emission wavelengths for tile = 0, bit_idx = 1\ndatastore.save_local_wavelengths_um(\n    (.561, .590),\n    tile=0,\n    bit=1,\n)\n\n# code to read image tile here\n# Assume the images are of shape [n_channels,nz,nx,ny]\nbit002_data = imread(\"/path/to/dataset/raw_data/tile001/image.tif\")[2,:]\n\n# save second readout channel for tile = 0, bit_idx = 2\ndatastore.save_local_corrected_image(\n    bit002_data,\n    tile=0,\n    psf_idx=2,\n    gain_correction=True,\n    hotpixel_correction=True,\n    shading_correction=False,\n    bit=2,\n)\n\n# save excitation and emission wavelengths for tile = 0, bit_idx = 2\ndatastore.save_local_wavelengths_um(\n    (.635, .670),\n    tile=0,\n    bit=2,\n)\n\n# update datastore state that corrected data is saved \ndatastore_state = datastore.datastore_state\ndatastore_state.update({\"Corrected\": True})\ndatastore.datastore_state = datastore_state\n</code></pre>"},{"location":"datastore/#datastore-structure","title":"DataStore structure","text":"<pre><code>/experiment \n\u251c\u2500\u2500 raw_data/ \n  \u2514\u2500\u2500 &lt;data&gt; (raw experimental data and metadata)\n\u251c\u2500\u2500 qi2labdatastore/ \n    \u251c\u2500\u2500 datastore.json (information on the state of the datastore)\n    \u251c\u2500\u2500 calibrations.zarr/ (calibration information)\n    \u251c\u2500\u2500 .zattrs\n      \u251c\u2500\u2500 &lt;exp_codebook&gt;\n      \u251c\u2500\u2500 &lt;exp_order&gt;\n      \u2514\u2500\u2500 &lt;exp_codebook&gt;\n    \u251c\u2500\u2500 camera_noise_map/ (camera noise map array)\n    \u2514\u2500\u2500 psf_data/ (psf arrays)\n  \u251c\u2500\u2500 polyDT/ (raw and processed data for polyDT label)\n    \u251c\u2500\u2500 tile0000/\n      \u251c\u2500\u2500 round0000.zarr/\n        \u251c\u2500\u2500 .zattrs\n          \u251c\u2500\u2500 &lt;stage_zyx_um&gt; (global stage position in zyx order; unit: microns)\n          \u251c\u2500\u2500 &lt;wavelengths_um&gt; (wavelength in (excitation,emission) order; unit: microns)\n          \u251c\u2500\u2500 &lt;voxel_size_zyx_um&gt; (voxel size in zyx order; unit: microns)\n          \u251c\u2500\u2500 &lt;bit_linker&gt; (what codebook bits are linked to this fidicual image)\n          \u251c\u2500\u2500 &lt;affine_zyx_um&gt; (4x4 affine matrix generated during global registration; unit: microns)\n          \u251c\u2500\u2500 &lt;origin_zyx_um&gt; (tile origin generated during global registration; unit: microns)\n          \u2514\u2500\u2500 &lt;spacing_zyx_um&gt; (voxel size used during global registration, this must match &lt;voxel_size_zyx_um&gt;; unit: microns)\n        \u251c\u2500\u2500 camera_data/ (gain and offset corrected data in zyx order)\n        \u251c\u2500\u2500 corrected_data/ (gain and offset corrected data in zyx order)\n        \u2514\u2500\u2500 registered_decon_data/ (deconvolved data in zyx order)\n      \u251c\u2500\u2500 round0001.zarr\n        \u251c\u2500\u2500 .zattrs\n          \u251c\u2500\u2500 &lt;stage_zyx_um&gt; (global stage position in zyx order; unit: microns)\n          \u251c\u2500\u2500 &lt;wavelengths_um&gt; (wavelength in (excitation,emission) order; unit: microns)\n          \u251c\u2500\u2500 &lt;voxel_size_zyx_um&gt; (voxel size in zyx order; unit: microns)\n          \u251c\u2500\u2500 &lt;bit_linker&gt; (what codebook bits are linked to this fidicual image)\n          \u251c\u2500\u2500 &lt;affine_zyx_um&gt; (4x4 affine matrix generated during global registration; unit: microns)\n          \u251c\u2500\u2500 &lt;origin_zyx_um&gt; (tile origin generated during global registration; unit: microns)\n          \u2514\u2500\u2500 &lt;spacing_zyx_um&gt; (voxel size used during global registration, this must match &lt;voxel_size_zyx_um&gt;; unit: microns)\n        \u251c\u2500\u2500 camera_data/ (gain and offset corrected data in zyx order)\n        \u251c\u2500\u2500 corrected_data/ (gain and offset corrected data in zyx order)\n        \u251c\u2500\u2500 of_xyz_3x_downsample/ (3x downsampled optical flow field for round 0 alginment in pixels)\n        \u251c\u2500\u2500 registered_decon_data/ (deconvolved, registered back to round 0 image data in zyx order)\n        \u251c\u2500\u2500 ... \n        \u2514\u2500\u2500 roundNNNN.zarr/\n    \u251c\u2500\u2500 tile0001/\n    \u251c\u2500\u2500 tile0002/\n    \u251c\u2500\u2500 ...\n    \u2514\u2500\u2500 tileNNNN/\n  \u251c\u2500\u2500 readouts/ (raw and processed data for MERFISH bits)\n    \u251c\u2500\u2500 tile0001/\n      \u251c\u2500\u2500 bit000.zarr/\n        \u251c\u2500\u2500 .zattrs\n          \u251c\u2500\u2500 &lt;stage_zyx_um&gt; (global stage position in zyx order; unit: microns)\n          \u251c\u2500\u2500 &lt;wavelengths_um&gt; (wavelength in (excitation,emission) order; unit: microns)\n          \u251c\u2500\u2500 &lt;voxel_size_zyx_um&gt; (voxel size in zyx order; unit: microns)\n          \u2514\u2500\u2500 &lt;round_linker&gt; (what fidicual round is linked to this bit image) \n        \u251c\u2500\u2500 camera_data/\n        \u251c\u2500\u2500 corrected_data/\n        \u251c\u2500\u2500 registered_decon_data/\n        \u2514\u2500\u2500 registered_ufish_data/\n      \u251c\u2500\u2500 bit001.zarr/\n      \u251c\u2500\u2500 ...\n      \u2514\u2500\u2500 bitNNN.zarr/\n</code></pre>"},{"location":"datastore/#datastore-api","title":"DataStore API","text":"<p>Nearly all parameters are accessible as class properties and all data has helper functions for reading and writing. The full API reference is available at qi2labDataStore.</p>"},{"location":"installation/","title":"Install","text":"<p>Create a python 3.12 environment using your favorite package manager, e.g. <pre><code>conda create -n merfish3d python=3.12\n</code></pre></p> <p>Activate the environment and install the GPU dependencies. This install method assumes an Nvidia GPU capable of running CUDA 12.8. <pre><code>conda activate merfish3d\n</code></pre></p> <p>Next, clone the repository in your location of choice and enter the directory using <pre><code>git clone https://github.com/QI2lab/merfish3d-analysis\ncd merfish3d-analysis\n</code></pre></p> <p>and install using  <pre><code>pip install .\n</code></pre></p> <p>For interactive editing use  <pre><code>pip install -e .\n</code></pre></p> <p>Finally, install the <code>merfish3d-analysis</code> package using the command  ``` setup-merfish3d ```` </p> <p>This will automatically setup the correct CUDA libraries and other packages in the conda environmnent. Note: Due to package incompatability, the install script currently creates a second conda/mamba environment called <code>merfish3d-stitcher</code>. In this environment, we install the minimal packages requried to read the datastore used by <code>merfish3d-analysis</code> and multiview-stitcher. The reason for this change is that one of the <code>multiview-stitcher</code> sub-dependencies (<code>xarray-dataclass</code>) now requires <code>numpy&gt;2.0</code>, which is incompatible with the scientific computing packages used for <code>merfish3d-analysis</code>.</p> <p>The <code>merfish3d-stitcher</code> environment is only used when individual tiles are registered into a global coordinate system. The code automatically invokes this second environment, but it is important to note that the current install strategy does create a new conda/mamba environment beyond what you as the user creates. As soon as the dependency issue is solved, we will remove this work around.</p>"},{"location":"installation/#optional-installing-baysor","title":"(Optional) Installing Baysor","text":"<p>Please follow the Baysor documentation to install for Linux. Keep track of the installation directory for use with <code>merfish3d-analysis</code>.</p>"},{"location":"installation/#documentation","title":"Documentation","text":"<p>To build the documentation, install using <code>pip install .[docs]</code>. Then execute <code>mkdocs build --clean</code> and <code>mkdocs serve</code>. The documentation is available in your web browser at <code>http://127.0.0.1:8000/</code>.</p>"},{"location":"workflow/","title":"Full Experiment Analysis Workflow","text":""},{"location":"workflow/#initialize-datastore","title":"Initialize datastore","text":"<pre><code>%%{init: { \"theme\": \"default\", \"themeVariables\": { \"htmlLabels\": true, \"curve\": \"linear\", \"layout\": \"elk\" } } }%%\nflowchart TD\n subgraph s1[\"Define datastore\"]\n        n1[\"Number of tiles\"]\n        n2[\"Microscope metadata\"]\n        n3[\"Experiment metadata\"]\n        n4[\"qi2labdatastore\"]\n  end\n    n1 --&gt; n4\n    n2 --&gt; n4\n    n3 --&gt; n4\n\n    n1@{ shape: notch-rect}\n    n2@{ shape: notch-rect}\n    n3@{ shape: notch-rect}\n    n4@{ shape: lin-cyl}\n</code></pre>"},{"location":"workflow/#populate-datastore","title":"Populate datastore","text":"<pre><code>%%{init: { \"theme\": \"default\", \"themeVariables\": { \"htmlLabels\": true, \"curve\": \"linear\", \"layout\": \"elk\" } } }%%\nflowchart TD\n subgraph s1[\"Place raw data into datastore\"]\n        n5[\"Raw data\"]\n        n12[\"Camera correction\"]\n        n6[\"Geometric transformation\n        (local to global)\"]\n        n7[\"Experiment order\"]\n        n8[\"Fidicual data\"]\n        n9[\"MERFISH data\"]\n        n10[\"Global tile positions\"]\n        n11[\"qi2labdatastore\"]\n end\n    n5 --&gt; n12\n    n12 --&gt; n6\n    n6 --&gt; n7\n    n7 --&gt; n8\n    n7 --&gt; n9\n    n8 --&gt; n11\n    n9 --&gt; n11\n    n10 --&gt; n11\n\n    n5@{ shape: procs}\n    n6@{ shape: notch-rect}\n    n7@{ shape: notch-rect}\n    n8@{ shape: procs}\n    n9@{ shape: procs}\n    n10@{ shape: procs}\n    n11@{ shape: lin-cyl}\n    n12@{ shape: notch-rect}\n</code></pre>"},{"location":"workflow/#deconvolve-locally-register-and-spot-predict-all-tiles","title":"Deconvolve, locally register, and spot predict all tiles","text":"<pre><code>%%{init: { \"theme\": \"default\", \"themeVariables\": { \"htmlLabels\": true, \"curve\": \"linear\", \"layout\": \"elk\" } } }%%\nflowchart TD\n subgraph s2[\"MERFISH preprocessing\"]\n        n17[\"Deconvolution\"]\n        n18[\"U-FISH prediction\"]\n        n19[\"Tile warping\"]\n  end\n subgraph s3[\"Fiducial preprocessing\"]\n        n14[\"Deconvolution\"]\n        n15[\"Rigid registration\"]\n        n16[\"Deformable registration\"]\n  end\n subgraph s1[\"Local preprocessing\"]\n        s2\n        s3\n        n1[\"Local tile registrations back to round 1\"]\n  end\n  n13[\"qi2labdatastore\"]\n\n    s3 --&gt; n1\n    n1 --&gt; s2\n    s1 &lt;--&gt; n13\n    n14 --&gt; n15\n    n15 --&gt; n16\n    n17 --&gt; n18\n    n18 --&gt; n19\n    n1@{ shape: notch-rect}\n    n13@{ shape: lin-cyl}\n    n14@{ shape: procs}\n    n15@{ shape: procs}\n    n16@{ shape: procs}\n    n17@{ shape: procs}\n    n18@{ shape: procs}\n    n19@{ shape: procs}\n</code></pre>"},{"location":"workflow/#global-registration-and-fusion-of-first-fiducial-round","title":"Global registration and fusion of first fiducial round","text":"<pre><code>%%{init: { \"theme\": \"default\", \"themeVariables\": { \"htmlLabels\": true, \"curve\": \"linear\", \"layout\": \"elk\" } } }%%\nflowchart TD\n subgraph s1[\"Global registration\"]\n    n3[\"Fiducial data\"]\n    n4[\"Multiview-Stitcher\"]\n end\n n1[\"qi2labdatastore\"]\n n2[\"XY downsampled, Z max projected, and fused global fiducial image\"]\n n5[\"Cellpose\"]\n n6[\"2D cell segmentations\"]\n n7[\"Optimized global tile positions\"]\n n8[\"User optimized parameters\"]\n\n    s1 &lt;--&gt; n7\n    n7 &lt;--&gt; n1\n    s1 --&gt; n2\n    n3 &lt;--&gt; n4\n    n2 --&gt; n5\n    n5 --&gt; n6\n    n6 --&gt; n1\n    n8 --&gt; n5\n    n1@{ shape: lin-cyl}\n    n2@{ shape: procs}\n    n3@{ shape: procs}\n    n6@{ shape: procs}\n    n7@{ shape: notch-rect}\n    n8@{ shape: notch-rect}\n</code></pre>"},{"location":"workflow/#pixel-decoding","title":"Pixel decoding","text":"<pre><code>%%{init: { \"theme\": \"default\", \"themeVariables\": { \"htmlLabels\": true, \"curve\": \"linear\", \"layout\": \"elk\" } } }%%\nflowchart TD\n subgraph s1[\"Pixel decoding\"]\n    n2[\"MERFISH data\"]\n    n3[\"Global normalization estimate\"]\n    n4[\"Iterative normalization estimate\"]\n    n5[\"Pixel decoding\"]\n    n6[\"False-positive filtering\"]\n    n7[\"Overlap cleanup\"]\n    n8[\"Cell assignment\"]\n    n9[\"Data prep for resegmentation\"]\n end\n n1[\"qi2labdatastore\"]\n\n    s1 &lt;--&gt; n1\n    n2 --&gt; n3\n    n3 --&gt; n4\n    n4 --&gt; n5\n    n5 --&gt; n6\n    n6 --&gt; n7\n    n7 --&gt; n8\n    n8 --&gt; n9\n\n    n1@{ shape: lin-cyl}\n    n2@{ shape: procs}\n    n3@{ shape: procs}\n    n4@{ shape: procs}\n    n5@{ shape: procs}\n    n6@{ shape: procs}\n    n7@{ shape: procs}\n    n8@{ shape: procs}\n    n9@{ shape: procs}\n</code></pre>"},{"location":"workflow/#3d-segmentation-based-on-decoded-rna","title":"3D segmentation based on decoded RNA","text":"<pre><code>%%{init: { \"theme\": \"default\", \"themeVariables\": { \"htmlLabels\": true, \"curve\": \"linear\", \"layout\": \"elk\" } } }%%\nflowchart TD\n subgraph s1[\"3D segmentation\"]\n    n3[\"Decoded, cell-assigned RNA\"]\n    n4[\"Baysor\"]\n    n6[\"User optimized parameters\"]\n end\n n1[\"qi2labdatastore\"]\n n2[\"3D segmentations\"]\n n5[\"Updated RNA assignments\"]\n\n    s1 &lt;--&gt; n1\n    n3 --&gt; n4\n    n6 --&gt; n4\n    s1 --&gt; n2\n    n2 --&gt; n5\n    n5 --&gt; n1\n    n1@{ shape: lin-cyl}\n    n2@{ shape: procs}\n    n3@{ shape: procs}\n    n5@{ shape: procs}\n    n6@{ shape: notch-rect}\n</code></pre>"},{"location":"examples/_zhuang_lab_mouse_brain/","title":"Zhuang laboratory mouse brain example","text":""},{"location":"examples/_zhuang_lab_mouse_brain/#overview","title":"Overview","text":"<p>The goal of this example is to run <code>merfish3d-analysis</code> on an existing 2D MERFISH dataset generated by the Zhuang laboratory at Harvard. They graciously deposisted their nearly raw data on BIL and we can adapt the fully featured functionality of our package to re-process their data. This is a 22-bit MERFISH experiment that has been pre-registered across rounds within each tile.</p>"},{"location":"examples/_zhuang_lab_mouse_brain/#preliminaries","title":"Preliminaries","text":"<p>You need to make sure you have a working python environment with <code>merfish3d-analysis</code> properly installed, <code>Baysor</code> properly installed, and the Zhuang laboratory mouse brain 2D MERFISH dataset downloaded. The dataset is ~760 GB and you will need roughly another 700 GB of temporary space to create the <code>qi2labDataStore</code> structure we use to perform tile registration, global registration, segmentation, pixel decoding, filtering, and cell assignment. </p> <p>We are only going to analyze one of their samples, specifically <code>mouse1_sample1_raw</code>.</p> <ul> <li>merfish3d-analysis</li> <li>Baysor</li> <li>Raw 2D MERFISH data. Download the <code>additional_files/</code> folder, <code>mouse1_sample1_raw/</code> folder, and <code>processed_data/spots_mouse1sample1.csv</code>.</li> </ul>"},{"location":"examples/_zhuang_lab_mouse_brain/#downloading-the-data","title":"Downloading the data","text":"<p>All of the required code to process this data is in the examples/zhuang_lab folder on <code>merfish3d-analysis</code> github repo. After downloading the data from BIL, there should be the following data structure on the disk:</p> <pre><code>/mop\n  \u251c\u2500\u2500 mouse1_sample1_raw/\n    \u251c\u2500\u2500 aligned_images1.tif\n    \u251c\u2500\u2500 aligned_images2.tif\n    \u251c\u2500\u2500 ...\n    \u2514\u2500\u2500 aligned_images447.tif\n  \u251c\u2500\u2500 additional_files/\n    \u251c\u2500\u2500 codebook.csv\n    \u251c\u2500\u2500 data_organization_raw.csv\n    \u251c\u2500\u2500 microscope.json\n    \u251c\u2500\u2500 fov_positions/\n      \u2514\u2500\u2500 mouse1_sample1.txt\n  \u2514\u2500\u2500 processed_data/\n    \u2514\u2500\u2500 spots_mouse1sample1.csv\n</code></pre>"},{"location":"examples/_zhuang_lab_mouse_brain/#processing-non-qi2lab-data","title":"Processing non-qi2lab data","text":"<p>Because this data is generated by a different custom microscope design with a unique microscope control package and is already pre-processed, we have to manually write most of the data conversion steps. Please see the DataStore page for more information on the key parameters that are requred to create a <code>qi2labDataStore</code>.</p> <p>The data in this BIL dataset consists of six small section of mouse brain, all separated on the slide. Looking at the <code>additional_files/fov_positions/mouse1_sample1.txt</code> file, we can find where there are discontinuous jumps in the stage positions. Those jumps indicate the stage moved to a new brain section, for example lines 58-61 in the file show,</p> <pre><code>58 2404.87, 1766.11\n59 2204.87, 1366.11\n60 -309.64, 2741.84\n61 -309.64, 2541.84\n</code></pre> <p>So we know tiles 1 through 59 are the first continuous tissue slice area.</p> <p>A key issue with this data is the stage direction and camera orientation are flipped, which can be quite confusing when trying to figure out how the tiles are spatially related to each other. The first step is to load the first two tiles with the known spatial transformations by setting line 167 in <code>00a_test_image_orientation.py</code> to <code>n_tiles=2</code>. This will generate two plots, showing the result if we interpret the positions in the <code>additional_files/fov_positions/mouse1_sample1.txt</code> file as <code>yx</code> or <code>xy</code> order.</p> <p>From the overlap area, it appears that the <code>xy</code> order interpreation (on the right) is correct. To check more thoroughly, we can set line 167 in <code>00a_test_image_orientation.py</code> to <code>n_tiles=59</code>. This will plot the entire first tissue slice.</p> <p>The check with the first tissue slice on the slide confirms that we need to interpret the stage positions in <code>xy</code> order, while we know Python intreprets the data in the tif files as <code>yx</code> order. To account for this and correctly register the data, we have to swap the xy axes. This can be accomplished by swapping the last two axes of the tif,</p> <pre><code>flipped_image = np.swapaxes(raw_image,axes=(-2,-1))\n</code></pre> <p>or by swapping the <code>xy</code> positions in the array generated from <code>additional_files/fov_positions/mouse1_sample1.txt</code>,</p> <pre><code>stage_positions_um = np.swapaxes(stage_positions_um,axes=(-2,-1))\n</code></pre> <p>Another difference in this data is that the spacing between z-planes is 1.5 microns, quite a bit larger than the Nyquist sampling of ~0.3 microns given the objective NA. It does not make sense to perform 3D deconvolution or decoding for this large of a sampling, so each z plane is deconvolved and decoded as independent from the surrounding planes. At the end, we collapse decoded transcripts that show up in adajacent Z planes to the transcript with the largest brightness.</p> <p>Finally, much of the metadata information we need (refractive index, numerical aperture, wavelengths, camera specifications, etc...) is only easily available via the publication, Spatially resolved cell atlas of the mouse primary motor cortex by MERFISH. We have noted in the conversion script where we had to look up these values or find values from equipment suppliers.</p> <p>While we have already created the data conversion code for this example, please reach out with questions if the process is not clear.</p>"},{"location":"examples/_zhuang_lab_mouse_brain/#processing-steps","title":"Processing steps","text":"<p>For each of the python files in the <code>examples/zhuang_lab</code> directory, you will need to scroll to the bottom and replace the path with the correct path. For example, in <code>01_convert_to_datastore.py</code> you'll want to change this section:</p> <pre><code>if __name__ == \"__main__\":\n    root_path = Path(r\"/path/to/download/mop\")\n    baysor_binary_path = Path(\n        r\"/path/to/Baysor/bin/baysor/bin/./baysor\"\n    )\n    baysor_options_path = Path(\n        r\"/path/to/merfish3d-analysis/examples/zhuang_lab/zhuang_mouse.toml\"\n    )\n    julia_threads = 20 # replace with number of threads to use.\n\n    hot_pixel_image_path = None\n\n    convert_data(\n        root_path=root_path,\n        baysor_binary_path=baysor_binary_path,\n        baysor_options_path=baysor_options_path,\n        julia_threads=julia_threads,\n        hot_pixel_image_path=hot_pixel_image_path\n    )\n</code></pre> <p>For all of the files in <code>processing_code</code>, you'll set the <code>root_path</code> to <code>root_path = Path(r\"/path/to/download/mop\")</code>. The package automatically places the datastore within that directory, <code>/path/to/download/mop/qi2labdatastore</code>.</p> <p>Once that is done, you can run <code>01_convert_to_datastore.py</code> and <code>02_register_and_deconvolve.py</code> without any interactions. Depending on your computing hardware, you should expect ~12 hours for <code>01_convert_to_datastore.py</code> and about a week for <code>02_register_and_deconvolve.py</code>. The second file is extremely compute intensive, because performs 2D deconvolution each z-plane of the 3 channel z-stacks at each tile, registers the polyDT channel back to the first round, runs U-FISH for the MERFISH bits in each tile, and finally performs global registration for all polyDT tiles in the first round. On a 12-core/24-thread AMD CPU workstation with 128 GB RAM and an Nvidia RTX 3090 24 GB GPU this step takes about 8 days to complete for all 441 tiles of shape <code>[40,7,2048,2408]</code> in the dataset.</p> <p>Once <code>02_register_and_deconvolve.py</code> is finished, you will need to create the correct cellpose settings. We have found that initially peforming cellpose segmentation on a downsampled and maximum Z projected polyDT image is sufficient to seed Baysor for segmentation refinement.</p> <p>If the standard \"cyto3\" model does not work for you data, you may need to retrain the cellpose model and pass that to the code. Please see the API documentation for how to perform that step. Given satisfactory cellpose settings, you fill them in at the bottom of <code>03_cell_segmentation.py</code>,</p> <pre><code>if __name__ == \"__main__\":\n    root_path = Path(r\"/path/to/download/mop\")\n    cellpose_parameters = {\n        'normalization' : [0.5,95.0],\n        'blur_kernel_size' : 2.0,\n        'flow_threshold' : 0.4,\n        'cellprob_threshold' : 0.0,\n        'diameter': 36.57\n    }\n    run_cellpose(root_path, cellpose_parameters)\n</code></pre> <p>and then run <code>03_cell_segmentation.py</code>. This will only take a an hour or so to generate the initial 2D segmentation guess. Our package rewrites the cell outlines using the ImageJ ROI file structure in global coordinates.</p> <p>Next, you'll run <code>04_pixeldecode_and_baysor.py</code> to first optimize the pixel-based decoding parameters on a subset of tiles, then perform pixel-based decoding for all tiles, filter the data to limit false positives, remove overlapping spots in adajacent spatial tiles, and finally re-segment the cell boundaries in 3D using Baysor. This step should take ~0.5-1 week, depending on your hard disk and GPU configuration.</p>"},{"location":"examples/qi2lab_human_olfactory_bulb/","title":"qi2lab human olfactory bulb example","text":""},{"location":"examples/qi2lab_human_olfactory_bulb/#overview","title":"Overview","text":"<p>The goal of this example is to retrieve a 3D MERFISH experiment generated by the qi2lab at ASU from the cloud and run merfish3d-analysis through quantifying transcripts and assigning transcripts to cells. This is a 119-gene MERFISH and 2-gene smFISH experiment on post-mortem human olfactory bulb tissue.</p>"},{"location":"examples/qi2lab_human_olfactory_bulb/#preliminaries","title":"Preliminaries","text":"<p>You need to make sure you have a working python enviornment with <code>merfish3d-analysis</code> properly installed, <code>Baysor</code> properly installed, and our human olfactory bulb 3D MERFISH dataset downloaded. The dataset is ~.7 TB and you will need roughly another ~0.5 TB of space to create the <code>qi2labDataStore</code> structure we use to perform tile registration, global registration, segmentation, pixel decoding, filtering, and cell assignment.</p> <ul> <li>merfish3d-analysis = Raw 3D Human olfactory bulb MERFISH data</li> <li>Baysor</li> </ul>"},{"location":"examples/qi2lab_human_olfactory_bulb/#downloaded-data","title":"Downloaded data","text":"<p>In the downloaded data, the directory structure should be as follows:</p> <pre><code>/path/to/download/ \n\u251c\u2500\u2500 raw_data/ \n  \u251c\u2500\u2500 data_r0001_tile000_xyz/\n  \u251c\u2500\u2500 data_r0001_tile000_xyz.csv\n  ...\n  ...\n  \u251c\u2500\u2500 data_r0009_tile041_xyz/\n  \u251c\u2500\u2500 data_r0009_tile041_xyz.csv\n  \u251c\u2500\u2500 codebook.csv\n  \u251c\u2500\u2500 bit_order.csv\n  \u251c\u2500\u2500 hot_pixel_flir.tiff\n  \u2514\u2500\u2500 scan_metadata.csv\n</code></pre>"},{"location":"examples/qi2lab_human_olfactory_bulb/#processing-steps","title":"Processing steps","text":"<p>All of the required code to process this data is contained within the <code>merfish3d-analysis</code> package through command line interfaces (CLI).  The package will automatically utilize multiple GPUs if the <code>--num-gpu</code> parameter is passed in with the number of available GPUs. The default value for <code>--num-gpu</code> is <code>1</code> and the package will use the first GPU that it finds. Because our workstation has two Nvidia 3090 RTXs GPUs, we provide the commands used to process the human olfactory bulb data in the preprint.  </p> <p>All of the commands given here are run from the terminal.  </p> <p>Each command list below can be queried using <code>--help</code> to discover all of the available parameters, if the defaults are not correct for your dataset. For example, <pre><code>conda run --live-stream -n merfished qi2lab-decode --help\n</code></pre></p>"},{"location":"examples/qi2lab_human_olfactory_bulb/#datastore-creation-including-hot-pixel-correction-camera-adu-to-photoelectron-conversion-illumination-estimation-and-flatfield-correction","title":"Datastore creation including hot pixel correction, camera ADU to photoelectron conversion, illumination estimation, and flatfield correction","text":"<pre><code>conda run -n merfish3d --live-stream bash -lc \"qi2lab-datastore /path/to/data --num-gpus 2 --hot-pixel-image hot_pixel_flir.tiff\"\n</code></pre>"},{"location":"examples/qi2lab_human_olfactory_bulb/#pre-processing-including-deconvolution-drift-correction-and-neural-network-prediction-of-spots","title":"Pre-processing including deconvolution, drift correction, and neural network prediction of spots","text":"<pre><code>conda run -n merfish3d --live-stream bash -lc \"qi2lab-preprocess /path/to/data --num-gpus 2\"\n</code></pre>"},{"location":"examples/qi2lab_human_olfactory_bulb/#global-registration-and-first-round-poly-dt-tile-fusion","title":"Global registration and first round poly-dT tile fusion","text":"<pre><code>conda run -n merfish3d-stitcher --live-stream bash -lc \"qi2lab-globalregister /path/to/data\"\n</code></pre>"},{"location":"examples/qi2lab_human_olfactory_bulb/#poly-dt-cell-segmentation","title":"poly-dT cell segmentation","text":"<p>The values for Cellpose-SAM need to be pre-determined using the Cellpose GUI.</p> <pre><code>conda run -n merfish3d --live-stream bash -lc \"qi2lab-segment /path/to/data --diameter 90 --normalization-low 0.5 --normalization-high 99.0 --cellprobthreshold \"\n</code></pre>"},{"location":"examples/qi2lab_human_olfactory_bulb/#pixel-decoding-including-fdr-filtering","title":"Pixel decoding including FDR filtering","text":"<p>Because there are 2 smFISH bits at the end of the codebook, we instruct the <code>merfish3d-analysis</code> pixel decoder to process only the initial 16 bits of the codebook.</p> <pre><code>conda run -n merfish3d --live-stream bash -lc \"qi2lab-decode /path/to/data --perform-baysor True --num-gpus 2 --merfish-bits 16\"\n</code></pre>"},{"location":"examples/statphysbio_synthetic/","title":"Synthetic data example","text":""},{"location":"examples/statphysbio_synthetic/#overview","title":"Overview","text":"<p>The goal of this example is to retrieve and analyze simulated iterative RNA-FISH experiments generated by the statphysbio lab at ASU using <code>merfish3d-analysis</code>.</p>"},{"location":"examples/statphysbio_synthetic/#try-without-installing-google-colab-notebook","title":"Try without installing: Google Colab Notebook","text":"<p>If you would like to try out <code>merfish3d-analysis</code> without installing, there is an example notebook that can be run on Google Colab here.</p>"},{"location":"examples/statphysbio_synthetic/#preliminaries","title":"Preliminaries","text":"<p>You need to make sure you have a working python enviornment with <code>merfish3d-analysis</code> properly installed and the synthetic dataset downloaded. The dataset is ~1 GB and you will need roughly another 1 GB of temporary space to create the <code>qi2labDataStore</code> structure we use to perform tile registration, pixel decoding, filtering, and cell assignment.</p> <ul> <li>merfish3d-analysis</li> <li>simulation data</li> </ul>"},{"location":"examples/statphysbio_synthetic/#downloading-the-data","title":"Downloading the data","text":"<p>The Zenodo link contains four types of simulations: (1) MERFISH in cells; (2) MERFISH randomly distributed; (3) smFISH in cells; (4) smFISH randomly distributed. For each simulation type, there are three axial spacings: 0.315, 1.0, and 1.5 micrometers. The directory structure is as follows:  </p> <pre><code>/path/to/download/\n\u251c\u2500\u2500 example_16bit_cells/\n  \u251c\u2500\u2500 0.315/\n    \u251c\u2500\u2500 beads\n        \u251c\u2500\u2500 codebook.csv\n        \u2514\u2500\u2500 experiment_and_GT.h5\n    \u251c\u2500\u2500 aligned_1.tif\n    \u251c\u2500\u2500 bit_order.csv\n    \u251c\u2500\u2500 codebook.csv\n    \u251c\u2500\u2500 GT_spots.csv\n    \u251c\u2500\u2500 experiment_and_GT.h5\n    \u251c\u2500\u2500 norm_offset.json\n    \u2514\u2500\u2500 scan_metadata.csv\n  \u251c\u2500\u2500 1.0/\n    \u251c\u2500\u2500 beads/\n        \u251c\u2500\u2500 codebook.csv\n        \u2514\u2500\u2500 experiment_and_GT.h5\n    \u251c\u2500\u2500 aligned_1.tif\n    \u251c\u2500\u2500 bit_order.csv\n    \u251c\u2500\u2500 codebook.csv\n    \u251c\u2500\u2500 GT_spots.csv\n    \u251c\u2500\u2500 experiment_and_GT.h5\n    \u251c\u2500\u2500 norm_offset.json\n    \u2514\u2500\u2500 scan_metadata.csv\n  \u251c\u2500\u2500 1.5/\n    \u251c\u2500\u2500 beads/\n        \u251c\u2500\u2500 codebook.csv\n        \u2514\u2500\u2500 experiment_and_GT.h5\n    \u251c\u2500\u2500 aligned_1.tif\n    \u251c\u2500\u2500 bit_order.csv\n    \u251c\u2500\u2500 codebook.csv\n    \u251c\u2500\u2500 GT_spots.csv\n    \u251c\u2500\u2500 experiment_and_GT.h5\n    \u251c\u2500\u2500 norm_offset.json\n    \u2514\u2500\u2500 scan_metadata.csv\n\u251c\u2500\u2500 example_16bit_uniform/\n...\n</code></pre>"},{"location":"examples/statphysbio_synthetic/#processing-non-qi2lab-data","title":"Processing non-qi2lab data","text":"<p>Because this is a simulated experiment, it does not follow the standard metadata or file structure of the microscope file format we internally use in qi2lab. Therefore, we first convert the simulation to the qi2lab experiment format and proceed from there.</p>"},{"location":"examples/statphysbio_synthetic/#processing-steps","title":"Processing steps","text":"<p>We provide a command line interface (CLI) to run the simulation analysis. This consists of a series of commands for each processing step.</p> <ol> <li>Simulation conversion <pre><code>conda run -n merfish3d --live-stream bash -lc \"sim-convert /path/to/simulation/example_16bit_cells/0.315\"\n</code></pre></li> <li>Simulation to qi2lab datastore conversion <pre><code>conda run -n merfish3d --live-stream bash -lc \"sim-datastore /path/to/simulation/example_16bit_cells/0.315/sim_acquisition\"\n</code></pre></li> <li>Data pre-processing <pre><code>conda run -n merfish3d --live-stream bash -lc \"sim-preprocess /path/to/simulation/example_16bit_cells/0.315/sim_acquisition\"\n</code></pre></li> <li>Pixel decoding and RNA calling <pre><code>conda run -n merfish3d --live-stream bash -lc \"sim-decode /path/to/simulation/example_16bit_cells/0.315/sim_acquisition\"\n</code></pre></li> <li>Calculate F1-score <pre><code>conda run -n merfish3d --live-stream bash -lc \"sim-f1score /path/to/simulation/example_16bit_cells/0.315\"\n</code></pre></li> </ol>"},{"location":"examples/statphysbio_synthetic/#ensuring-a-sucessful-run","title":"Ensuring a sucessful run","text":"<p>If the runs are succesful, you should have F1 scores that match the following values:</p> Simulation Type Axial Spacing (\u00b5m) Precision Recall F1 score Uniform MERFISH 0.315 1.00 1.00 1.00 Uniform MERFISH 1.0 0.93 0.92 0.93 Uniform MERFISH 1.5 0.66 0.65 0.66 Cells MERFISH 0.315 1.00 1.00 1.00 Cells MERFISH 1.0 0.96 1.00 0.98 Cells MERFISH 1.5 0.70 0.62 0.66"},{"location":"examples/zhuang_lab_mouse_brain/","title":"Zhuang laboratory mouse motor cortex example","text":"<p>Update in process for v0.7.5.</p>"},{"location":"reference/","title":"API Reference","text":"<p>Welcome to the API Reference for MERFISH3D Analysis.</p> <p>Select a module or class below for detailed documentation:</p>"},{"location":"reference/#classes","title":"Classes","text":"<ul> <li>DataRegistration</li> <li>PixelDecoder</li> <li>qi2labDataStore</li> </ul>"},{"location":"reference/#modules","title":"Modules","text":"<ul> <li>Data IO Module</li> <li>Image Processing Module</li> <li>OPM Tools Module</li> <li>Registration Module</li> </ul>"},{"location":"reference/classes/DataRegistration/","title":"DataRegistration Class","text":"<p>Register qi2lab 3D MERFISH data using cross-correlation and optical flow.</p> <p>This module enables the registration of MERFISH datasets by utilizing cross-correlation and optical flow techniques.</p> History: <ul> <li>2025/07:<ul> <li>Implement anistropic downsampling for registration.</li> <li>Implement RLGC deconvolution.</li> <li>Implement new GPU based pixel-warping strategy using warpfield</li> <li>Implement multi-GPU processing.</li> </ul> </li> <li>2024/12: Refactor repo structure.</li> <li>2024/08:<ul> <li>Switched to qi2labdatastore for data access.</li> <li>Implemented numba-accelerated downsampling.</li> <li>Cleaned numpy usage for ryomen tiling.</li> </ul> </li> <li>2024/07: Integrated pycudadecon and removed Dask usage.</li> <li>2024/04: Updated for U-FISH, removed SPOTS3D.</li> <li>2024/01: Adjusted for qi2lab MERFISH file format v0.1.</li> <li>2023/09: Initial commit.</li> </ul> <p>Classes:</p> Name Description <code>DataRegistration</code> <p>Register 2D or 3D MERFISH data across rounds.</p>"},{"location":"reference/classes/DataRegistration/#merfish3danalysis.DataRegistration.DataRegistration","title":"<code>DataRegistration</code>","text":"<p>Register 2D or 3D MERFISH data across rounds.</p> <p>Parameters:</p> Name Type Description Default <code>datastore</code> <code>qi2labDataStore</code> <p>Initialized qi2labDataStore object</p> required <code>decon_polyDT</code> <code>bool</code> <p>Deconvolve ALL polyDT rounds. False = only deconvolve round 1 for downstream stitching.</p> <code>False</code> <code>bkd_subtract_polyDT</code> <code>bool</code> <p>Background subtraction ALL polyDT rounds.</p> <code>True</code> <code>overwrite_registered</code> <code>bool</code> <p>Overwrite existing registered data and registrations</p> <code>False</code> <code>perform_optical_flow</code> <code>bool</code> <p>Perform optical flow registration</p> <code>True</code> <code>save_all_polyDT_registered</code> <code>bool</code> <p>Save fidicual polyDT rounds &gt; 1. These are not used for analysis.</p> <code>False</code> <code>num_gpus</code> <code>int</code> <p>Number of GPUs to use for registration.</p> <code>1</code> <code>crop_yx_decon</code> <code>int</code> <p>Crop size for deconvolution applied to both y and x dimensions.</p> <code>1024</code> <p>Methods:</p> Name Description <code>dataset_path</code> <p>Set the qi2labDataStore object.</p> <code>register_all_tiles</code> <p>Helper function to register all tiles.</p> <code>register_one_tile</code> <p>Helper function to register one tile.</p> <p>Attributes:</p> Name Type Description <code>datastore</code> <p>Return the qi2labDataStore object.</p> <code>overwrite_registered</code> <p>Get the overwrite_registered flag.</p> <code>perform_optical_flow</code> <p>Get the perform_optical_flow flag.</p> <code>tile_id</code> <p>Get the current tile id.</p> Source code in <code>src/merfish3danalysis/DataRegistration.py</code> <pre><code>class DataRegistration:\n    \"\"\"Register 2D or 3D MERFISH data across rounds.\n\n    Parameters\n    ----------\n    datastore : qi2labDataStore\n        Initialized qi2labDataStore object\n    decon_polyDT: bool, default False\n        Deconvolve ALL polyDT rounds. False = only deconvolve round 1 for downstream stitching.\n    bkd_subtract_polyDT: bool, default True\n        Background subtraction ALL polyDT rounds.\n    overwrite_registered: bool, default False\n        Overwrite existing registered data and registrations\n    perform_optical_flow: bool, default False\n        Perform optical flow registration\n    save_all_polyDT_registered: bool, default True\n        Save fidicual polyDT rounds &gt; 1. These are not used for analysis. \n    num_gpus: int, default 1\n        Number of GPUs to use for registration.\n    crop_yx_decon: int, default 1024\n        Crop size for deconvolution applied to both y and x dimensions.\n    \"\"\"\n\n    def __init__(\n        self,\n        datastore: qi2labDataStore,\n        decon_polyDT: bool = False,\n        bkd_subtract_polyDT: bool = True,\n        overwrite_registered: bool = False,\n        perform_optical_flow: bool = True,\n        save_all_polyDT_registered: bool = False,\n        num_gpus: int = 1,\n        crop_yx_decon: int = 1024\n    ):\n\n        self._datastore = datastore\n        self._decon_polyDT = decon_polyDT\n        self._tile_ids = self._datastore.tile_ids\n        self._round_ids = self._datastore.round_ids\n        self._bit_ids = self._datastore.bit_ids\n        self._psfs = self._datastore.channel_psfs\n        self._num_gpus = num_gpus\n        self._crop_yx_decon = crop_yx_decon\n        self._bkd_subtract_polyDT = bkd_subtract_polyDT\n\n        self._perform_optical_flow = perform_optical_flow\n        self._data_raw = None\n        self._has_registered_data = None\n        self._overwrite_registered = overwrite_registered\n        self.save_all_polyDT_registered = save_all_polyDT_registered\n        self._decon = True\n        self._original_print = builtins.print\n\n    # -----------------------------------\n    # property access for class variables\n    # -----------------------------------\n    @property\n    def datastore(self):\n        \"\"\"Return the qi2labDataStore object.\n\n        Returns\n        -------\n        qi2labDataStore\n            qi2labDataStore object\n        \"\"\"\n\n        if self._dataset_path is not None:\n            return self._datastore\n        else:\n            print(\"Datastore not defined.\")\n            return None\n\n    @datastore.setter\n    def dataset_path(self, value: qi2labDataStore):\n        \"\"\"Set the qi2labDataStore object.\n\n        Parameters\n        ----------\n        value : qi2labDataStore\n            qi2labDataStore object\n        \"\"\"\n\n        del self._datastore\n        self._datastore = value\n\n    @property\n    def tile_id(self):\n        \"\"\"Get the current tile id.\n\n        Returns\n        -------\n        tile_id: Union[int,str]\n            Tile id\n        \"\"\"\n\n        if self._tile_id is not None:\n            tile_id = self._tile_id\n            return tile_id\n        else:\n            print(\"Tile coordinate not defined.\")\n            return None\n\n    @tile_id.setter\n    def tile_id(self, value: Union[int,str]):\n        \"\"\"Set the tile id.\n\n        Parameters\n        ----------\n        value : Union[int,str]\n            Tile id\n        \"\"\"\n\n        if isinstance(value, int):\n            if value &lt; 0 or value &gt; self._datastore.num_tiles:\n                print(\"Set value index &gt;=0 and &lt;=\" + str(self._datastore.num_tiles))\n                return None\n            else:\n                self._tile_id = self._datastore.tile_ids[value]\n        elif isinstance(value, str):\n            if value not in self._datastore.tile_ids:\n                print(\"set valid tile id\")\n                return None\n            else:\n                self._tile_id = value\n\n    @property\n    def perform_optical_flow(self):\n        \"\"\"Get the perform_optical_flow flag.\n\n        Returns\n        -------\n        perform_optical_flow: bool\n            Perform optical flow registration\n        \"\"\"\n\n        return self._perform_optical_flow\n\n    @perform_optical_flow.setter\n    def perform_optical_flow(self, value: bool):\n        \"\"\"Set the perform_optical_flow flag.\n\n        Parameters\n        ----------\n        value : bool\n            Perform optical flow registration\n        \"\"\"\n\n        self._perform_optical_flow = value\n\n    @property\n    def overwrite_registered(self):\n        \"\"\"Get the overwrite_registered flag.\n\n        Returns\n        -------\n        overwrite_registered: bool\n            Overwrite existing registered data and registrations\n        \"\"\"\n\n        return self._overwrite_registered\n\n    @overwrite_registered.setter\n    def overwrite_registered(self, value: bool):\n        \"\"\"Set the overwrite_registered flag.\n\n        Parameters\n        ----------\n        value : bool\n            Overwrite existing registered data and registrations\n        \"\"\"\n\n        self._overwrite_registered = value\n\n    def register_all_tiles(self):\n        \"\"\"Helper function to register all tiles.\"\"\"\n        for tile_id in self._datastore.tile_ids:\n            self.tile_id=tile_id\n            self._generate_registrations()\n            self._apply_registration_to_bits()\n\n    def register_one_tile(self, tile_id: Union[int,str]):\n        \"\"\"Helper function to register one tile.\n\n        Parameters\n        ----------\n        tile_id : Union[int,str]\n            Tile id\n        \"\"\"\n\n        self.tile_id = tile_id\n        self._generate_registrations()\n        self._apply_registration_to_bits()\n\n    def _load_raw_data(self):\n        \"\"\"Load raw data across rounds for one tile.\"\"\"\n\n        self._data_raw = []\n        stage_positions = []\n\n        for round_id in self._round_ids:\n            self._data_raw.append(\n                self._datastore.load_local_corrected_image(\n                tile=self._tile_id,\n                round=round_id,\n                )\n            )\n\n            stage_position, _ = self._datastore.load_local_stage_position_zyx_um(\n                    tile=self._tile_id,\n                    round=round_id\n                )\n\n            stage_positions.append(stage_position)\n\n        self._stage_positions = np.stack(stage_positions, axis=0)\n        del stage_positions\n        gc.collect()\n\n    def _generate_registrations(self):\n        \"\"\"Generate registered, deconvolved fiducial data and save to datastore.\"\"\"\n        test =  self._datastore.load_local_registered_image(\n            tile=self._tile_id,\n            round=self._round_ids[0]\n        )\n\n        if test is None:\n            has_reg_decon_data = False\n        else:\n            has_reg_decon_data = True\n\n        if not (has_reg_decon_data) or self._overwrite_registered:\n\n            p_first = mp.Process(target=_apply_first_polyDT_on_gpu, args=(self,0))\n            p_first.start()\n            p_first.join()\n\n        # 1) How many GPUs do we have?\n        if self._num_gpus == 0:\n            raise RuntimeError(\"No GPUs detected. Cannot run _generate_registrations().\")\n\n        # 2) Grab all rounds IDs after round 0 and split into `num_gpus` chunks\n        all_rounds = list(self._round_ids[1:])\n        chunk_size = (len(all_rounds) + self._num_gpus - 1) // self._num_gpus  # ceiling division\n\n        # 3) Launch one process per GPU (only as many as needed)\n        processes = []\n        for gpu_id in range(self._num_gpus):\n            start = gpu_id * chunk_size\n            end = min(start + chunk_size, len(all_rounds))\n            if start &gt;= end:\n                break  # no more rounds to assign\n\n            subset = all_rounds[start:end]\n\n            old_vis = os.environ.get(\"CUDA_VISIBLE_DEVICES\")\n            os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n            try:\n                # Inside child, logical device 0 maps to this physical GPU.\n                p = mp.Process(target=_apply_polyDT_on_gpu, args=(self, subset, 0))\n                p.start()\n                processes.append(p)\n            finally:\n                if old_vis is None:\n                    os.environ.pop(\"CUDA_VISIBLE_DEVICES\", None)\n                else:\n                    os.environ[\"CUDA_VISIBLE_DEVICES\"] = old_vis\n\n        # 4) Wait for all GPU\u2010workers to finish\n        for p in processes:\n            p.join()\n\n    def _apply_registration_to_bits(self):\n        \"\"\"Generate ufish + deconvolved, registered readout data and save to datastore.\"\"\"\n        # 1) How many GPUs do we have?\n        if self._num_gpus == 0:\n            raise RuntimeError(\"No GPUs detected. Cannot run _apply_registration_to_bits().\")\n\n        # 2) Grab all bit IDs and split into `num_gpus` chunks\n        all_bits = list(self._bit_ids)\n        chunk_size = (len(all_bits) + self._num_gpus - 1) // self._num_gpus  # ceiling division\n\n        # 3) Launch one process per GPU (only as many as needed)\n        processes = []\n        for gpu_id in range(self._num_gpus):\n            start = gpu_id * chunk_size\n            end = min(start + chunk_size, len(all_bits))\n            if start &gt;= end:\n                break  # no more bits to assign\n\n            subset = all_bits[start:end]\n\n            old_vis = os.environ.get(\"CUDA_VISIBLE_DEVICES\")\n            os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n            try:\n                p = mp.Process(target=_apply_bits_on_gpu, args=(self, subset, 0))\n                p.start()\n                processes.append(p)\n            finally:\n                if old_vis is None:\n                    os.environ.pop(\"CUDA_VISIBLE_DEVICES\", None)\n                else:\n                    os.environ[\"CUDA_VISIBLE_DEVICES\"] = old_vis\n\n\n        # 4) Wait for all GPU\u2010workers to finish\n        for p in processes:\n            p.join()\n</code></pre>"},{"location":"reference/classes/DataRegistration/#merfish3danalysis.DataRegistration.DataRegistration.datastore","title":"<code>datastore</code>  <code>property</code>","text":"<p>Return the qi2labDataStore object.</p> <p>Returns:</p> Type Description <code>qi2labDataStore</code> <p>qi2labDataStore object</p>"},{"location":"reference/classes/DataRegistration/#merfish3danalysis.DataRegistration.DataRegistration.overwrite_registered","title":"<code>overwrite_registered</code>  <code>property</code> <code>writable</code>","text":"<p>Get the overwrite_registered flag.</p> <p>Returns:</p> Name Type Description <code>overwrite_registered</code> <code>bool</code> <p>Overwrite existing registered data and registrations</p>"},{"location":"reference/classes/DataRegistration/#merfish3danalysis.DataRegistration.DataRegistration.perform_optical_flow","title":"<code>perform_optical_flow</code>  <code>property</code> <code>writable</code>","text":"<p>Get the perform_optical_flow flag.</p> <p>Returns:</p> Name Type Description <code>perform_optical_flow</code> <code>bool</code> <p>Perform optical flow registration</p>"},{"location":"reference/classes/DataRegistration/#merfish3danalysis.DataRegistration.DataRegistration.tile_id","title":"<code>tile_id</code>  <code>property</code> <code>writable</code>","text":"<p>Get the current tile id.</p> <p>Returns:</p> Name Type Description <code>tile_id</code> <code>Union[int, str]</code> <p>Tile id</p>"},{"location":"reference/classes/DataRegistration/#merfish3danalysis.DataRegistration.DataRegistration._apply_registration_to_bits","title":"<code>_apply_registration_to_bits()</code>","text":"<p>Generate ufish + deconvolved, registered readout data and save to datastore.</p> Source code in <code>src/merfish3danalysis/DataRegistration.py</code> <pre><code>def _apply_registration_to_bits(self):\n    \"\"\"Generate ufish + deconvolved, registered readout data and save to datastore.\"\"\"\n    # 1) How many GPUs do we have?\n    if self._num_gpus == 0:\n        raise RuntimeError(\"No GPUs detected. Cannot run _apply_registration_to_bits().\")\n\n    # 2) Grab all bit IDs and split into `num_gpus` chunks\n    all_bits = list(self._bit_ids)\n    chunk_size = (len(all_bits) + self._num_gpus - 1) // self._num_gpus  # ceiling division\n\n    # 3) Launch one process per GPU (only as many as needed)\n    processes = []\n    for gpu_id in range(self._num_gpus):\n        start = gpu_id * chunk_size\n        end = min(start + chunk_size, len(all_bits))\n        if start &gt;= end:\n            break  # no more bits to assign\n\n        subset = all_bits[start:end]\n\n        old_vis = os.environ.get(\"CUDA_VISIBLE_DEVICES\")\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n        try:\n            p = mp.Process(target=_apply_bits_on_gpu, args=(self, subset, 0))\n            p.start()\n            processes.append(p)\n        finally:\n            if old_vis is None:\n                os.environ.pop(\"CUDA_VISIBLE_DEVICES\", None)\n            else:\n                os.environ[\"CUDA_VISIBLE_DEVICES\"] = old_vis\n\n\n    # 4) Wait for all GPU\u2010workers to finish\n    for p in processes:\n        p.join()\n</code></pre>"},{"location":"reference/classes/DataRegistration/#merfish3danalysis.DataRegistration.DataRegistration._generate_registrations","title":"<code>_generate_registrations()</code>","text":"<p>Generate registered, deconvolved fiducial data and save to datastore.</p> Source code in <code>src/merfish3danalysis/DataRegistration.py</code> <pre><code>def _generate_registrations(self):\n    \"\"\"Generate registered, deconvolved fiducial data and save to datastore.\"\"\"\n    test =  self._datastore.load_local_registered_image(\n        tile=self._tile_id,\n        round=self._round_ids[0]\n    )\n\n    if test is None:\n        has_reg_decon_data = False\n    else:\n        has_reg_decon_data = True\n\n    if not (has_reg_decon_data) or self._overwrite_registered:\n\n        p_first = mp.Process(target=_apply_first_polyDT_on_gpu, args=(self,0))\n        p_first.start()\n        p_first.join()\n\n    # 1) How many GPUs do we have?\n    if self._num_gpus == 0:\n        raise RuntimeError(\"No GPUs detected. Cannot run _generate_registrations().\")\n\n    # 2) Grab all rounds IDs after round 0 and split into `num_gpus` chunks\n    all_rounds = list(self._round_ids[1:])\n    chunk_size = (len(all_rounds) + self._num_gpus - 1) // self._num_gpus  # ceiling division\n\n    # 3) Launch one process per GPU (only as many as needed)\n    processes = []\n    for gpu_id in range(self._num_gpus):\n        start = gpu_id * chunk_size\n        end = min(start + chunk_size, len(all_rounds))\n        if start &gt;= end:\n            break  # no more rounds to assign\n\n        subset = all_rounds[start:end]\n\n        old_vis = os.environ.get(\"CUDA_VISIBLE_DEVICES\")\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n        try:\n            # Inside child, logical device 0 maps to this physical GPU.\n            p = mp.Process(target=_apply_polyDT_on_gpu, args=(self, subset, 0))\n            p.start()\n            processes.append(p)\n        finally:\n            if old_vis is None:\n                os.environ.pop(\"CUDA_VISIBLE_DEVICES\", None)\n            else:\n                os.environ[\"CUDA_VISIBLE_DEVICES\"] = old_vis\n\n    # 4) Wait for all GPU\u2010workers to finish\n    for p in processes:\n        p.join()\n</code></pre>"},{"location":"reference/classes/DataRegistration/#merfish3danalysis.DataRegistration.DataRegistration._load_raw_data","title":"<code>_load_raw_data()</code>","text":"<p>Load raw data across rounds for one tile.</p> Source code in <code>src/merfish3danalysis/DataRegistration.py</code> <pre><code>def _load_raw_data(self):\n    \"\"\"Load raw data across rounds for one tile.\"\"\"\n\n    self._data_raw = []\n    stage_positions = []\n\n    for round_id in self._round_ids:\n        self._data_raw.append(\n            self._datastore.load_local_corrected_image(\n            tile=self._tile_id,\n            round=round_id,\n            )\n        )\n\n        stage_position, _ = self._datastore.load_local_stage_position_zyx_um(\n                tile=self._tile_id,\n                round=round_id\n            )\n\n        stage_positions.append(stage_position)\n\n    self._stage_positions = np.stack(stage_positions, axis=0)\n    del stage_positions\n    gc.collect()\n</code></pre>"},{"location":"reference/classes/DataRegistration/#merfish3danalysis.DataRegistration.DataRegistration.dataset_path","title":"<code>dataset_path(value)</code>","text":"<p>Set the qi2labDataStore object.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>qi2labDataStore</code> <p>qi2labDataStore object</p> required Source code in <code>src/merfish3danalysis/DataRegistration.py</code> <pre><code>@datastore.setter\ndef dataset_path(self, value: qi2labDataStore):\n    \"\"\"Set the qi2labDataStore object.\n\n    Parameters\n    ----------\n    value : qi2labDataStore\n        qi2labDataStore object\n    \"\"\"\n\n    del self._datastore\n    self._datastore = value\n</code></pre>"},{"location":"reference/classes/DataRegistration/#merfish3danalysis.DataRegistration.DataRegistration.register_all_tiles","title":"<code>register_all_tiles()</code>","text":"<p>Helper function to register all tiles.</p> Source code in <code>src/merfish3danalysis/DataRegistration.py</code> <pre><code>def register_all_tiles(self):\n    \"\"\"Helper function to register all tiles.\"\"\"\n    for tile_id in self._datastore.tile_ids:\n        self.tile_id=tile_id\n        self._generate_registrations()\n        self._apply_registration_to_bits()\n</code></pre>"},{"location":"reference/classes/DataRegistration/#merfish3danalysis.DataRegistration.DataRegistration.register_one_tile","title":"<code>register_one_tile(tile_id)</code>","text":"<p>Helper function to register one tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile_id</code> <code>Union[int, str]</code> <p>Tile id</p> required Source code in <code>src/merfish3danalysis/DataRegistration.py</code> <pre><code>def register_one_tile(self, tile_id: Union[int,str]):\n    \"\"\"Helper function to register one tile.\n\n    Parameters\n    ----------\n    tile_id : Union[int,str]\n        Tile id\n    \"\"\"\n\n    self.tile_id = tile_id\n    self._generate_registrations()\n    self._apply_registration_to_bits()\n</code></pre>"},{"location":"reference/classes/DataRegistration/#merfish3danalysis.DataRegistration._apply_bits_on_gpu","title":"<code>_apply_bits_on_gpu(dr, bit_list, gpu_id=0)</code>","text":"<p>Run the \u201cdeconvolve\u2192rigid+optical\u2010flow\u2192UFish\u201d loop for a subset of bits on a single GPU.</p> <p>Parameters:</p> Name Type Description Default <code>dr</code> <p>DataRegistration instance (pickled into this process)</p> required <code>bit_list</code> <code>list</code> <p>bit_ids to process on this GPU</p> required <code>gpu_id</code> <code>int</code> <p>physical GPU to bind in this process</p> <code>0</code> Source code in <code>src/merfish3danalysis/DataRegistration.py</code> <pre><code>def _apply_bits_on_gpu(\n    dr,\n    bit_list: list, \n    gpu_id: int = 0\n):\n    \"\"\"\n    Run the \u201cdeconvolve\u2192rigid+optical\u2010flow\u2192UFish\u201d loop for a subset of bits on a single GPU.\n\n    Parameters\n    ----------\n    dr       : \n        DataRegistration instance (pickled into this process)\n    bit_list : list \n        bit_ids to process on this GPU\n    gpu_id : int\n        physical GPU to bind in this process\n    \"\"\"\n\n    import torch\n    import cupy as cp\n\n    torch.cuda.set_device(gpu_id)\n    cp.cuda.Device(gpu_id).use()\n\n    from warpfield.warp import warp_volume\n    import os\n    os.environ[\"ORT_LOG_SEVERITY_LEVEL\"] = \"3\"\n    import onnxruntime as ort\n    ort.set_default_logger_severity(3)\n    from ufish.api import UFish\n    from merfish3danalysis.utils.rlgc import chunked_rlgc, rlgc_biggs\n    from merfish3danalysis.utils.registration import apply_transform\n\n\n    for bit_id in bit_list:\n\n        r_idx = dr._datastore.load_local_round_linker(tile=dr._tile_id, bit=bit_id) - 1\n        ex_wl, em_wl = dr._datastore.load_local_wavelengths_um(tile=dr._tile_id, bit=bit_id)\n        psf_idx = 1 if ex_wl &lt; 600 else 2\n\n        test = dr._datastore.load_local_registered_image(tile=dr._tile_id, bit=bit_id)\n        reg_on_disk = (test is not None)\n\n        if (not reg_on_disk) or dr._overwrite_registered:\n            # load data\n            corrected_image = dr._datastore.load_local_corrected_image(\n                tile=dr._tile_id, bit=bit_id, return_future=False\n            )\n\n            # deconvolution\n            if dr._decon:\n                if dr._datastore.microscope_type == \"2D\":\n                    decon_image = np.zeros_like(corrected_image,dtype=np.float32)\n                    for z_idx in range(corrected_image.shape[0]):\n                        decon_image[z_idx,:] = rlgc_biggs(\n                            image = corrected_image[z_idx,:],\n                            psf = dr._psfs[psf_idx,:],\n                            gpu_id = gpu_id,\n                            eager_mode=False\n                        )\n                    decon_image = decon_image.clip(0,2**16-1).astype(np.uint16)\n                else:\n                    decon_image = chunked_rlgc(\n                        image=corrected_image,\n                        psf=dr._psfs[psf_idx, :],\n                        gpu_id = gpu_id,\n                        crop_yx = dr._crop_yx_decon\n                    )\n            else:\n                decon_image = corrected_image.copy()\n\n            # apply rigid + (optional) optical\u2010flow if r_idx &gt; 0\n            if r_idx &gt; 0:\n                rigid_xyz_px = dr._datastore.load_local_rigid_xform_xyz_px(\n                    tile=dr._tile_id, round=dr._round_ids[r_idx]\n                )\n                shift_xyz = [float(v) for v in rigid_xyz_px]\n                xyz_tx = sitk.TranslationTransform(3, np.asarray(shift_xyz))\n\n                # apply rigid\n                decon_image_rigid = apply_transform(decon_image, decon_image, xyz_tx)\n                del decon_image\n\n                if dr._perform_optical_flow:\n                    warp_field, block_size, block_stride = dr._datastore.load_coord_of_xform_px(\n                         tile=dr._tile_id, \n                         round=dr._round_ids[r_idx], \n                         return_future=False\n                    )\n\n                    block_size = cp.asarray(block_size, dtype=cp.float32)\n                    block_stride = cp.asarray(block_stride, dtype=cp.float32)\n                    decon_image_warped_cp = warp_volume(\n                        decon_image_rigid, \n                        warp_field, \n                        block_stride, \n                        cp.array(-block_size / block_stride / 2), \n                        out=None,\n                        gpu_id=gpu_id\n                    )\n                    data_reg = cp.asnumpy(decon_image_warped_cp).astype(np.float32)\n                    del decon_image_warped_cp\n                    gc.collect()\n\n                else:\n                    data_reg = decon_image_rigid.copy()\n                    del decon_image_rigid\n                gc.collect()\n            else:\n                data_reg = decon_image.copy()\n                del decon_image\n                gc.collect()\n\n            # clip to uint16\n            data_reg = data_reg.clip(0,2**16-1).astype(np.uint16)\n\n            # UFISH\n            ufish = UFish(device=f\"cuda:{gpu_id}\")\n            ufish.load_weights_from_internet()\n            ufish_loc, ufish_data = ufish.predict(\n                data_reg, axes=\"zyx\", blend_3d=False, batch_size=1\n            )\n\n            ufish_loc = ufish_loc.rename(\n                columns={\"axis-0\": \"z\", \"axis-1\": \"y\", \"axis-2\": \"x\"}\n            )\n            del ufish\n            gc.collect()\n\n            torch.cuda.empty_cache()\n            cp.get_default_memory_pool().free_all_blocks()\n            gc.collect()\n\n            # UFISH ROI sums\n            roi_z, roi_y, roi_x = 7, 5, 5\n\n            def sum_pixels_in_roi(row, image, roi_dims):\n                z, y, x = row[\"z\"], row[\"y\"], row[\"x\"]\n                rz, ry, rx = roi_dims\n                zmin = max(0, z - rz // 2)\n                ymin = max(0, y - ry // 2)\n                xmin = max(0, x - rx // 2)\n                zmax = min(image.shape[0], zmin + rz)\n                ymax = min(image.shape[1], ymin + ry)\n                xmax = min(image.shape[2], xmin + rx)\n                roi = image[int(zmin):int(zmax), int(ymin):int(ymax), int(xmin):int(xmax)]\n                return np.sum(roi)\n\n            ufish_loc[\"sum_prob_pixels\"] = ufish_loc.apply(\n                sum_pixels_in_roi, axis=1, image=ufish_data, roi_dims=(roi_z, roi_y, roi_x)\n            )\n            ufish_loc[\"sum_decon_pixels\"] = ufish_loc.apply(\n                sum_pixels_in_roi, axis=1, image=data_reg, roi_dims=(roi_z, roi_y, roi_x)\n            )\n\n            ufish_loc[\"tile_idx\"] = dr._tile_ids.index(dr._tile_id)\n            ufish_loc[\"bit_idx\"] = dr._bit_ids.index(bit_id) + 1\n            ufish_loc[\"tile_z_px\"] = ufish_loc[\"z\"]\n            ufish_loc[\"tile_y_px\"] = ufish_loc[\"y\"]\n            ufish_loc[\"tile_x_px\"] = ufish_loc[\"x\"]\n\n            # save results\n            dr._datastore.save_local_registered_image(\n                data_reg, tile=dr._tile_id, deconvolution=True, bit=bit_id\n            )\n            dr._datastore.save_local_ufish_image(ufish_data, tile=dr._tile_id, bit=bit_id)\n            dr._datastore.save_local_ufish_spots(ufish_loc, tile=dr._tile_id, bit=bit_id)\n            print(time_stamp(), f\"Finished readout tile id: {dr._tile_id}; bit id: {bit_id}.\")\n\n            del data_reg, ufish_data, ufish_loc\n            gc.collect()\n\n    return True\n</code></pre>"},{"location":"reference/classes/DataRegistration/#merfish3danalysis.DataRegistration._apply_polyDT_on_gpu","title":"<code>_apply_polyDT_on_gpu(dr, round_list, gpu_id=0)</code>","text":"<p>Run the \u201cdeconvolve\u2192rigid+optical\u2010flow\u201d loop for a subset of polyDT rounds on a single GPU.</p> <p>Parameters:</p> Name Type Description Default <code>dr</code> <code>Registration</code> <p>DataRegistration instance (pickled into this process)</p> required <code>bit_list</code> <code>list</code> <p>bit_ids to process on this GPU</p> required <code>gpu_id</code> <code>int</code> <p>physical GPU to bind in this process</p> <code>0</code> Source code in <code>src/merfish3danalysis/DataRegistration.py</code> <pre><code>def _apply_polyDT_on_gpu(\n    dr,\n    round_list: list,\n    gpu_id: int = 0\n):\n    \"\"\"\n    Run the \u201cdeconvolve\u2192rigid+optical\u2010flow\u201d loop for a subset of polyDT rounds on a single GPU.\n\n    Parameters\n    ----------\n    dr : Registration\n        DataRegistration instance (pickled into this process)\n    bit_list : list \n        bit_ids to process on this GPU\n    gpu_id : int\n        physical GPU to bind in this process\n    \"\"\"\n    import torch\n    torch.cuda.set_device(gpu_id)\n    import cupy as cp\n    cp.cuda.Device(gpu_id).use()\n\n\n    import imagej\n    import io\n    import contextlib\n    from merfish3danalysis.utils.registration import compute_warpfield\n    from merfish3danalysis.utils.registration import (\n        apply_transform,\n        compute_rigid_transform\n    )\n    from merfish3danalysis.utils.imageprocessing import downsample_image_anisotropic\n\n\n    stderr_buffer = io.StringIO()\n    with contextlib.redirect_stderr(stderr_buffer):\n\n        ij_success = False\n        while not(ij_success):\n            try:\n                os.environ.setdefault(\"CLIJ_OPENCL_ALLOWED_DEVICE_TYPE\", \"CPU\")\n                ij = imagej.init()\n                ij_success = True\n            except:\n                time.sleep(.5)\n                ij_success = False\n        for r_idx, round_id in enumerate(round_list):\n\n            test =  dr._datastore.load_local_registered_image(\n                tile=dr._tile_id,\n                round=round_id,\n                return_future=False\n            )\n            if test is None:\n                has_reg_decon_data = False\n            else:\n                has_reg_decon_data = True\n\n            if not (has_reg_decon_data) or dr._overwrite_registered:\n                if dr._decon_polyDT:\n                    ref_image_decon_float = dr._datastore.load_local_registered_image(\n                        tile=dr._tile_id,\n                        round=dr._round_ids[0],\n                        return_future=False\n                    ).astype(np.float32)\n                else:\n                    ref_image = dr._datastore.load_local_corrected_image(\n                        tile=dr._tile_id,\n                        round=dr._round_ids[0],\n                        return_future=False\n                    )\n                    if dr._bkd_subtract_polyDT:\n                        imp_array = ij.py.to_imageplus(ref_image)\n                        imp_array.setStack(imp_array.getStack().duplicate())\n                        imp_array.show()\n                        ij.IJ.run(imp_array,\"Subtract Background...\", \"rolling=200 disable stack\")\n                        imp_array.show()\n                        ij.py.sync_image(imp_array)\n                        bkd_output = ij.py.from_java(imp_array.duplicate())\n                        imp_array.close()\n                        ref_image_bkd = np.swapaxes(bkd_output.data.transpose(2,1,0),1,2).clip(0,2**16-1).astype(np.uint16).copy()\n                        ref_image_decon_float = ref_image_bkd.copy().astype(np.float32)\n                        del imp_array, ref_image, bkd_output, ref_image_bkd\n                    else:\n                        ref_image_decon_float = ref_image.copy().astype(np.float32)\n                        del ref_image\n\n                raw = dr._datastore.load_local_corrected_image(\n                    tile=dr._tile_id,\n                    round=round_id,\n                    return_future=False\n                )\n\n                if dr._decon_polyDT:\n                    from merfish3danalysis.utils.rlgc import chunked_rlgc, rlgc_biggs\n                    if dr._datastore.microscope_type == \"2D\":\n                        imp_array = ij.py.to_imageplus(raw)\n                        imp_array.setStack(imp_array.getStack().duplicate())\n                        imp_array.show()\n                        ij.IJ.run(imp_array,\"Subtract Background...\", \"rolling=200 disable stack\")\n                        imp_array.show()\n                        ij.py.sync_image(imp_array)\n                        bkd_output = ij.py.from_java(imp_array.duplicate())\n                        imp_array.close()\n                        raw_bkd = np.swapaxes(bkd_output.data.transpose(2,1,0),1,2).clip(0,2**16-1).astype(np.uint16).copy()\n                        del imp_array, bkd_output, raw\n                        mov_image_decon = np.zeros_like(raw_bkd,dtype=np.float32)\n                        for z_idx in range(raw.shape[0]):\n                            mov_image_decon[z_idx,:] = rlgc_biggs(\n                                image = raw[z_idx,:],\n                                psf = dr._psfs[0,:],\n                                gpu_id = gpu_id,\n                                eager_mode=True\n                            )\n                        mov_image_decon = mov_image_decon.clip(0,2**16-1).astype(np.uint16)\n                    else:\n                        mov_image_decon = chunked_rlgc(\n                            image=raw,\n                            psf=dr._psfs[0, :],\n                            gpu_id=gpu_id,\n                            crop_yx = dr._crop_yx_decon,\n                            bkd = True,\n                            ij = ij\n                        )\n                else:\n                    if dr._bkd_subtract_polyDT:\n                        imp_array = ij.py.to_imageplus(raw)\n                        imp_array.setStack(imp_array.getStack().duplicate())\n                        imp_array.show()\n                        ij.IJ.run(imp_array,\"Subtract Background...\", \"rolling=200 disable stack\")\n                        imp_array.show()\n                        ij.py.sync_image(imp_array)\n                        bkd_output = ij.py.from_java(imp_array.duplicate())\n                        imp_array.close()\n                        mov_image_decon = np.swapaxes(bkd_output.data.transpose(2,1,0),1,2).clip(0,2**16-1).astype(np.uint16).copy()\n                        del imp_array, raw, bkd_output\n                    else:\n                        mov_image_decon = raw.copy().astype(np.uint16)\n                        del raw\n\n                mov_image_decon_float = mov_image_decon.copy().astype(np.float32)\n                del mov_image_decon\n\n                if dr._datastore.microscope_type == \"3D\":\n                    downsample_factors = [3,9,9]\n                    if max(downsample_factors) &gt; 1:\n                        ref_image_decon_float_ds = downsample_image_anisotropic(\n                            ref_image_decon_float, downsample_factors\n                        )\n                        mov_image_decon_float_ds = downsample_image_anisotropic(\n                            mov_image_decon_float, downsample_factors\n                        )\n                    else:\n                        ref_image_decon_float_ds = ref_image_decon_float.copy()\n                        mov_image_decon_float_ds = mov_image_decon_float.copy()\n                else:\n                    downsample_factors = [1,3,3]\n                    if max(downsample_factors) &gt; 1:\n                        ref_image_decon_float_ds = downsample_image_anisotropic(\n                            ref_image_decon_float, downsample_factors\n                        )\n                        mov_image_decon_float_ds = downsample_image_anisotropic(\n                            mov_image_decon_float, downsample_factors\n                        )\n                    else:\n                        ref_image_decon_float_ds = ref_image_decon_float.copy()\n                        mov_image_decon_float_ds = mov_image_decon_float.copy()\n\n\n                _, lowres_xyz_shift = compute_rigid_transform(\n                    ref_image_decon_float_ds,\n                    mov_image_decon_float_ds,\n                    downsample_factors=downsample_factors,\n                    mask = None,\n                    projection=None,\n                    gpu_id = gpu_id\n                )\n\n                xyz_shift = np.asarray(lowres_xyz_shift,dtype=np.float32)\n                xyz_shift_float = [round(float(v),1) for v in lowres_xyz_shift]\n\n                #print(time_stamp(), f\"GPU {gpu_id}: processed tile id: {dr._tile_id}; round id: {round_id}; rigid xyz offset: {xyz_shift_float}.\")\n\n                initial_xyz_transform = sitk.TranslationTransform(3, xyz_shift_float)\n                warped_mov_image_decon_float = apply_transform(\n                    ref_image_decon_float, mov_image_decon_float, initial_xyz_transform\n                )\n                del mov_image_decon_float\n                gc.collect()\n\n                mov_image_decon_float = warped_mov_image_decon_float.copy().astype(np.float32)\n                del warped_mov_image_decon_float\n                gc.collect()\n\n                dr._datastore.save_local_rigid_xform_xyz_px(\n                    rigid_xform_xyz_px=xyz_shift,\n                    tile=dr._tile_id,\n                    round=round_id\n                )\n\n                if dr._perform_optical_flow:\n\n                    data_registered, warp_field, block_size, block_stride = compute_warpfield(\n                        ref_image_decon_float,\n                        mov_image_decon_float,\n                        gpu_id = gpu_id\n                    )\n\n                    dr._datastore.save_coord_of_xform_px(\n                        of_xform_px=warp_field,\n                        tile=dr._tile_id,\n                        block_size=block_size,\n                        block_stride=block_stride,\n                        round=round_id\n                    )\n\n                    data_registered = data_registered.clip(0,2**16-1).astype(np.uint16)\n\n                    del warp_field\n                    gc.collect()\n                else:\n                    data_registered = mov_image_decon_float.clip(0,2**16-1).astype(np.uint16)\n\n                if dr.save_all_polyDT_registered:\n                    dr._datastore.save_local_registered_image(\n                        registered_image=data_registered.astype(np.uint16),\n                        tile=dr._tile_id,\n                        deconvolution=True,\n                        round=round_id\n                    )\n                print(time_stamp(), f\"Finished polyDT tile id: {dr._tile_id}; round id: {round_id}.\")\n\n                del data_registered\n                gc.collect()\n\n                try:\n                    cp.cuda.Stream.null.synchronize()\n                    cp.get_default_memory_pool().free_all_blocks()\n                    cp.get_default_pinned_memory_pool().free_all_blocks()\n                    try:\n                        import cupyx\n                        cupyx.scipy.fft.clear_plan_cache()\n                    except Exception:\n                        pass\n                    try:\n                        cp.fft.config.get_plan_cache().clear()\n                    except Exception:\n                        pass\n                except Exception:\n                    pass\n        del ij\n        gc.collect()\n\n    return True\n</code></pre>"},{"location":"reference/classes/DataRegistration/#merfish3danalysis.DataRegistration._no_op","title":"<code>_no_op(*args, **kwargs)</code>","text":"<p>Function to monkey patch print to suppress output.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments</p> <code>{}</code> Source code in <code>src/merfish3danalysis/DataRegistration.py</code> <pre><code>def _no_op(*args, **kwargs):\n    \"\"\"Function to monkey patch print to suppress output.\n\n    Parameters\n    ----------\n    *args\n        Variable length argument list\n    **kwargs\n        Arbitrary keyword arguments\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/classes/PixelDecoder/","title":"PixelDecoder Class","text":"<p>Perform pixel-based decoding for qi2lab widefield MERFISH data using GPU acceleration.</p> <p>This module leverages GPU acceleration to decode pixel-based widefield MERFISH datasets efficiently.</p> History: <ul> <li>2025/07: <ul> <li>Refactor for multiple GPU support.</li> <li>Switch to cuvs for distance calculations.</li> </ul> </li> <li>2024/12: Refactor repo structure.</li> <li>2024/03: Reworked GPU logic to reduce out-of-memory crashes.</li> <li>2024/01: Updated for qi2lab MERFISH file format v1.0.</li> </ul> <p>Classes:</p> Name Description <code>PixelDecoder</code> <p>Retrieve and process one tile from qi2lab 3D widefield zarr structure.</p> <p>Functions:</p> Name Description <code>decode_tiles_worker</code> <p>Worker that runs decode_one_tile on a subset of tiles under one GPU.</p>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder","title":"<code>PixelDecoder</code>","text":"<p>Retrieve and process one tile from qi2lab 3D widefield zarr structure. Normalize codebook and data, perform plane-by-plane pixel decoding, extract barcode features, and save to disk.</p> <p>Parameters:</p> Name Type Description Default <code>datastore</code> <code>qi2labDataStore</code> <p>qi2labDataStore object</p> required <code>merfish_bits</code> <code>int</code> <p>number of merfish bits. Assumes that in codebook, MERFISH rounds are [0,merfish_bits].</p> <code>16</code> <code>num_gpus</code> <code>int</code> <p>number of GPUs to use for decoding. If &gt; 1, will split decoding across GPUs.</p> <code>1</code> <code>verbose</code> <code>int</code> <p>control verbosity. 0 - no output, 1 - tqdm bars, 2 - diagnostic outputs</p> <code>1</code> <code>use_mask</code> <code>Optional[bool]</code> <p>use mask stored in polyDT directory</p> <code>False</code> <code>z_range</code> <code>Optional[Sequence[int]]</code> <p>z range to analyze. In integer indices from [0,N] where N is number of z planes.</p> <code>None</code> <code>include_blanks</code> <code>Optional[bool]</code> <p>Include Blank codewords in decoding process.</p> <code>True</code> <p>Methods:</p> Name Description <code>calculate_fdr</code> <p>Calculate false discovery rate.</p> <code>decode_all_tiles</code> <p>Optimize normalization by decoding.</p> <code>decode_one_tile</code> <p>Decode one tile.</p> <code>optimize_filtering</code> <p>Optimize filtering.</p> <code>optimize_normalization_by_decoding</code> <p>Optimize normalization by decoding.</p> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>class PixelDecoder:\n    \"\"\"\n    Retrieve and process one tile from qi2lab 3D widefield zarr structure.\n    Normalize codebook and data, perform plane-by-plane pixel decoding,\n    extract barcode features, and save to disk.\n\n    Parameters\n    ----------\n    datastore: qi2labDataStore\n        qi2labDataStore object\n    merfish_bits: int, default 16\n        number of merfish bits. Assumes that in codebook, MERFISH rounds are [0,merfish_bits].\n    num_gpus: int, default 1\n        number of GPUs to use for decoding. If &gt; 1, will split decoding across GPUs.\n    verbose: int, default 1\n        control verbosity. 0 - no output, 1 - tqdm bars, 2 - diagnostic outputs\n    use_mask: Optiona[bool], default False\n        use mask stored in polyDT directory\n    z_range: Optional[Sequence[int]], default None\n        z range to analyze. In integer indices from [0,N] where N is number of\n        z planes.\n    include_blanks: Optional[bool], default True\n        Include Blank codewords in decoding process.\n    \"\"\"\n\n    def __init__(\n        self,\n        datastore: qi2labDataStore,\n        merfish_bits: int = 16,\n        num_gpus: int = 1,\n        verbose: int = 1,\n        use_mask: Optional[bool] = False,\n        z_range: Optional[Sequence[int]] = None,\n        include_blanks: Optional[bool] = True,\n        smFISH: Optional[bool] = False\n    ):\n        self._datastore_path = Path(datastore._datastore_path)\n        self._datastore = datastore\n        self._num_gpus = num_gpus\n        self._verbose = verbose\n        self._barcodes_filtered = False\n        self._include_blanks = include_blanks\n        self._smFISH = smFISH\n\n        self._n_merfish_bits = merfish_bits\n\n        if self._datastore.microscope_type == \"2D\":\n            self._is_3D = False\n        else:\n            self._is_3D = True\n        if z_range is None:\n            self._z_crop = False\n        else:\n            self._z_crop = True\n            self._z_range = [z_range[0], z_range[1]]\n\n        self._load_codebook()\n        self._decoding_matrix_no_errors = self._normalize_codebook(include_errors=False)\n        self._decoding_matrix = self._decoding_matrix_no_errors.copy()\n        self._barcode_count = self._decoding_matrix.shape[0]\n        self._bit_count = self._decoding_matrix.shape[1]\n\n        if use_mask:\n            self._load_mask()  # TO DO: implement\n        else:\n            self._mask_image = None\n\n        self._codebook_style = 1\n        self._optimize_normalization_weights = False\n        self._global_normalization_loaded = False\n        self._iterative_normalization_loaded = False\n        self._distance_threshold = 0.5176 # default for HW4D4 code.\n\n    def _load_codebook(self):\n        \"\"\"Load and parse codebook into gene_id and codeword matrix.\"\"\"\n\n        self._df_codebook = self._datastore.codebook.copy()\n        self._df_codebook.fillna(0, inplace=True)\n\n        self._blank_count = (\n            self._df_codebook[\"gene_id\"].str.lower().str.startswith(\"blank\").sum()\n        )\n\n        if not (self._include_blanks):\n            self._df_codebook.drop(\n                self._df_codebook[self._df_codebook[0].str.startswith(\"Blank\")].index,\n                inplace=True,\n            )\n\n        self._codebook_matrix = self._df_codebook.iloc[:, 1:].to_numpy().astype(int)\n        self._gene_ids = self._df_codebook.iloc[:, 0].tolist()\n\n    def _normalize_codebook(self, gpu_id: int = 0, include_errors: bool = False):\n        \"\"\"Normalize each codeword by L2 norm.\n\n        Parameters\n        ----------\n        include_errors : bool, default False\n            Include single-bit errors as unique barcodes in the decoding matrix.\"\"\"\n\n        with cp.cuda.Device(gpu_id):\n            self._barcode_set = cp.asarray(\n                self._codebook_matrix[:, 0 : self._n_merfish_bits]\n            )\n            magnitudes = cp.linalg.norm(self._barcode_set, axis=1, keepdims=True)\n            magnitudes[magnitudes == 0] = 1  # ensure with smFISH rounds have magnitude 1\n\n            if not include_errors:\n                # Normalize directly using broadcasting\n                normalized_barcodes = self._barcode_set / magnitudes\n                return cp.asnumpy(normalized_barcodes)\n            else:\n                # Pre-compute the normalized barcodes\n                normalized_barcodes = self._barcode_set / magnitudes\n\n                # Initialize an empty list to hold all barcodes with single errors\n                barcodes_with_single_errors = [normalized_barcodes]\n\n                # Generate single-bit errors\n                for bit_index in range(self._barcode_set.shape[1]):\n                    flipped_barcodes = self._barcode_set.copy()\n                    flipped_barcodes[:, bit_index] = 1 - flipped_barcodes[:, bit_index]\n                    flipped_magnitudes = cp.sqrt(cp.sum(flipped_barcodes**2, axis=1))\n                    flipped_magnitudes = cp.where(\n                        flipped_magnitudes == 0, 1, flipped_magnitudes\n                    )\n                    normalized_flipped = flipped_barcodes / flipped_magnitudes\n                    barcodes_with_single_errors.append(normalized_flipped)\n\n                # Stack all barcodes (original normalized + with single errors)\n                all_barcodes = cp.vstack(barcodes_with_single_errors)\n\n                cp.cuda.Stream.null.synchronize()\n                cp.get_default_memory_pool().free_all_blocks()\n                cp.get_default_pinned_memory_pool().free_all_blocks()\n\n                return cp.asnumpy(all_barcodes)\n\n    def _load_global_normalization_vectors(self,gpu_id: int = 0):\n        \"\"\"Load or calculate global normalization and background vectors.\"\"\"\n        with cp.cuda.Device(gpu_id):\n            normalization_vector = self._datastore.global_normalization_vector\n            background_vector = self._datastore.global_background_vector\n            if (normalization_vector is not None and background_vector is not None):\n                self._global_normalization_vector = cp.asarray(normalization_vector)\n                self._global_background_vector = cp.asarray(background_vector)\n                self._global_normalization_loaded = True\n            else:\n                self._global_normalization_vectors()\n\n            cp.cuda.Stream.null.synchronize()\n            cp.get_default_memory_pool().free_all_blocks()\n            cp.get_default_pinned_memory_pool().free_all_blocks()\n\n    def _global_normalization_vectors(\n        self,\n        low_percentile_cut: float = 10.0,\n        high_percentile_cut: float = 90.0,\n        hot_pixel_threshold: int = 50000,\n        gpu_id: int = 0\n    ):\n        \"\"\"Calculate global normalization and background vectors.\n\n        Parameters\n        ----------\n        low_percentile_cut : float, default 10.0\n            Lower percentile cut for background estimation.\n        high_percentile_cut : float, default 90.0\n            Upper percentile cut for normalization estimation.\n        hot_pixel_threshold : int, default 50000\n            Threshold for hot pixel removal.\n        \"\"\"\n\n        with cp.cuda.Device(gpu_id):\n            if len(self._datastore.tile_ids) &gt; 5:\n                random_tiles = sample(self._datastore.tile_ids, 5)\n            else:\n                random_tiles = self._datastore.tile_ids\n\n            normalization_vector = cp.ones(len(self._datastore.bit_ids), dtype=cp.float32)\n            background_vector = cp.zeros(len(self._datastore.bit_ids), dtype=cp.float32)\n\n            if self._verbose &gt;= 1:\n                print(\"calculate normalizations\")\n                iterable_bits = enumerate(\n                    tqdm(self._datastore.bit_ids, desc=\"bit\", leave=False)\n                )\n            else:\n                iterable_bits = enumerate(self._datastore.bit_ids)\n\n            for bit_idx, bit_id in iterable_bits:\n                all_images = []\n\n                if self._verbose &gt;= 1:\n                    iterable_tiles = tqdm(random_tiles, desc=\"loading tiles\", leave=False)\n                else:\n                    iterable_tiles = random_tiles\n\n                for tile_id in iterable_tiles:\n                    decon_image = self._datastore.load_local_registered_image(\n                        tile=tile_id, bit=bit_id, return_future=False\n                    )\n                    ufish_image = self._datastore.load_local_ufish_image(\n                        tile=tile_id, bit=bit_id, return_future=False\n                    )\n\n                    current_image = cp.where(\n                        cp.asarray(ufish_image, dtype=cp.float32) &gt; 0.1,\n                        cp.asarray(decon_image, dtype=cp.float32),\n                        0.0,\n                    )\n                    current_image[current_image &gt; hot_pixel_threshold] = cp.median(\n                        current_image[current_image.shape[0] // 2, :, :]\n                    ).astype(cp.float32)\n                    if self._z_crop:\n                        all_images.append(\n                            cp.asnumpy(\n                                current_image[self._z_range[0] : self._z_range[1], :]\n                            ).astype(np.float32)\n                        )\n                    else:\n                        all_images.append(cp.asnumpy(current_image).astype(np.float32))\n                    del current_image\n                    cp.get_default_memory_pool().free_all_blocks()\n                    gc.collect()\n\n                all_images = np.array(all_images)\n\n                if self._verbose &gt;= 1:\n                    iterable_tiles = enumerate(\n                        tqdm(random_tiles, desc=\"background est.\", leave=False)\n                    )\n                else:\n                    iterable_tiles = enumerate(random_tiles)\n\n                low_pixels = []\n                for tile_idx, tile_id in iterable_tiles:\n                    current_image = cp.asarray(all_images[tile_idx, :], dtype=cp.float32)\n                    low_cutoff = cp.percentile(current_image, low_percentile_cut)\n                    low_pixels.append(\n                        current_image[current_image &lt; low_cutoff]\n                        .flatten()\n                        .astype(cp.float32)\n                    )\n                    del current_image\n                    cp.get_default_memory_pool().free_all_blocks()\n                    gc.collect()\n\n                low_pixels = cp.concatenate(low_pixels, axis=0)\n                if low_pixels.shape[0] &gt; 0:\n                    background_vector[bit_idx] = cp.median(low_pixels)\n                else:\n                    background_vector[bit_idx] = 0\n\n                del low_pixels\n                cp.get_default_memory_pool().free_all_blocks()\n                gc.collect()\n\n                if self._verbose &gt;= 1:\n                    iterable_tiles = enumerate(\n                        tqdm(random_tiles, desc=\"normalization est.\", leave=False)\n                    )\n                else:\n                    iterable_tiles = enumerate(random_tiles)\n\n                high_pixels = []\n                for tile_idx, tile_id in iterable_tiles:\n                    current_image = (\n                        cp.asarray(all_images[tile_idx, :], dtype=cp.float32)\n                        - background_vector[bit_idx]\n                    )\n                    current_image[current_image &lt; 0] = 0\n                    high_cutoff = cp.percentile(current_image, high_percentile_cut)\n                    high_pixels.append(\n                        current_image[current_image &gt; high_cutoff]\n                        .flatten()\n                        .astype(cp.float32)\n                    )\n\n                    del current_image\n                    cp.get_default_memory_pool().free_all_blocks()\n                    gc.collect()\n\n                high_pixels = cp.concatenate(high_pixels, axis=0)\n                if high_pixels.shape[0] &gt; 0:\n                    normalization_vector[bit_idx] = cp.median(high_pixels)\n                else:\n                    normalization_vector[bit_idx] = 1\n\n                del high_pixels\n                cp.get_default_memory_pool().free_all_blocks()\n                gc.collect()\n\n            self._datastore.global_normalization_vector = (\n                cp.asnumpy(normalization_vector).astype(np.float32).tolist()\n            )\n            self._datastore.global_background_vector = (\n                cp.asnumpy(background_vector).astype(np.float32).tolist()\n            )\n\n            self._global_background_vector = background_vector\n            self._global_normalization_vector = normalization_vector\n            self._global_normalization_loaded = True\n\n            cp.cuda.Stream.null.synchronize()\n            cp.get_default_memory_pool().free_all_blocks()\n            cp.get_default_pinned_memory_pool().free_all_blocks()\n\n    def _load_iterative_normalization_vectors(self,gpu_id: int = 0):\n        \"\"\"Load or calculate iterative normalization and background vectors.\"\"\"\n        with cp.cuda.Device(gpu_id):\n            normalization_vector = self._datastore.iterative_normalization_vector\n            background_vector = self._datastore.iterative_background_vector\n\n            if normalization_vector is not None and background_vector is not None:\n                background_vector = np.nan_to_num(background_vector, 0.0)\n                normalization_vector = np.nan_to_num(normalization_vector, 1.0)\n                self._iterative_normalization_vector = cp.asarray(normalization_vector)\n                self._iterative_background_vector = cp.asarray(background_vector)\n                self._iterative_normalization_loaded = True\n            else:\n                self._iterative_normalization_vectors()\n\n            cp.cuda.Stream.null.synchronize()\n            cp.get_default_memory_pool().free_all_blocks()\n            cp.get_default_pinned_memory_pool().free_all_blocks()\n\n    def _iterative_normalization_vectors(self, gpu_id: int = 0):\n        \"\"\"Calculate iterative normalization and background vectors.\"\"\"\n        with cp.cuda.Device(gpu_id):\n\n            keep = ~self._df_barcodes_loaded[\"gene_id\"].astype(\"string\").str.startswith(\"Blank\", na=False)\n\n            df_barcodes_loaded_no_blanks = self._df_barcodes_loaded[keep]\n\n            bit_columns = [\n                col\n                for col in df_barcodes_loaded_no_blanks.columns\n                if col.startswith(\"bit\") and col.endswith(\"_mean_intensity\")\n            ]\n\n            barcode_intensities = []\n            barcode_background = []\n            for index, row in df_barcodes_loaded_no_blanks.iterrows():\n\n                if self._smFISH == False:\n                    selected_columns = [\n                        f'bit{int(row[\"on_bit_1\"]):02d}_mean_intensity',\n                        f'bit{int(row[\"on_bit_2\"]):02d}_mean_intensity',\n                        f'bit{int(row[\"on_bit_3\"]):02d}_mean_intensity',\n                        f'bit{int(row[\"on_bit_4\"]):02d}_mean_intensity',\n                    ]\n                else:\n                    selected_columns = [\n                        f'bit{int(row[\"on_bit_1\"]):02d}_mean_intensity'\n                    ]\n\n                selected_dict = {\n                    col: (row[col] if col in selected_columns else None)\n                    for col in bit_columns\n                }\n                not_selected_dict = {\n                    col: (row[col] if col not in selected_columns else None)\n                    for col in bit_columns\n                }\n\n                barcode_intensities.append(selected_dict)\n                barcode_background.append(not_selected_dict)\n\n            df_barcode_intensities = pd.DataFrame(barcode_intensities)\n            df_barcode_background = pd.DataFrame(barcode_background)\n\n            df_barcode_intensities = df_barcode_intensities.reindex(\n                sorted(df_barcode_intensities.columns), axis=1\n            )\n            df_barcode_background = df_barcode_background.reindex(\n                sorted(df_barcode_background.columns), axis=1\n            )\n\n            barcode_based_normalization_vector = np.round(\n                df_barcode_intensities.median(skipna=True).to_numpy(\n                    dtype=np.float32, copy=True\n                ),\n                1,\n            )\n            barcode_based_background_vector = np.round(\n                df_barcode_background.median(skipna=True).to_numpy(\n                    dtype=np.float32, copy=True\n                ),\n                1,\n            )\n\n            barcode_based_normalization_vector = np.nan_to_num(\n                barcode_based_normalization_vector, 1.0\n            )\n            barcode_based_normalization_vector = np.where(\n                barcode_based_normalization_vector == 0.0,\n                1.0,\n                barcode_based_normalization_vector,\n            )\n            barcode_based_background_vector = np.nan_to_num(\n                barcode_based_background_vector, 0.0\n            )\n\n            if (\n                self._iterative_background_vector is None\n                and self._iterative_normalization_vector is None\n            ):\n                old_iterative_background_vector = np.round(\n                    cp.asnumpy(self._global_background_vector[0 : self._n_merfish_bits]), 1\n                )\n                old_iterative_normalization_vector = np.round(\n                    cp.asnumpy(self._global_normalization_vector[0 : self._n_merfish_bits]),\n                    1,\n                )\n            else:\n                old_iterative_background_vector = np.asarray(\n                    cp.asnumpy(self._iterative_background_vector)\n                )\n                old_iterative_normalization_vector = np.asarray(\n                    cp.asnumpy(self._iterative_normalization_vector)\n                )\n\n            diff_iterative_background_vector = np.round(\n                np.abs(barcode_based_background_vector - old_iterative_background_vector), 1\n            )\n            diff_iterative_normalization_vector = np.round(\n                np.abs(\n                    barcode_based_normalization_vector - old_iterative_normalization_vector\n                ),\n                1,\n            )\n            self._datastore.iterative_background_vector = (\n                barcode_based_background_vector.astype(np.float32)\n            )\n            self._datastore.iterative_normalization_vector = (\n                barcode_based_normalization_vector.astype(np.float32)\n            )\n\n            if self._verbose &gt; 1:\n                print(time_stamp(), \"Normalizations updated.\")\n                print(\"---\")\n                print(f\"Background delta: {diff_iterative_background_vector}\")\n                print(f\"Background estimate: {barcode_based_background_vector}\")\n                print(\"---\")\n                print(f\"Foreground delta: {diff_iterative_normalization_vector}\")\n                print(f\"Foreground estimate: {barcode_based_normalization_vector}\")\n                print(\"---\")\n                print(f\"Num. barcodes: {len(df_barcodes_loaded_no_blanks)}\")\n                print(\"---\")\n\n            self._iterative_normalization_vector = barcode_based_normalization_vector\n            self._iterative_background_vector = barcode_based_background_vector\n            self._datastore.iterative_normalization_vector = (\n                barcode_based_normalization_vector\n            )\n            self._datastore.iterative_background_vector = barcode_based_background_vector\n\n            self._iterative_normalization_loaded = True\n\n            del df_barcodes_loaded_no_blanks\n            gc.collect()\n            cp.cuda.Stream.null.synchronize()\n            cp.get_default_memory_pool().free_all_blocks()\n            cp.get_default_pinned_memory_pool().free_all_blocks()\n\n    def _load_bit_data(self, ufish_threshold: Optional[float] = 0.1):\n        \"\"\"Load raw data for all bits in the tile.\n\n        Parameters\n        ----------\n        ufish_threshold : Optional[float], default 0.5\n            Threshold for ufish image.\n        \"\"\"\n\n        if self._verbose &gt; 1:\n            print(\"load raw data\")\n            iterable_bits = tqdm(\n                self._datastore.bit_ids[0 : self._n_merfish_bits],\n                desc=\"bit\",\n                leave=False,\n            )\n        elif self._verbose &gt;= 1:\n            iterable_bits = tqdm(\n                self._datastore.bit_ids[0 : self._n_merfish_bits],\n                desc=\"loading\",\n                leave=False,\n            )\n        else:\n            iterable_bits = self._datastore.bit_ids[0 : self._n_merfish_bits]\n\n        images = []\n        self._em_wvl = []\n        for bit_id in iterable_bits:\n            decon_image = self._datastore.load_local_registered_image(\n                tile=self._tile_idx,\n                bit=bit_id,\n            )\n            ufish_image = self._datastore.load_local_ufish_image(\n                tile=self._tile_idx,\n                bit=bit_id,\n            )\n\n            if self._z_crop:\n                current_mask = np.asarray(\n                    ufish_image[self._z_range[0] : self._z_range[1], :].result(),\n                    dtype=np.float32,\n                )\n                images.append(\n                    np.where(\n                        current_mask &gt; ufish_threshold,\n                        np.asarray(\n                            decon_image[\n                                self._z_range[0] : self._z_range[1], :\n                            ].result(),\n                            dtype=np.float32,\n                        ),\n                        0,\n                    )\n                )\n            else:\n                current_mask = np.asarray(ufish_image.result(), dtype=np.float32)\n                images.append(\n                    np.where(\n                        current_mask &gt; ufish_threshold,\n                        np.asarray(decon_image.result(), dtype=np.float32),\n                        0,\n                    )\n                )\n            self._em_wvl.append(\n                self._datastore.load_local_wavelengths_um(\n                    tile=self._tile_idx,\n                    bit=bit_id,\n                )[1]\n            )\n\n        self._image_data = np.stack(images, axis=0)\n        voxel_size_zyx_um = self._datastore.voxel_size_zyx_um\n        self._pixel_size = voxel_size_zyx_um[1]\n        self._axial_step = voxel_size_zyx_um[0]\n\n        affine, origin, spacing = self._datastore.load_global_coord_xforms_um(\n            tile=self._tile_idx\n        )\n        if affine is None or origin is None or spacing is None:\n            if self._is_3D:\n                affine = np.eye(4)\n                origin = self._datastore.load_local_stage_position_zyx_um(\n                    tile=self._tile_idx, round=0\n                )\n                spacing = self._datastore.voxel_size_zyx_um\n            else:\n                affine = np.eye(4)\n                origin = self._datastore.load_local_stage_position_zyx_um(\n                    tile=self._tile_idx, round=0\n                )\n                origin = [0, origin[0], origin[1]]\n                spacing = self._datastore.voxel_size_zyx_um\n\n        self._affine = affine\n        self._origin = origin\n        self._spacing = spacing\n\n        del images\n        gc.collect()\n\n    def _lp_filter(self, gpu_id: int = 0, sigma=(3, 1, 1)):\n        \"\"\"Apply low-pass filter to the raw data.\n\n        Parameters\n        ----------\n        sigma : Tuple[int, int, int], default [3,1,1]\n            Sigma values for Gaussian filter.\n        \"\"\"\n\n        with cp.cuda.Device(gpu_id):\n            self._image_data_lp = self._image_data.copy()\n\n            if self._verbose &gt; 1:\n                print(\"lowpass filter\")\n                iterable_lp = tqdm(\n                    range(self._image_data_lp.shape[0]), desc=\"bit\", leave=False\n                )\n            elif self._verbose &gt;= 1:\n                iterable_lp = tqdm(\n                    range(self._image_data_lp.shape[0]), desc=\"lowpass\", leave=False\n                )\n            else:\n                iterable_lp = range(self._image_data_lp.shape[0])\n\n            for i in iterable_lp:\n                if self._is_3D:\n                    image_data_cp = cp.asarray(self._image_data[i, :], dtype=cp.float32)\n                    max_image_data = cp.asnumpy(\n                        cp.max(image_data_cp, axis=(0, 1, 2))\n                    ).astype(np.float32)\n                    if max_image_data == 0:\n                        self._image_data_lp[i, :, :, :] = 0\n                    else:\n                        self._image_data_lp[i, :, :, :] = cp.asnumpy(\n                            gaussian_filter(image_data_cp, sigma=sigma)\n                        ).astype(np.float32)\n                        max_image_data_lp = np.max(\n                            self._image_data_lp[i, :, :, :], axis=(0, 1, 2)\n                        )\n                        self._image_data_lp[i, :, :, :] = self._image_data_lp[\n                            i, :, :, :\n                        ] * (max_image_data / max_image_data_lp)\n                else:\n                    for z_idx in range(self._image_data.shape[1]):\n                        image_data_cp = cp.asarray(\n                            self._image_data[i, z_idx, :], dtype=cp.float32\n                        )\n                        max_image_data = cp.asnumpy(\n                            cp.max(image_data_cp, axis=(0, 1))\n                        ).astype(np.float32)\n                        if max_image_data == 0:\n                            self._image_data_lp[i, z_idx, :, :] = 0\n                        else:\n                            self._image_data_lp[i, z_idx, :, :] = cp.asnumpy(\n                                gaussian_filter(image_data_cp, sigma=(sigma[1], sigma[2]))\n                            ).astype(np.float32)\n                            max_image_data_lp = np.max(\n                                self._image_data_lp[i, z_idx, :, :], axis=(0, 1)\n                            )\n                            self._image_data_lp[i, z_idx, :, :] = self._image_data_lp[\n                                i, z_idx, :, :\n                            ] * (max_image_data / max_image_data_lp)\n\n            self._filter_type = \"lp\"\n\n            del image_data_cp\n            del self._image_data\n            gc.collect()\n            cp.cuda.Stream.null.synchronize()\n            cp.get_default_memory_pool().free_all_blocks()\n            cp.get_default_pinned_memory_pool().free_all_blocks()\n\n    @staticmethod\n    def _scale_pixel_traces(\n        pixel_traces: Union[np.ndarray, cp.ndarray],\n        background_vector: Union[np.ndarray, cp.ndarray],\n        normalization_vector: Union[np.ndarray, cp.ndarray],\n        merfish_bits=16,\n        gpu_id: int = 0\n    ) -&gt; cp.ndarray:\n        \"\"\"Scale pixel traces using background and normalization vectors.\n\n        Parameters\n        ----------\n        pixel_traces : Union[np.ndarray, cp.ndarray]\n            Pixel traces to scale.\n        background_vector : Union[np.ndarray, cp.ndarray]\n            Background vector.\n        normalization_vector : Union[np.ndarray, cp.ndarray]\n            Normalization vector.\n        merfish_bits : int = 16\n            Number of MERFISH bits. Default 16. Assume MERFISH bits are [0, merfish_bits].\n\n        Returns\n        -------\n        scaled_traces : cp.ndarray\n            Scaled pixel traces.\n        \"\"\"\n\n        with cp.cuda.Device(gpu_id):\n            if isinstance(pixel_traces, np.ndarray):\n                pixel_traces = cp.asarray(pixel_traces, dtype=cp.float32)\n            if isinstance(background_vector, np.ndarray):\n                background_vector = cp.asarray(background_vector, dtype=cp.float32)\n            if isinstance(normalization_vector, np.ndarray):\n                normalization_vector = cp.asarray(normalization_vector, dtype=cp.float32)\n\n            background_vector = background_vector[0:merfish_bits]\n            normalization_vector = normalization_vector[0:merfish_bits]\n\n            cp.cuda.Stream.null.synchronize()\n            cp.get_default_memory_pool().free_all_blocks()\n            cp.get_default_pinned_memory_pool().free_all_blocks()\n\n            return (pixel_traces - background_vector[:, cp.newaxis]) / normalization_vector[\n                :, cp.newaxis\n            ]\n\n    @staticmethod\n    def _clip_pixel_traces(\n        pixel_traces: Union[np.ndarray, cp.ndarray],\n        clip_lower: float = 0.0,\n        clip_upper: float = 1.0,\n        gpu_id: int = 0\n    ) -&gt; cp.ndarray:\n        \"\"\"Clip pixel traces to a range.\n\n        Parameters\n        ----------\n        pixel_traces : Union[np.ndarray, cp.ndarray]\n            Pixel traces to clip.\n        clip_lower : float, default 0.0\n            clip lower bound.\n        clip_upper : float, default 1.0\n            clip upper bound.\n\n        Returns\n        -------\n        clipped_traces : cp.ndarray\n            Clipped pixel traces.\n        \"\"\"\n        with cp.cuda.Device(gpu_id):\n\n            clipped = cp.clip(pixel_traces, clip_lower, clip_upper, pixel_traces)\n            cp.cuda.Stream.null.synchronize()\n            cp.get_default_memory_pool().free_all_blocks()\n            cp.get_default_pinned_memory_pool().free_all_blocks()\n            return clipped\n\n    @staticmethod\n    def _normalize_pixel_traces(\n        pixel_traces: Union[np.ndarray, cp.ndarray],\n        gpu_id: int = 0\n    ) -&gt; Tuple[cp.ndarray, cp.ndarray]:\n        \"\"\"Normalize pixel traces by L2 norm.\n\n        Parameters\n        ----------\n        pixel_traces : Union[np.ndarray, cp.ndarray]\n            Pixel traces to normalize.\n\n        Returns\n        -------\n        normalized_traces : cp.ndarray\n            Normalized pixel traces.\n        norms : cp.ndarray\n            L2 norms of pixel traces.    \n        \"\"\"\n\n        with cp.cuda.Device(gpu_id):\n            if isinstance(pixel_traces, np.ndarray):\n                pixel_traces = cp.asarray(pixel_traces, dtype=cp.float32)\n\n            norms = cp.linalg.norm(pixel_traces, axis=0)\n            norms = cp.where(norms == 0, np.inf, norms)\n            normalized_traces = pixel_traces / norms\n            norms = cp.where(norms == np.inf, -1, norms)\n\n            cp.cuda.Stream.null.synchronize()\n            cp.get_default_memory_pool().free_all_blocks()\n            cp.get_default_pinned_memory_pool().free_all_blocks()\n\n            return normalized_traces, norms\n\n    @staticmethod\n    def _calculate_distances(\n        pixel_traces: Union[np.ndarray, cp.ndarray],\n        codebook_matrix: Union[np.ndarray, cp.ndarray],\n        gpu_id: int = 0\n    ) -&gt; Tuple[cp.ndarray, cp.ndarray]:\n        \"\"\"Calculate distances between pixel traces and codebook matrix.\n\n        Parameters\n        ----------\n        pixel_traces : Union[np.ndarray, cp.ndarray]\n            Pixel traces.\n        codebook_matrix : Union[np.ndarray, cp.ndarray]\n            Codebook matrix.\n\n        Returns\n        -------\n        min_distances : cp.ndarray\n            Minimum distances.\n        min_indices : cp.ndarray\n            Minimum indices.\n        \"\"\"\n\n        with cp.cuda.Device(gpu_id):\n            if isinstance(pixel_traces, np.ndarray):\n                pixel_traces = cp.asarray(pixel_traces, dtype=cp.float32)\n            if isinstance(codebook_matrix, np.ndarray):\n                codebook_matrix = cp.asarray(codebook_matrix, dtype=cp.float32)\n\n            distances = cp.ascontiguousarray(\n                cp.zeros((pixel_traces.shape[1], codebook_matrix.shape[0]), dtype=cp.float32)\n            )\n            pairwise_distance(\n                cp.ascontiguousarray(pixel_traces.T),\n                cp.ascontiguousarray(codebook_matrix),\n                metric=\"euclidean\",\n                out=distances\n            )\n\n            min_indices = cp.argmin(distances, axis=1)\n            min_distances = cp.min(distances, axis=1)\n\n            del pixel_traces, codebook_matrix\n            gc.collect()\n            cp.cuda.Stream.null.synchronize()\n            cp.get_default_memory_pool().free_all_blocks()\n            cp.get_default_pinned_memory_pool().free_all_blocks()\n\n            return min_distances, min_indices\n\n    def _decode_pixels(\n        self, distance_threshold: float = 0.5176, \n        magnitude_threshold: Sequence[float] = (1.1, 2.0),\n        gpu_id: int = 0\n    ):\n        \"\"\"Decode pixels using the decoding matrix.\n\n        Parameters\n        ----------\n        distance_threshold : float, default 0.5176.\n            Distance threshold for decoding. The default is for a 4-bit,\n            4-distance Hamming codebook.\n        magnitude_threshold : Sequence[float], default (1.1, 2.0).\n            Magnitude threshold for decoding. \n        \"\"\"\n\n        with cp.cuda.Device(gpu_id):\n            if self._filter_type == \"lp\":\n                original_shape = self._image_data_lp.shape\n                self._decoded_image = np.zeros((original_shape[1:]), dtype=np.int16)\n                self._magnitude_image = np.zeros((original_shape[1:]), dtype=np.float16)\n                self._scaled_pixel_images = np.zeros((original_shape), dtype=np.float16)\n                self._distance_image = np.zeros((original_shape[1:]), dtype=np.float16)\n            else:\n                original_shape = self._image_data.shape\n                self._decoded_image = np.zeros((original_shape[1:]), dtype=np.int16)\n                self._magnitude_image = np.zeros((original_shape[1:]), dtype=np.float16)\n                self._scaled_pixel_images = np.zeros((original_shape), dtype=np.float16)\n                self._distance_image = np.zeros((original_shape[1:]), dtype=np.float16)\n\n            if self._verbose &gt; 1:\n                print(\"decode pixels\")\n                iterable_z = tqdm(range(original_shape[1]), desc=\"z\", leave=False)\n            elif self._verbose &gt;= 1:\n                iterable_z = tqdm(range(original_shape[1]), desc=\"decoding\", leave=False)\n            else:\n                iterable_z = range(original_shape[1])\n\n            for z_idx in iterable_z:\n                if self._filter_type == \"lp\":\n                    z_plane_shape = self._image_data_lp[:, z_idx, :].shape\n                    scaled_pixel_traces = (\n                        cp.asarray(self._image_data_lp[:, z_idx, :])\n                        .reshape(self._n_merfish_bits, -1)\n                        .astype(cp.float32)\n                    )\n                else:\n                    z_plane_shape = self._image_data[:, z_idx, :].shape\n                    scaled_pixel_traces = (\n                        cp.asarray(self._image_data[:, z_idx, :])\n                        .reshape(self._n_merfish_bits, -1)\n                        .astype(cp.float32)\n                    )\n\n                if self._iterative_normalization_loaded:\n                    scaled_pixel_traces = self._scale_pixel_traces(\n                        scaled_pixel_traces,\n                        self._iterative_background_vector,\n                        self._iterative_normalization_vector,\n                        self._n_merfish_bits,\n                        gpu_id=gpu_id\n                    )\n                elif self._global_normalization_loaded:\n                    scaled_pixel_traces = self._scale_pixel_traces(\n                        scaled_pixel_traces,\n                        self._global_background_vector,\n                        self._global_normalization_vector,\n                        self._n_merfish_bits,\n                        gpu_id=gpu_id\n                    )\n\n                scaled_pixel_traces = self._clip_pixel_traces(scaled_pixel_traces,gpu_id=gpu_id)\n                normalized_pixel_traces, pixel_magnitude_trace = (\n                    self._normalize_pixel_traces(scaled_pixel_traces,gpu_id=gpu_id)\n                )\n                distance_trace, codebook_index_trace = self._calculate_distances(\n                    normalized_pixel_traces, self._decoding_matrix,gpu_id=gpu_id\n                )\n\n                del normalized_pixel_traces\n                gc.collect()\n                cp.cuda.Stream.null.synchronize()\n                cp.get_default_memory_pool().free_all_blocks()\n                cp.get_default_pinned_memory_pool().free_all_blocks()\n\n                decoded_trace = cp.full(distance_trace.shape[0], -1, dtype=cp.int16)\n                mask_trace = distance_trace &lt; distance_threshold\n                decoded_trace[mask_trace] = codebook_index_trace[mask_trace]\n                decoded_trace[pixel_magnitude_trace &lt; magnitude_threshold[0]] = -1\n                decoded_trace[pixel_magnitude_trace &gt; magnitude_threshold[1]] = -1\n\n                self._decoded_image[z_idx, :] = cp.asnumpy(\n                    cp.reshape(cp.round(decoded_trace, 5), z_plane_shape[1:])\n                )\n                self._magnitude_image[z_idx, :] = cp.asnumpy(\n                    cp.reshape(cp.round(pixel_magnitude_trace, 5), z_plane_shape[1:])\n                )\n                self._scaled_pixel_images[:, z_idx, :] = cp.asnumpy(\n                    cp.reshape(cp.round(scaled_pixel_traces, 5), z_plane_shape)\n                )\n                self._distance_image[z_idx, :] = cp.asnumpy(\n                    cp.reshape(cp.round(distance_trace, 5), z_plane_shape[1:])\n                )\n\n                del (\n                    decoded_trace,\n                    pixel_magnitude_trace,\n                    scaled_pixel_traces,\n                    distance_trace,\n                )\n                gc.collect()\n                cp.cuda.Stream.null.synchronize()\n                cp.get_default_memory_pool().free_all_blocks()\n                cp.get_default_pinned_memory_pool().free_all_blocks()\n\n    @staticmethod\n    def _warp_pixel(\n        pixel_space_point: np.ndarray,\n        spacing: np.ndarray,\n        origin: np.ndarray,\n        affine: np.ndarray,\n    ) -&gt; np.ndarray:\n        \"\"\"Warp pixel space point to physical space point.\n\n        Parameters\n        ----------\n        pixel_space_point : np.ndarray\n            Pixel space point.\n        spacing : np.ndarray\n            Spacing.\n        origin : np.ndarray\n            Origin.\n        affine : np.ndarray \n            Affine transformation matrix.\n\n        Returns\n        -------\n        registered_space_point : np.ndarray\n            Registered space point.\n        \"\"\"\n\n        physical_space_point = pixel_space_point * spacing + origin\n        registered_space_point = (\n            np.array(affine) @ np.array(list(physical_space_point) + [1])\n        )[:-1]\n\n\n\n        return registered_space_point\n\n    def _extract_barcodes(\n        self, \n        minimum_pixels: int = 9, \n        maximum_pixels: int = 1000,\n        gpu_id: int = 0\n    ):\n        \"\"\"Extract barcodes from decoded image.\n\n        Parameters\n        ----------\n        minimum_pixels : int, default 9\n            Minimum number of pixels for a barcode. \n        maximum_pixels : int, default 1000\n            Maximum number of pixels for a barcode. \n        \"\"\"\n\n        self._df_barcodes = pd.DataFrame()\n\n        with cp.cuda.Device(gpu_id):\n            if self._verbose &gt; 1:\n                print(\"extract barcodes\")\n            if self._verbose &gt;= 1:\n                iterable_barcode = tqdm(\n                    range(self._codebook_matrix.shape[0]), desc=\"barcode\", leave=False\n                )\n            else:\n                iterable_barcode = range(self._codebook_matrix.shape[0])\n            decoded_image = cp.asarray(self._decoded_image, dtype=cp.int16)\n            if self._optimize_normalization_weights:\n                if self._filter_type == \"lp\":\n                    intensity_image = np.concatenate(\n                        [np.expand_dims(self._distance_image, axis=0), self._image_data_lp],\n                        axis=0,\n                    ).transpose(1, 2, 3, 0)\n                else:\n                    intensity_image = np.concatenate(\n                        [np.expand_dims(self._distance_image, axis=0), self._image_data],\n                        axis=0,\n                    ).transpose(1, 2, 3, 0)\n            else:\n                intensity_image = np.concatenate(\n                    [\n                        np.expand_dims(self._distance_image, axis=0),\n                        self._scaled_pixel_images,\n                    ],\n                    axis=0,\n                ).transpose(1, 2, 3, 0)\n\n            for barcode_index in iterable_barcode:\n                on_bits_indices = np.where(self._codebook_matrix[barcode_index])[0]\n\n                if len(on_bits_indices) == 1 and not(self._smFISH):\n                    break\n\n                if self._is_3D:\n                    if self._verbose &gt; 1:\n                        print(\"\")\n                        print(\"label image\")\n                    labeled_image = label(decoded_image == barcode_index, connectivity=3)\n\n                    if self._verbose &gt; 1:\n                        print(\"remove large\")\n                    pixel_counts = cp.bincount(labeled_image.ravel())\n                    large_labels = cp.where(pixel_counts &gt;= maximum_pixels)[0]\n                    large_label_mask = cp.zeros_like(labeled_image, dtype=bool)\n                    large_label_mask = cp.isin(labeled_image, large_labels)\n                    labeled_image[large_label_mask] = 0\n\n                    if self._verbose &gt; 1:\n                        print(\"remove small\")\n                    labeled_image = remove_small_objects(\n                        labeled_image, min_size=(minimum_pixels - 1), connectivity=3\n                    )\n                    if self._verbose &gt; 1:\n                        print(\"regionprops table\")\n\n                    labeled_image = cp.asnumpy(labeled_image).astype(np.int64)\n\n                    props = regionprops_table(\n                        labeled_image,\n                        intensity_image=intensity_image,\n                        properties=[\n                            \"label\",\n                            \"area\",\n                            \"centroid\",\n                            \"intensity_mean\",\n                            \"inertia_tensor_eigvals\",\n                        ]\n                    )\n                    df_barcode = pd.DataFrame(props)\n\n                    props_magnitude = regionprops_table(\n                        labeled_image,\n                        intensity_image=self._magnitude_image,\n                        properties=[\n                            \"label\",\n                            \"intensity_mean\",\n                        ]\n                    )\n                    df_magnitude = pd.DataFrame(props_magnitude)\n\n                    del labeled_image, props, props_magnitude\n                    gc.collect()\n                    cp.cuda.Stream.null.synchronize()\n                    cp.get_default_memory_pool().free_all_blocks()\n                    cp.get_default_pinned_memory_pool().free_all_blocks()\n\n                    df_magnitude = df_magnitude.rename(\n                        columns={'intensity_mean': 'magnitude_mean'}\n                    )\n                    df_barcode = df_barcode.merge(\n                        df_magnitude[[\"label\", \"magnitude_mean\"]],\n                        on=\"label\",\n                        how=\"left\",\n                    )\n\n                    df_barcode.drop(columns=\"label\", inplace=True)\n                    df_barcode = df_barcode[df_barcode[\"area\"] &gt; 0.1].reset_index(drop=True)\n\n\n                    if self._smFISH == False:\n                        df_barcode[\"on_bit_1\"] = on_bits_indices[0] + 1\n                        df_barcode[\"on_bit_2\"] = on_bits_indices[1] + 1\n                        df_barcode[\"on_bit_3\"] = on_bits_indices[2] + 1\n                        df_barcode[\"on_bit_4\"] = on_bits_indices[3] + 1\n                    else:\n                        df_barcode[\"on_bit_1\"] = on_bits_indices[0] + 1\n                    df_barcode[\"barcode_id\"] = df_barcode.apply(\n                        lambda x: (barcode_index + 1), axis=1\n                    )\n                    df_barcode[\"gene_id\"] = df_barcode.apply(\n                        lambda x: self._gene_ids[barcode_index], axis=1\n                    )\n                    df_barcode[\"tile_idx\"] = self._tile_idx\n\n                    df_barcode.rename(columns={\"centroid-0\": \"z\"}, inplace=True)\n                    df_barcode.rename(columns={\"centroid-1\": \"y\"}, inplace=True)\n                    df_barcode.rename(columns={\"centroid-2\": \"x\"}, inplace=True)\n\n                    if self._z_crop:\n                        df_barcode[\"z\"] = df_barcode[\"z\"] + self._z_range[0]\n\n                    df_barcode[\"tile_z\"] = np.round(df_barcode[\"z\"], 0).astype(int)\n                    df_barcode[\"tile_y\"] = np.round(df_barcode[\"y\"], 0).astype(int)\n                    df_barcode[\"tile_x\"] = np.round(df_barcode[\"x\"], 0).astype(int)\n                    pts = df_barcode[[\"z\", \"y\", \"x\"]].to_numpy()\n                    for pt_idx, pt in enumerate(pts):\n                        pts[pt_idx, :] = self._warp_pixel(\n                            pts[pt_idx, :].copy(), self._spacing, self._origin, self._affine\n                        )\n\n                    df_barcode[\"global_z\"] = np.round(pts[:, 0], 2)\n                    df_barcode[\"global_y\"] = np.round(pts[:, 1], 2)\n                    df_barcode[\"global_x\"] = np.round(pts[:, 2], 2)\n\n                    df_barcode.rename(\n                        columns={\"intensity_mean-0\": \"distance_mean\"}, inplace=True\n                    )\n                    for i in range(1, self._n_merfish_bits + 1):\n                        df_barcode.rename(\n                            columns={\n                                \"intensity_mean-\" + str(i): \"bit\"\n                                + str(i).zfill(2)\n                                + \"_mean_intensity\"\n                            },\n                            inplace=True,\n                        )\n\n                    on_bits = on_bits_indices + np.ones(4)\n\n                    signal_mean_columns = [\n                        f\"bit{int(bit):02d}_mean_intensity\" for bit in on_bits\n                    ]\n                    bkd_mean_columns = [\n                        f\"bit{int(bit):02d}_mean_intensity\"\n                        for bit in range(1, self._n_merfish_bits + 1)\n                        if bit not in on_bits\n                    ]\n\n                    df_barcode[\"signal_mean\"] = df_barcode[signal_mean_columns].mean(axis=1)\n                    df_barcode[\"bkd_mean\"] = df_barcode[bkd_mean_columns].mean(axis=1)\n                    df_barcode[\"s-b_mean\"] = (\n                        df_barcode[\"signal_mean\"] - df_barcode[\"bkd_mean\"]\n                    )\n\n                    if self._verbose &gt; 1:\n                        print(\"dataframe aggregation\")\n                    if barcode_index == 0:\n                        self._df_barcodes = df_barcode.copy()\n                    else:\n                        if not df_barcode.empty:\n                            self._df_barcodes = pd.concat([self._df_barcodes, df_barcode])\n                            self._df_barcodes.reset_index(drop=True, inplace=True)\n\n                    del df_barcode\n                    gc.collect()\n                else:\n                    if self._verbose &gt; 1:\n                        print(\"\")\n                        print(\"label image\")\n\n                    from cupyx.scipy import ndimage as cpx_ndi\n                    structure = cp.zeros((3, 3, 3), dtype=cp.uint8)\n                    structure[1, :, :] = 1  # only same-Z neighbors are connected\n                    structure[1, 0, 0] = 0\n                    structure[1, 0, 2] = 0\n                    structure[1, 2, 0] = 0\n                    structure[1, 2, 2] = 0\n                    labeled_image, _ = cpx_ndi.label(decoded_image == barcode_index, structure=structure)\n\n                    if self._verbose &gt; 1:\n                        print(\"remove large\")\n                    pixel_counts = cp.bincount(labeled_image.ravel())\n                    large_labels = cp.where(pixel_counts &gt; maximum_pixels)[0]\n                    large_label_mask = cp.zeros_like(labeled_image, dtype=bool)\n                    large_label_mask = cp.isin(labeled_image, large_labels)\n                    labeled_image[large_label_mask] = 0\n\n                    if self._verbose &gt; 1:\n                        print(\"remove small\")\n                    labeled_image = remove_small_objects(\n                        labeled_image, min_size=minimum_pixels\n                    )\n                    if self._verbose &gt; 1:\n                        print(\"regionprops table\")\n\n                    labeled_image = cp.asnumpy(labeled_image).astype(np.int64)\n                    props = regionprops_table(\n                        labeled_image,\n                        intensity_image=intensity_image,\n                        properties=[\n                            \"label\",\n                            \"area\",\n                            \"centroid\",\n                            \"intensity_mean\",\n                            \"inertia_tensor_eigvals\",\n                        ],\n                    )\n                    df_barcode = pd.DataFrame(props)\n\n                    props_magnitude = regionprops_table(\n                        labeled_image,\n                        intensity_image=self._magnitude_image,\n                        properties=[\n                            \"label\",\n                            \"intensity_mean\",\n                        ]\n                    )\n                    df_magnitude = pd.DataFrame(props_magnitude)\n\n                    del labeled_image, props, props_magnitude\n                    gc.collect()\n                    cp.cuda.Stream.null.synchronize()\n                    cp.get_default_memory_pool().free_all_blocks()\n                    cp.get_default_pinned_memory_pool().free_all_blocks()\n\n                    if not (df_magnitude.index.empty):\n                        df_magnitude = df_magnitude.rename(\n                            columns={'intensity_mean': 'magnitude_mean'}\n                        )\n                        df_barcode = df_barcode.merge(\n                            df_magnitude[[\"label\", \"magnitude_mean\"]],\n                            on=\"label\",\n                            how=\"left\",\n                        )\n                        df_barcode.drop(columns=\"label\", inplace=True)\n\n                    df_barcode = df_barcode[df_barcode[\"area\"] &gt; 0.1].reset_index(drop=True)\n\n                    if self._smFISH == False:\n                        df_barcode[\"on_bit_1\"] = on_bits_indices[0] + 1\n                        df_barcode[\"on_bit_2\"] = on_bits_indices[1] + 1\n                        df_barcode[\"on_bit_3\"] = on_bits_indices[2] + 1\n                        df_barcode[\"on_bit_4\"] = on_bits_indices[3] + 1\n                    else:\n                        df_barcode[\"on_bit_1\"] = on_bits_indices[0] + 1\n                    df_barcode[\"barcode_id\"] = df_barcode.apply(\n                        lambda x: (barcode_index + 1), axis=1\n                    )\n                    df_barcode[\"gene_id\"] = df_barcode.apply(\n                        lambda x: self._gene_ids[barcode_index], axis=1\n                    )\n                    df_barcode[\"tile_idx\"] = self._tile_idx\n\n\n                    df_barcode.rename(columns={\"centroid-0\": \"z\"}, inplace=True)\n                    df_barcode.rename(columns={\"centroid-1\": \"y\"}, inplace=True)\n                    df_barcode.rename(columns={\"centroid-2\": \"x\"}, inplace=True)\n\n                    if self._z_crop:\n                        df_barcode[\"z\"] = df_barcode[\"z\"] + self._z_range[0]\n\n                    df_barcode[\"tile_z\"] = np.round(df_barcode[\"z\"], 0).astype(int)\n                    df_barcode[\"tile_y\"] = np.round(df_barcode[\"y\"], 0).astype(int)\n                    df_barcode[\"tile_x\"] = np.round(df_barcode[\"x\"], 0).astype(int)\n\n                    pts = df_barcode[[\"z\", \"y\", \"x\"]].to_numpy()\n                    for pt_idx, pt in enumerate(pts):\n                        pts[pt_idx, :] = self._warp_pixel(\n                            pts[pt_idx, :].copy(),\n                            self._spacing,\n                            self._origin,\n                            self._affine,\n                        )\n\n                    df_barcode[\"global_z\"] = np.round(pts[:, 0], 2)\n                    df_barcode[\"global_y\"] = np.round(pts[:, 1], 2)\n                    df_barcode[\"global_x\"] = np.round(pts[:, 2], 2)\n\n                    df_barcode.rename(\n                        columns={\"intensity_mean-0\": \"distance_mean\"}, inplace=True\n                    )\n                    for i in range(1, self._n_merfish_bits + 1):\n                        df_barcode.rename(\n                            columns={\n                                \"intensity_mean-\" + str(i): \"bit\"\n                                + str(i).zfill(2)\n                                + \"_mean_intensity\"\n                            },\n                            inplace=True,\n                        )\n\n                    on_bits = on_bits_indices + np.ones(4)\n\n                    signal_mean_columns = [\n                        f\"bit{int(bit):02d}_mean_intensity\" for bit in on_bits\n                    ]\n                    bkd_mean_columns = [\n                        f\"bit{int(bit):02d}_mean_intensity\"\n                        for bit in range(1, self._n_merfish_bits + 1)\n                        if bit not in on_bits\n                    ]\n\n                    df_barcode[\"signal_mean\"] = df_barcode[signal_mean_columns].mean(\n                        axis=1\n                    )\n                    df_barcode[\"bkd_mean\"] = df_barcode[bkd_mean_columns].mean(axis=1)\n                    df_barcode[\"s-b_mean\"] = (\n                        df_barcode[\"signal_mean\"] - df_barcode[\"bkd_mean\"]\n                    )\n\n                    if self._verbose &gt; 1:\n                        print(\"dataframe aggregation\")\n                    if barcode_index == 0:\n                        self._df_barcodes = df_barcode.copy()\n                    else:\n                        if not df_barcode.empty:\n                            self._df_barcodes = pd.concat([self._df_barcodes, df_barcode])\n                            self._df_barcodes.reset_index(drop=True, inplace=True)\n\n                    del df_barcode\n                    gc.collect()\n\n            del decoded_image, intensity_image\n            gc.collect()\n            cp.cuda.Stream.null.synchronize()\n            cp.get_default_memory_pool().free_all_blocks()\n            cp.get_default_pinned_memory_pool().free_all_blocks()\n\n    def _save_barcodes(self):\n        \"\"\"Save barcodes to datastore.\"\"\"\n\n        if self._verbose &gt; 1:\n            print(\"save barcodes\")\n\n        if self._optimize_normalization_weights:\n            decoded_dir_path = self._temp_dir\n            decoded_dir_path.mkdir(parents=True, exist_ok=True)\n            temp_decoded_path = decoded_dir_path / Path(\n                \"tile\" + str(self._tile_idx).zfill(3) + \"_temp_decoded.parquet\"\n            )\n            self._df_barcodes.to_parquet(temp_decoded_path)\n        else:\n            if not (self._barcodes_filtered):\n                self._datastore.save_local_decoded_spots(\n                    self._df_barcodes, tile=self._tile_idx\n                )\n            else:\n                self._datastore.save_global_filtered_decoded_spots(\n                    self._df_filtered_barcodes\n                )\n\n    def _reformat_barcodes_for_baysor(self):\n        \"\"\"Reformat barcodes for Baysor and save to datastore.\"\"\"\n\n        if self._barcodes_filtered:\n            missing_columns = [\n                col\n                for col in [\n                    \"gene_id\",\n                    \"global_z\",\n                    \"global_y\",\n                    \"global_x\",\n                    \"cell_id\",\n                    \"tile_idx\",\n                    \"distance_mean\",\n                ]\n                if col not in self._df_filtered_barcodes.columns\n            ]\n            if missing_columns:\n                print(f\"The following columns are missing: {missing_columns}\")\n            baysor_df = self._df_filtered_barcodes[\n                [\n                    \"gene_id\",\n                    \"global_z\",\n                    \"global_y\",\n                    \"global_x\",\n                    \"cell_id\",\n                    \"tile_idx\",\n                    \"distance_mean\",\n                ]\n            ].copy()\n            baysor_df.rename(\n                columns={\n                    \"gene_id\": \"feature_name\",\n                    \"global_x\": \"x_location\",\n                    \"global_y\": \"y_location\",\n                    \"global_z\": \"z_location\",\n                    \"barcode_id\": \"codeword_index\",\n                    \"tile_idx\": \"fov_name\",\n                    \"distance_mean\": \"qv\",\n                },\n                inplace=True,\n            )\n\n            baysor_df[\"cell_id\"] = baysor_df[\"cell_id\"] + 1\n            baysor_df[\"transcript_id\"] = pd.util.hash_pandas_object(\n                baysor_df, index=False\n            )\n            baysor_df[\"is_gene\"] = ~baysor_df[\"feature_name\"].str.contains(\n                \"Blank\", na=False\n            )\n            self._datastore.save_spots_prepped_for_baysor(baysor_df)\n\n    def _load_all_barcodes(self):\n        \"\"\"Load all barcodes from datastore.\"\"\"\n\n        if self._optimize_normalization_weights:\n            decoded_dir_path = self._temp_dir\n\n            tile_files = decoded_dir_path.glob(\"*.parquet\")\n            tile_files = sorted(tile_files, key=lambda x: x.name)\n\n            if self._verbose &gt;= 1:\n                iterable_files = tqdm(tile_files, desc=\"tile\", leave=False)\n            else:\n                iterable_files = tile_files\n\n            tile_data = [\n                pd.read_parquet(parquet_file) for parquet_file in iterable_files\n            ]\n            self._df_barcodes_loaded = pd.concat(tile_data)\n        elif self._load_tile_decoding:\n            tile_data = []\n            for tile_id in self._datastore.tile_ids:\n                tile_data.append(self._datastore.load_local_decoded_spots(tile_id))\n            self._df_barcodes_loaded = pd.concat(tile_data)\n        else:\n            self._df_filtered_barcodes = (\n                self._datastore.load_global_filtered_decoded_spots()\n            )\n            self._barcodes_filtered = True\n\n        self._df_barcodes_loaded = self._df_barcodes_loaded[self._df_barcodes_loaded[\"gene_id\"].notna() &amp; self._df_barcodes_loaded[\"gene_id\"].astype(str).str.strip().ne(\"\")]\n\n    @staticmethod\n    def calculate_fdr(\n        df: pd.DataFrame, \n        threshold: float, \n        blank_count: int, \n        barcode_count: int, \n        verbose: bool = False) -&gt; float:\n        \"\"\"Calculate false discovery rate.\n\n        (# noncoding found ) / (# noncoding in codebook) / (# coding found) / (# coding in codebook)\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n            Dataframe containing decoded spots.\n        threshold : float\n            Threshold for predicted probability.\n        blank_count : int\n            Number of blank barcodes.\n        barcode_count : int\n            Number of barcodes.\n        verbose : bool = False\n            Verbose output. Default False.\n\n        Returns\n        -------\n        fdr : float\n            False discovery rate.\n        \"\"\"\n\n        if threshold &gt;= 0:\n            df[\"prediction\"] = df[\"predicted_probability\"] &gt; threshold\n\n            coding = df[\n                (~df[\"gene_id\"].str.startswith(\"Blank\"))\n                &amp; (df[\"predicted_probability\"] &gt; threshold)\n            ].shape[0]\n            noncoding = df[\n                (df[\"gene_id\"].str.startswith(\"Blank\"))\n                &amp; (df[\"predicted_probability\"] &gt; threshold)\n            ].shape[0]\n        else:\n            coding = df[(~df[\"gene_id\"].str.startswith(\"Blank\"))].shape[0]\n            noncoding = df[(df[\"gene_id\"].str.startswith(\"Blank\"))].shape[0]\n\n        if coding &gt; 0:\n            fdr = (noncoding / blank_count) / (coding / (barcode_count - blank_count))\n        else:\n            fdr = np.inf\n\n        if verbose &gt; 1:\n            print(f\"threshold: {threshold}\")\n            print(f\"coding: {coding}\")\n            print(f\"noncoding: {noncoding}\")\n            print(f\"fdr: {fdr}\")\n\n        return fdr\n\n    def _filter_all_barcodes(self, fdr_target: float = 0.05):\n        \"\"\"Filter barcodes using a classifier and FDR target.\n\n        Uses a MLP classifier to predict whether a barcode is a blank or not.\n\n        TO DO: evaluate other classifiers.\n\n        Parameters\n        ----------\n        fdr_target : float, default 0.05\n            False discovery rate target. \n        \"\"\"\n\n        from sklearn.model_selection import train_test_split\n        from sklearn.preprocessing import StandardScaler\n        from sklearn.neural_network import MLPClassifier\n        from sklearn.metrics import classification_report\n        from imblearn.over_sampling import SMOTE\n\n        self._df_barcodes_loaded[\"X\"] = ~self._df_barcodes_loaded[\n            \"gene_id\"\n        ].str.startswith(\"Blank\")\n        if self._is_3D:\n            columns = [\n                \"X\",\n                \"signal_mean\",\n                \"s-b_mean\",\n                \"distance_mean\",\n                \"magnitude_mean\",\n                \"inertia_tensor_eigvals-0\",\n                \"inertia_tensor_eigvals-1\",\n                \"inertia_tensor_eigvals-2\",\n            ]\n        else:\n            columns = [\n                \"X\",\n                \"signal_mean\",\n                \"s-b_mean\",\n                \"distance_mean\",\n                \"magnitude_mean\",\n                \"inertia_tensor_eigvals-0\",\n                \"inertia_tensor_eigvals-1\",\n            ]\n        df_true = self._df_barcodes_loaded[self._df_barcodes_loaded[\"X\"] == True][ #noqa\n            columns\n        ]  # noqa\n        df_false = self._df_barcodes_loaded[self._df_barcodes_loaded[\"X\"] == False][ #noqa\n            columns\n        ]  # noqa\n\n        if len(df_false) &gt; 0:\n            df_true_sampled = df_true.sample(n=len(df_false), random_state=42)\n            df_combined = pd.concat([df_true_sampled, df_false])\n            x = df_combined.drop(\"X\", axis=1)\n            y = df_combined[\"X\"]\n            X_train, X_test, y_train, y_test = train_test_split(\n                x, y, test_size=0.1, random_state=42\n            )\n\n            if self._verbose &gt; 1:\n                print(\"generating synthetic samples for class balance\")\n            smote = SMOTE(random_state=42)\n            X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\n            if self._verbose &gt; 1:\n                print(\"scaling features\")\n            scaler = StandardScaler()\n            X_train_scaled = scaler.fit_transform(X_train_resampled)\n            X_test_scaled = scaler.transform(X_test)\n\n            if self._verbose &gt; 1:\n                print(\"training classifier\")\n            # logistic = LogisticRegression(solver='liblinear', random_state=42)\n            mlp = MLPClassifier(solver=\"adam\", max_iter=10000, random_state=42)\n            mlp.fit(X_train_scaled, y_train_resampled)\n            predictions = mlp.predict(X_test_scaled)\n\n            if self._verbose &gt; 1:\n                print(classification_report(y_test, predictions))\n\n            if self._verbose &gt; 1:\n                print(\"predicting on full data\")\n\n            full_data_scaled = scaler.transform(self._df_barcodes_loaded[columns[1:]])\n            self._df_barcodes_loaded[\"predicted_probability\"] = mlp.predict_proba(\n                full_data_scaled\n            )[:, 1]\n\n            if self._verbose &gt; 1:\n                print(\"filtering blanks\")\n\n            coarse_threshold = 0\n            for threshold in np.arange(0, 1, 0.1):  # Coarse step: 0.1\n                fdr = self.calculate_fdr(\n                    self._df_barcodes_loaded,\n                    threshold,\n                    self._blank_count,\n                    self._barcode_count,\n                    self._verbose,\n                )\n                if fdr &lt;= fdr_target:\n                    coarse_threshold = threshold\n                    break\n\n            fine_threshold = coarse_threshold\n            for threshold in np.arange(\n                coarse_threshold - 0.1, coarse_threshold + 0.1, 0.01\n            ):\n                fdr = self.calculate_fdr(\n                    self._df_barcodes_loaded,\n                    threshold,\n                    self._blank_count,\n                    self._barcode_count,\n                    self._verbose,\n                )\n                if fdr &lt;= fdr_target:\n                    fine_threshold = threshold\n                    break\n\n            df_above_threshold = self._df_barcodes_loaded[\n                self._df_barcodes_loaded[\"predicted_probability\"] &gt; fine_threshold\n            ]\n            self._df_filtered_barcodes = df_above_threshold[\n                [\n                    \"tile_idx\",\n                    \"gene_id\",\n                    \"global_z\",\n                    \"global_y\",\n                    \"global_x\",\n                    \"distance_mean\",\n                ]\n            ].copy()\n            self._df_filtered_barcodes[\"cell_id\"] = -1\n            self._barcodes_filtered = True\n\n            if self._verbose &gt; 1:\n                print(f\"fdr : {fdr}\")\n                print(f\"retained barcodes: {len(self._df_filtered_barcodes)}\")\n\n            del df_above_threshold, full_data_scaled\n            del (\n                mlp,\n                predictions,\n                X_train,\n                X_test,\n                y_test,\n                y_train,\n                X_train_scaled,\n                X_test_scaled,\n            )\n            del df_true, df_false, df_true_sampled, df_combined\n            gc.collect()\n        else:\n            self._df_filtered_barcodes = self._df_barcodes_loaded.copy()\n            self._df_filtered_barcodes[\"cell_id\"] = -1\n            self._df_filtered_barcodes.drop(\"X\", axis=1, inplace=True)\n            self._barcodes_filtered = True\n\n    def _filter_all_barcodes_LR(self, fdr_target: float = 0.05):\n        \"\"\"Filter barcodes using a classifier and FDR target.\n\n        Uses a logistic regression classifier to predict whether a barcode is a blank or not.\n\n        Parameters\n        ----------\n        fdr_target : float, default 0.05\n            False discovery rate target. \n        \"\"\"\n\n        from sklearn.model_selection import train_test_split\n        from sklearn.preprocessing import StandardScaler\n        from sklearn.linear_model import LogisticRegression\n        from sklearn.metrics import classification_report\n        from imblearn.over_sampling import SMOTE\n\n        self._df_barcodes_loaded[\"X\"] = ~self._df_barcodes_loaded[\n            \"gene_id\"\n        ].str.startswith(\"Blank\")\n\n        if self._is_3D:\n            columns = [\n                \"X\",\n                \"area\",\n                \"signal_mean\",\n                \"s-b_mean\",\n                \"distance_mean\",\n                \"magnitude_mean\",\n                \"inertia_tensor_eigvals-0\",\n                \"inertia_tensor_eigvals-1\",\n                \"inertia_tensor_eigvals-2\",\n            ]\n        else:\n            columns = [\n                \"X\",\n                \"area\",\n                \"signal_mean\",\n                \"s-b_mean\",\n                \"distance_mean\",\n                \"magnitude_mean\",\n                \"inertia_tensor_eigvals-0\",\n                \"inertia_tensor_eigvals-1\",\n            ]\n\n        df_true = self._df_barcodes_loaded[self._df_barcodes_loaded[\"X\"] == True][columns] #noqa\n        df_false = self._df_barcodes_loaded[self._df_barcodes_loaded[\"X\"] == False][columns] #noqa\n        if len(df_false) &gt; 1:\n            df_true_sampled = df_true.sample(n=len(df_false), random_state=42)\n            df_combined = pd.concat([df_true_sampled, df_false])\n            x = df_combined.drop(\"X\", axis=1)\n            y = df_combined[\"X\"]\n            X_train, X_test, y_train, y_test = train_test_split(\n                x, y, test_size=0.1, random_state=42\n            )\n\n            if self._verbose &gt; 1:\n                print(\"generating synthetic samples for class balance\")\n            smote = SMOTE(random_state=42)\n            #X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n            X_train_resampled = X_train.copy()\n            y_train_resampled = y_train.copy()\n\n            if self._verbose &gt; 1:\n                print(\"scaling features\")\n            scaler = StandardScaler()\n            X_train_scaled = scaler.fit_transform(X_train_resampled)\n            X_test_scaled = scaler.transform(X_test)\n\n            if self._verbose &gt; 1:\n                print(\"training classifier\")\n            logistic = LogisticRegression(solver='liblinear', random_state=42)\n            logistic.fit(X_train_scaled, y_train_resampled)\n            predictions = logistic.predict(X_test_scaled)\n\n            if self._verbose &gt; 1:\n                print(classification_report(y_test, predictions))\n\n            if self._verbose &gt; 1:\n                print(\"predicting on full data\")\n\n            full_data_scaled = scaler.transform(self._df_barcodes_loaded[columns[1:]])\n            self._df_barcodes_loaded[\"predicted_probability\"] = logistic.predict_proba(\n                full_data_scaled\n            )[:, 1]\n\n            if self._verbose &gt; 1:\n                print(\"filtering blanks\")\n\n            coarse_threshold = 0\n            for threshold in np.arange(0, 1, 0.1):\n                fdr = self.calculate_fdr(\n                    self._df_barcodes_loaded,\n                    threshold,\n                    self._blank_count,\n                    self._barcode_count,\n                    self._verbose,\n                )\n                if fdr &lt;= fdr_target:\n                    coarse_threshold = threshold\n                    break\n\n            fine_threshold = coarse_threshold\n            for threshold in np.arange(\n                coarse_threshold - 0.1, coarse_threshold + 0.1, 0.01\n            ):\n                fdr = self.calculate_fdr(\n                    self._df_barcodes_loaded,\n                    threshold,\n                    self._blank_count,\n                    self._barcode_count,\n                    self._verbose,\n                )\n                if fdr &lt;= fdr_target:\n                    fine_threshold = threshold\n                    break\n\n            df_above_threshold = self._df_barcodes_loaded[\n                self._df_barcodes_loaded[\"predicted_probability\"] &gt; fine_threshold\n            ]\n            self._df_filtered_barcodes = df_above_threshold[\n                [\n                    \"tile_idx\",\n                    \"gene_id\",\n                    \"global_z\",\n                    \"global_y\",\n                    \"global_x\",\n                    \"distance_mean\",\n                ]\n            ].copy()\n            self._df_filtered_barcodes[\"cell_id\"] = -1\n            self._barcodes_filtered = True\n\n            if self._verbose &gt; 1:\n                print(f\"fdr : {fdr}\")\n                print(f\"retained barcodes: {len(self._df_filtered_barcodes)}\")\n\n            del df_above_threshold, full_data_scaled\n            del (\n                logistic,\n                predictions,\n                X_train,\n                X_test,\n                y_test,\n                y_train,\n                X_train_scaled,\n                X_test_scaled,\n            )\n            del df_true, df_false, df_true_sampled, df_combined\n            gc.collect()\n        else:\n            self._df_filtered_barcodes = self._df_barcodes_loaded.copy()\n            self._df_filtered_barcodes[\"cell_id\"] = -1\n            self._df_filtered_barcodes.drop(\"X\", axis=1, inplace=True)\n            self._barcodes_filtered = True\n            if self._verbose &gt;= 1:\n                print(\"Insufficient Blank barcodes called for filtering.\")\n\n    @staticmethod\n    def _roi_to_shapely(roi):\n        return Polygon(roi.subpixel_coordinates[:, ::-1])\n\n    def _assign_cells(self):\n        \"\"\"Assign cells to barcodes using Cellpose ROIs.\"\"\"\n\n        cellpose_roi_path = (\n            self._datastore._datastore_path\n            / Path(\"segmentation\")\n            / Path(\"cellpose\")\n            / Path(\"imagej_rois\")\n            / Path(\"global_coords_rois.zip\")\n        )\n\n        try:\n            rois = roiread(cellpose_roi_path)\n        except (FileNotFoundError, IOError, ValueError) as e:\n            print(f\"Failed to read ROIs: {e}\")\n            return\n\n        shapely_polygons = []\n        for roi in rois:\n            shapely_polygon = self._roi_to_shapely(roi)\n            if shapely_polygon:\n                shapely_polygons.append(shapely_polygon)\n\n        rtree_index = rtree.index.Index()\n        for polygon_idx, polygon in enumerate(shapely_polygons):\n            try:\n                rtree_index.insert(polygon_idx, polygon.bounds)\n            except rtree.RTreeError as e:\n                print(f\"Failed to insert polygon into R-tree: {e}\")\n\n        def check_point(row):\n            \"\"\"Check if point is within a polygon.\n\n            Parameters\n            ----------\n            row : pd.Series\n                Row containing global coordinates.\n\n            Returns\n            -------\n            cell_id : int\n                Cell ID. Returns 0 if not found.\n            \"\"\"\n            point = Point(row[\"global_y\"], row[\"global_x\"])\n\n            candidate_ids = list(rtree_index.intersection(point.bounds))\n            for candidate_id in candidate_ids:\n                if shapely_polygons[candidate_id].contains(point):\n                    return candidate_id + 1\n            return 0\n\n        self._df_filtered_barcodes[\"cell_id\"] = self._df_filtered_barcodes.apply(\n            check_point, axis=1\n        )\n\n    def _remove_duplicates_in_tile_overlap(self, radius: float = 0.75):\n        \"\"\"Remove duplicates in tile overlap.\n\n        Parameters\n        ----------\n        radius : float, default 0.75 \n            3D radius, in microns, for duplicate removal. \n        \"\"\"\n\n        self._df_filtered_barcodes.reset_index(drop=True, inplace=True)\n\n        coords = self._df_filtered_barcodes[[\"global_z\", \"global_y\", \"global_x\"]].values\n        tile_idxs = self._df_filtered_barcodes[\"tile_idx\"].values\n\n        tree = cKDTree(coords)\n        pairs = tree.query_pairs(radius)\n\n        rows_to_drop = set()\n        distances = []\n        for i, j in pairs:\n            if tile_idxs[i] != tile_idxs[j]:\n                if (\n                    self._df_filtered_barcodes.loc[i, \"distance_mean\"]\n                    &lt;= self._df_filtered_barcodes.loc[j, \"distance_mean\"]\n                ):\n                    rows_to_drop.add(j)\n                    distances.append(self._df_filtered_barcodes.loc[j, \"distance_mean\"])\n                else:\n                    rows_to_drop.add(i)\n                    distances.append(self._df_filtered_barcodes.loc[i, \"distance_mean\"])\n\n        self._df_filtered_barcodes.drop(rows_to_drop, inplace=True)\n        self._df_filtered_barcodes.reset_index(drop=True, inplace=True)\n\n        avg_distance = np.mean(distances) if distances else 0\n        dropped_count = len(rows_to_drop)\n\n        if self._verbose &gt; 1:\n            print(\n                \"Average distance metric of dropped points (overlap): \"\n                + str(avg_distance)\n            )\n            print(\"Dropped points: \" + str(dropped_count))\n\n\n    def _remove_duplicates_within_tile(\n        self,\n        radius_xy: float = 0.75,\n        radius_z: float = 0.50,\n    ) -&gt; None:\n        \"\"\"Collapse near-duplicate detections within each tile *and same gene_id*.\n\n        Two rows are considered neighbors if and only if:\n        1) They belong to the same tile (``tile_idx``),\n        2) Their XY separation is within ``radius_xy`` (microns),\n        3) Their absolute Z separation is within ``radius_z`` (microns), and\n        4) Their identity matches (``gene_id`` is equal).\n\n        For each connected component (cluster) under this neighbor relation,\n        keep exactly one row: the one with the smallest ``distance_mean``.\n        Ties on ``distance_mean`` are broken deterministically by the original row\n        index (lower index wins).\n\n        Parameters\n        ----------\n        radius_xy : float, default 0.75\n            Neighborhood radius in the XY plane, in microns.\n        radius_z : float, default 0.50\n            Neighborhood half-extent along Z, in microns.\n\n        Modifies\n        --------\n        self._df_filtered_barcodes : pandas.DataFrame\n            Drops non-winning rows per cluster; resets index at the end.\n\n        Notes\n        -----\n        Expected columns: ``global_z``, ``global_y``, ``global_x``,\n        ``tile_idx``, ``gene_id``, ``distance_mean``.\n        \"\"\"\n        try:\n            df = self._df_filtered_barcodes\n            filtered = True\n        except:\n            df = self._df_barcodes_loaded\n            filtered = False\n        if df.empty or len(df) &lt; 2:\n            return\n\n        # Stable order &amp; deterministic tie-breaks\n        df.reset_index(drop=True, inplace=True)\n\n        coords = df[[\"global_z\", \"global_y\", \"global_x\"]].to_numpy(dtype=float, copy=False)\n        tiles = df[\"tile_idx\"].to_numpy()\n        genes = df[\"gene_id\"].to_numpy()  # dtype can be int/str/object; equality works elementwise\n        dmean = df[\"distance_mean\"].to_numpy(dtype=float, copy=False)\n\n        rows_to_drop: set[int] = set()\n\n        # Union\u2013Find (Disjoint Set)\n        def uf_find(parent: np.ndarray, x: int) -&gt; int:\n            while parent[x] != x:\n                parent[x] = parent[parent[x]]\n                x = parent[x]\n            return x\n\n        def uf_union(parent: np.ndarray, rank: np.ndarray, a: int, b: int) -&gt; None:\n            ra, rb = uf_find(parent, a), uf_find(parent, b)\n            if ra == rb:\n                return\n            if rank[ra] &lt; rank[rb]:\n                parent[ra] = rb\n            elif rank[ra] &gt; rank[rb]:\n                parent[rb] = ra\n            else:\n                parent[rb] = ra\n                rank[ra] += 1\n\n        # Process each tile independently\n        for t in np.unique(tiles):\n            local_idx = np.flatnonzero(tiles == t)\n            if local_idx.size &lt; 2:\n                continue\n\n            sub = coords[local_idx]\n            z_local = sub[:, 0]\n            xy_local = sub[:, 1:3]       # (Y, X)\n            genes_local = genes[local_idx]\n\n            # 1) XY-near candidate pairs\n            tree = cKDTree(xy_local)\n            pairs_local = tree.query_pairs(r=radius_xy)\n            if not pairs_local:\n                continue\n\n            # 2) Filter by Z window *and same gene_id*\n            filtered_pairs = [\n                (i, j)\n                for (i, j) in pairs_local\n                if (abs(z_local[i] - z_local[j]) &lt;= radius_z) and (genes_local[i] == genes_local[j])\n            ]\n            if not filtered_pairs:\n                continue\n\n            # 3) Union\u2013Find over local nodes\n            n = local_idx.size\n            parent = np.arange(n)\n            rank = np.zeros(n, dtype=np.int8)\n            for i_loc, j_loc in filtered_pairs:\n                uf_union(parent, rank, i_loc, j_loc)\n\n            # 4) Gather components\n            comps: dict[int, list[int]] = {}\n            for i_loc in range(n):\n                r = uf_find(parent, i_loc)\n                comps.setdefault(r, []).append(i_loc)\n\n            # 5) Keep exactly one best per multi-member component\n            for members in comps.values():\n                if len(members) &lt; 2:\n                    continue\n                glob_members = local_idx[np.asarray(members)]\n                # Lexicographic: primary key is distance_mean, tie-breaker is original index\n                best_global = glob_members[np.lexsort((glob_members, dmean[glob_members]))][0]\n                for g in glob_members:\n                    if g != best_global:\n                        rows_to_drop.add(g)\n\n        if rows_to_drop:\n            df.drop(index=list(rows_to_drop), inplace=True)\n            df.reset_index(drop=True, inplace=True)\n\n        if getattr(self, \"_verbose\", 0) &gt; 1:\n            dropped = dmean[list(rows_to_drop)] if rows_to_drop else np.array([], dtype=float)\n            avg = float(dropped.mean()) if dropped.size else 0.0\n            print(\n                \"Average distance metric of dropped points (within-tile, same gene, clusters): \"\n                + str(avg)\n            )\n            print(\"Dropped points: \" + str(len(rows_to_drop)))\n\n        if filtered:\n            del self._df_filtered_barcodes\n            self._df_filtered_barcodes = df.copy()\n        else:\n            del self._df_barcodes_loaded\n            self._df_barcodes_loaded = df.copy()\n\n    def _display_results(self):\n        \"\"\"Display results using Napari.\"\"\"\n\n        import napari\n        from qtpy.QtWidgets import QApplication\n\n        def on_close_callback():\n            viewer.layers.clear()\n            gc.collect()\n\n        viewer = napari.Viewer()\n        app = QApplication.instance()\n\n        app.lastWindowClosed.connect(on_close_callback)\n\n\n        viewer.add_image(\n            self._image_data_lp,\n            scale=[self._axial_step, self._pixel_size, self._pixel_size],\n            name=\"image\",\n        )\n\n        viewer.add_image(\n            self._scaled_pixel_images,\n            scale=[self._axial_step, self._pixel_size, self._pixel_size],\n            name=\"scaled pixels\",\n        )\n\n        viewer.add_image(\n            self._decoded_image,\n            scale=[self._axial_step, self._pixel_size, self._pixel_size], # yes.\n            name=\"decoded\",\n        )\n\n        viewer.add_image(\n            self._magnitude_image,\n            scale=[self._axial_step, self._pixel_size, self._pixel_size],\n            name=\"magnitude\",\n        )\n\n        viewer.add_image(\n            self._distance_image,\n            scale=[self._axial_step, self._pixel_size, self._pixel_size],\n            name=\"distance\",\n        )\n\n        napari.run()\n\n    def _cleanup(self):\n        \"\"\"Cleanup memory.\"\"\"\n        for gpu_id in range(self._num_gpus):\n            cp.cuda.Device(gpu_id).use()\n            cp.cuda.Device(gpu_id).synchronize()\n            try:\n                if self._filter_type == \"lp\":\n                    del self._image_data_lp\n                else:\n                    del self._image_data\n            except AttributeError:\n                pass\n\n            try:\n                del (\n                    self._scaled_pixel_images,\n                    self._decoded_image,\n                    self._distance_image,\n                    self._magnitude_image,\n                )\n            except AttributeError:\n                pass\n\n            try:\n                del self._df_barcodes\n            except AttributeError:\n                pass\n            if self._barcodes_filtered:\n                try:\n                    del self._df_filtered_barcodes\n                except AttributeError:\n                    pass\n\n            gc.collect()\n            cp.cuda.Stream.null.synchronize()\n            cp.get_default_memory_pool().free_all_blocks()\n            cp.get_default_pinned_memory_pool().free_all_blocks()\n\n    def decode_one_tile(\n        self,\n        tile_idx: int = 0,\n        gpu_id: int = 0, \n        display_results: bool = False,\n        return_results: bool = False,\n        lowpass_sigma: Optional[Sequence[float]] = (3, 1, 1),\n        magnitude_threshold: Optional[list[float,float]] = (0.9,10.0),\n        minimum_pixels: Optional[float] = 2.0,\n        use_normalization: Optional[bool] = True,\n        ufish_threshold: Optional[float] = 0.1,\n    ) -&gt; Optional[tuple[np.ndarray, ...]]:\n        \"\"\"Decode one tile.\n\n        Helper function to decode one tile. Can also display results in napari or return results as np.ndarray.\n\n        Parameters\n        ----------\n        tile_idx : int, default 0\n            Tile index.\n        gpu_id : int, default 0\n            GPU ID to use for decoding.\n        display_results : bool, default False\n            Display results in napari.\n        return_results : bool, default False\n            Return results as np.ndarray\n        lowpass_sigma : Optional[Sequence[float]], default (3, 1, 1)\n            Lowpass sigma.\n        magnitude_threshold: Optional[Sequence[float]], default (1.1, 2.0)\n            L2-norm threshold\n        minimum_pixels : Optional[float], default 3.0\n            Minimum number of pixels for a barcode. \n        use_normalization : Optional[bool], default True\n            Use normalization. \n        ufish_threshold : Optional[float], default 0.5\n            Ufish threshold.\n\n        Returns\n        -------\n        Optional[tuple[np.ndarray,...]]\n            If return_results is True, returns a tuple of np.ndarray containing the following:\n            1. Image data (filtered or unfiltered).\n            2. Scaled pixel images.\n            3. Magnitude image.\n            4. Distance image.\n            5. Decoded image.\n        \"\"\"\n\n        with cp.cuda.Device(gpu_id):\n\n            if use_normalization:\n                self._load_iterative_normalization_vectors(gpu_id=gpu_id)\n\n            self._tile_idx = tile_idx\n            self._load_bit_data(ufish_threshold=ufish_threshold)\n            if not (np.any(lowpass_sigma == 0)):\n                self._lp_filter(sigma=lowpass_sigma,gpu_id=gpu_id)\n            self._decode_pixels(\n                distance_threshold=self._distance_threshold,\n                magnitude_threshold=magnitude_threshold,\n                gpu_id=gpu_id\n            )\n            self._extract_barcodes(minimum_pixels=minimum_pixels,gpu_id=gpu_id)\n\n            if display_results:\n                if not(self._df_barcodes.empty):\n                    print(f\"Number of extracted barcodes: {len(self._df_barcodes)}\")\n                else:\n                    print(\"No barcodes extracted.\")\n                self._display_results()\n            if return_results:\n                if self._filter_type == \"lp\":\n                    return (\n                        self._image_data_lp, \n                        self._scaled_pixel_images, \n                        self._magnitude_image, \n                        self._distance_image, \n                        self._decoded_image\n                    )\n                else:\n                    return (\n                        self._image_data, \n                        self._scaled_pixel_images, \n                        self._magnitude_image, \n                        self._distance_image, \n                        self._decoded_image\n                    )\n\n\n\n    def optimize_normalization_by_decoding(\n        self,\n        n_random_tiles: int = 5,\n        n_iterations: int = 10,\n        distance_threshold: Optional[float] = 0.52,\n        minimum_pixels: Optional[float] = 2.0,\n        ufish_threshold: Optional[float] = 0.1,\n        lowpass_sigma: Optional[Sequence[float]] = (3, 1, 1),\n        magnitude_threshold: Optional[Sequence[float]] = (0.9, 10.0)\n    ):\n        \"\"\"Optimize normalization by decoding.\n\n        Helper function to iteratively optimize normalization by decoding.\n\n        Parameters\n        ----------\n        n_random_tiles : int, default 10\n            Number of random tiles. \n        n_iterations : int, default 10\n            Number of iterations. \n        minimum_pixels : float, default 3.0\n            Minimum number of pixels for a barcode. \n        ufish_threshold : float, default 0.1\n            Ufish threshold. \n        lowpass_sigma : Optional[Sequence[float]], default (3, 1, 1)\n            Lowpass sigma.\n        magnitude_threshold: Optional[Sequence[float], default (0.9,10.0)\n            L2-norm threshold\n        \"\"\"\n        if self._num_gpus &lt; 1:\n            raise RuntimeError(\"No GPUs allocated.\")\n        all_tiles = list(range(len(self._datastore.tile_ids)))\n\n        # preload global normalization once\n        self._distance_threshold = distance_threshold\n        self._iterative_background_vector = None\n        self._iterative_normalization_vector = None\n        self._global_background_vector = None\n        self._optimize_normalization_weights = True\n        self._load_global_normalization_vectors(gpu_id=0)\n        temp_dir = Path(tempfile.mkdtemp())\n        self._temp_dir = temp_dir\n\n        # split the same set of random tiles each iteration\n        if len(all_tiles) &gt; n_random_tiles:\n            random_tiles = sample(all_tiles, n_random_tiles)\n        else:\n            random_tiles = all_tiles\n        chunk_size = (len(random_tiles) + self._num_gpus - 1) // self._num_gpus\n\n        if self._verbose &gt;= 1:\n            iterator = trange(n_iterations,desc=\"Iterative normalization\")\n        else:\n            iterator = range(n_iterations)\n\n        for iteration in iterator:\n\n            # launch one process per GPU\n            processes = []\n            for gpu in range(self._num_gpus):\n                start = gpu * chunk_size\n                end = min(start + chunk_size, len(random_tiles))\n                subset = random_tiles[start:end]\n                if not subset:\n                    continue\n                p = mp.Process(\n                    target=_optimize_norm_worker,\n                    args=(\n                        self._datastore_path,\n                        subset,\n                        gpu,\n                        self._n_merfish_bits,\n                        temp_dir,\n                        iteration,\n                        lowpass_sigma,\n                        distance_threshold,\n                        magnitude_threshold,\n                        minimum_pixels,\n                        ufish_threshold,\n                        self._smFISH\n                    ),\n                )\n                p.start()\n                processes.append(p)\n\n            for p in processes:\n                p.join()\n\n            with cp.cuda.Device(0):\n            # gather results and update\n                self._load_all_barcodes()\n                if not(self._is_3D):\n                    radius_z = self._datastore.voxel_size_zyx_um[0]*2\n                    self._remove_duplicates_within_tile(radius_z=radius_z)\n                self._load_global_normalization_vectors(gpu_id=0)\n                if not(self._verbose == 0):\n                    self._verbose = 2\n                self._iterative_normalization_vectors(gpu_id=0)\n                if not(self._verbose == 0):\n                    self._verbose = 1\n                del self._global_background_vector, self._global_normalization_vector\n                gc.collect()\n                cp.cuda.Stream.null.synchronize()\n                cp.get_default_memory_pool().free_all_blocks()\n                cp.get_default_pinned_memory_pool().free_all_blocks()\n\n        # cleanup temp files, etc.\n        self._cleanup()\n        self._optimize_normalization_weights = False\n        shutil.rmtree(self._temp_dir)\n\n    def decode_all_tiles(\n        self,\n        assign_to_cells: bool = True,\n        prep_for_baysor: bool = True,\n        distance_threshold: Optional[float] = 0.5176,\n        lowpass_sigma: Optional[Sequence[float]] = (3, 1, 1),\n        magnitude_threshold: Optional[Sequence[float]] = (0.9,10.0),\n        minimum_pixels: Optional[float] = 2.0,\n        ufish_threshold: Optional[float] = 0.1,\n        fdr_target: Optional[float] = 0.05,\n    ):\n        \"\"\"Optimize normalization by decoding.\n\n        Helper function to iteratively optimize normalization by decoding.\n\n        Parameters\n        ----------\n        n_random_tiles : int, default 10\n            Number of random tiles. \n        n_iterations : int, default 10\n            Number of iterations. \n        minimum_pixels : float, default 3.0\n            Minimum number of pixels for a barcode. \n        ufish_threshold : float, default 0.25\n            Ufish threshold. \n        lowpass_sigma : Optional[Sequence[float]], default (3, 1, 1)\n            Lowpass sigma.\n        magnitude_threshold: Optional[Sequence[float], default (1.1,2.0)\n            L2-norm threshold\n        \"\"\"\n\n        if self._num_gpus &lt; 1:\n            raise RuntimeError(\"No GPUs allocated.\")\n        all_tiles = list(range(len(self._datastore.tile_ids)))\n        chunk_size = (len(all_tiles) + self._num_gpus - 1) // self._num_gpus\n\n        self._distance_threshold = distance_threshold\n\n        processes = []\n        for gpu in range(self._num_gpus):\n            start = gpu * chunk_size\n            end = min(start + chunk_size, len(all_tiles))\n            subset = all_tiles[start:end]\n            if not subset:\n                continue\n            p = mp.Process(\n                target=decode_tiles_worker,\n                args=(\n                    self._datastore_path,\n                    subset,\n                    gpu,\n                    self._n_merfish_bits,\n                    lowpass_sigma,\n                    distance_threshold,\n                    magnitude_threshold,\n                    minimum_pixels,\n                    ufish_threshold,\n                    self._smFISH\n                ),\n            )\n            p.start()\n            processes.append(p)\n\n        for p in processes:\n            p.join()\n\n        # load all barcodes and filter\n        self._load_tile_decoding = True\n        self._load_all_barcodes()\n        self._filter_all_barcodes_LR(fdr_target=fdr_target)\n        if not(self._is_3D):\n            radius_z = self._datastore.voxel_size_zyx_um[0]\n            self._remove_duplicates_within_tile(radius_z=radius_z)\n\n        if len(all_tiles) &gt; 1:\n            self._remove_duplicates_in_tile_overlap()\n        if assign_to_cells:\n            self._assign_cells()\n        self._save_barcodes()\n        if self._verbose &gt;=1 :\n            print(f\"Number of retained barcodes: {len(self._df_filtered_barcodes)}\")\n        if prep_for_baysor:\n            self._reformat_barcodes_for_baysor()\n        self._cleanup()\n\n    def optimize_filtering(\n        self,\n        assign_to_cells: bool = False,\n        prep_for_baysor: bool = True,\n        fdr_target: Optional[float] = 0.05,\n    ):\n        \"\"\"Optimize filtering.\n\n        Helper function to opimize filtering for already decoded spots.\n\n        Parameters\n        ----------\n        assign_to_cells : bool, default False\n            Assign barcodes to cells. \n        prep_for_baysor : bool, default True\n            Prepare barcodes for Baysor. \n        fdr_target : Optional[float], default 0.05\n            False discovery rate target. \n        \"\"\"\n\n        self._load_tile_decoding = True\n        self._load_all_barcodes()\n        self._load_tile_decoding = False\n        all_tiles = list(range(len(self._datastore.tile_ids)))\n        if not(self._verbose == 0):\n            self._verbose = 2\n        if len(all_tiles) or not(self._is_3D):\n            if not(self._is_3D):\n                radius_z = self._datastore.voxel_size_zyx_um[0]*2\n                self._remove_duplicates_within_tile(radius_z=radius_z)\n            else:\n                self._remove_duplicates_in_tile_overlap()\n        self._filter_all_barcodes(fdr_target=fdr_target)\n        if not(self._verbose == 0):\n            self._verbose = 1\n\n        if assign_to_cells:\n            self._assign_cells()\n        self._save_barcodes()\n        if prep_for_baysor:\n            self._reformat_barcodes_for_baysor()\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._assign_cells","title":"<code>_assign_cells()</code>","text":"<p>Assign cells to barcodes using Cellpose ROIs.</p> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def _assign_cells(self):\n    \"\"\"Assign cells to barcodes using Cellpose ROIs.\"\"\"\n\n    cellpose_roi_path = (\n        self._datastore._datastore_path\n        / Path(\"segmentation\")\n        / Path(\"cellpose\")\n        / Path(\"imagej_rois\")\n        / Path(\"global_coords_rois.zip\")\n    )\n\n    try:\n        rois = roiread(cellpose_roi_path)\n    except (FileNotFoundError, IOError, ValueError) as e:\n        print(f\"Failed to read ROIs: {e}\")\n        return\n\n    shapely_polygons = []\n    for roi in rois:\n        shapely_polygon = self._roi_to_shapely(roi)\n        if shapely_polygon:\n            shapely_polygons.append(shapely_polygon)\n\n    rtree_index = rtree.index.Index()\n    for polygon_idx, polygon in enumerate(shapely_polygons):\n        try:\n            rtree_index.insert(polygon_idx, polygon.bounds)\n        except rtree.RTreeError as e:\n            print(f\"Failed to insert polygon into R-tree: {e}\")\n\n    def check_point(row):\n        \"\"\"Check if point is within a polygon.\n\n        Parameters\n        ----------\n        row : pd.Series\n            Row containing global coordinates.\n\n        Returns\n        -------\n        cell_id : int\n            Cell ID. Returns 0 if not found.\n        \"\"\"\n        point = Point(row[\"global_y\"], row[\"global_x\"])\n\n        candidate_ids = list(rtree_index.intersection(point.bounds))\n        for candidate_id in candidate_ids:\n            if shapely_polygons[candidate_id].contains(point):\n                return candidate_id + 1\n        return 0\n\n    self._df_filtered_barcodes[\"cell_id\"] = self._df_filtered_barcodes.apply(\n        check_point, axis=1\n    )\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._calculate_distances","title":"<code>_calculate_distances(pixel_traces, codebook_matrix, gpu_id=0)</code>  <code>staticmethod</code>","text":"<p>Calculate distances between pixel traces and codebook matrix.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_traces</code> <code>Union[ndarray, ndarray]</code> <p>Pixel traces.</p> required <code>codebook_matrix</code> <code>Union[ndarray, ndarray]</code> <p>Codebook matrix.</p> required <p>Returns:</p> Name Type Description <code>min_distances</code> <code>ndarray</code> <p>Minimum distances.</p> <code>min_indices</code> <code>ndarray</code> <p>Minimum indices.</p> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>@staticmethod\ndef _calculate_distances(\n    pixel_traces: Union[np.ndarray, cp.ndarray],\n    codebook_matrix: Union[np.ndarray, cp.ndarray],\n    gpu_id: int = 0\n) -&gt; Tuple[cp.ndarray, cp.ndarray]:\n    \"\"\"Calculate distances between pixel traces and codebook matrix.\n\n    Parameters\n    ----------\n    pixel_traces : Union[np.ndarray, cp.ndarray]\n        Pixel traces.\n    codebook_matrix : Union[np.ndarray, cp.ndarray]\n        Codebook matrix.\n\n    Returns\n    -------\n    min_distances : cp.ndarray\n        Minimum distances.\n    min_indices : cp.ndarray\n        Minimum indices.\n    \"\"\"\n\n    with cp.cuda.Device(gpu_id):\n        if isinstance(pixel_traces, np.ndarray):\n            pixel_traces = cp.asarray(pixel_traces, dtype=cp.float32)\n        if isinstance(codebook_matrix, np.ndarray):\n            codebook_matrix = cp.asarray(codebook_matrix, dtype=cp.float32)\n\n        distances = cp.ascontiguousarray(\n            cp.zeros((pixel_traces.shape[1], codebook_matrix.shape[0]), dtype=cp.float32)\n        )\n        pairwise_distance(\n            cp.ascontiguousarray(pixel_traces.T),\n            cp.ascontiguousarray(codebook_matrix),\n            metric=\"euclidean\",\n            out=distances\n        )\n\n        min_indices = cp.argmin(distances, axis=1)\n        min_distances = cp.min(distances, axis=1)\n\n        del pixel_traces, codebook_matrix\n        gc.collect()\n        cp.cuda.Stream.null.synchronize()\n        cp.get_default_memory_pool().free_all_blocks()\n        cp.get_default_pinned_memory_pool().free_all_blocks()\n\n        return min_distances, min_indices\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._cleanup","title":"<code>_cleanup()</code>","text":"<p>Cleanup memory.</p> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def _cleanup(self):\n    \"\"\"Cleanup memory.\"\"\"\n    for gpu_id in range(self._num_gpus):\n        cp.cuda.Device(gpu_id).use()\n        cp.cuda.Device(gpu_id).synchronize()\n        try:\n            if self._filter_type == \"lp\":\n                del self._image_data_lp\n            else:\n                del self._image_data\n        except AttributeError:\n            pass\n\n        try:\n            del (\n                self._scaled_pixel_images,\n                self._decoded_image,\n                self._distance_image,\n                self._magnitude_image,\n            )\n        except AttributeError:\n            pass\n\n        try:\n            del self._df_barcodes\n        except AttributeError:\n            pass\n        if self._barcodes_filtered:\n            try:\n                del self._df_filtered_barcodes\n            except AttributeError:\n                pass\n\n        gc.collect()\n        cp.cuda.Stream.null.synchronize()\n        cp.get_default_memory_pool().free_all_blocks()\n        cp.get_default_pinned_memory_pool().free_all_blocks()\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._clip_pixel_traces","title":"<code>_clip_pixel_traces(pixel_traces, clip_lower=0.0, clip_upper=1.0, gpu_id=0)</code>  <code>staticmethod</code>","text":"<p>Clip pixel traces to a range.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_traces</code> <code>Union[ndarray, ndarray]</code> <p>Pixel traces to clip.</p> required <code>clip_lower</code> <code>float</code> <p>clip lower bound.</p> <code>0.0</code> <code>clip_upper</code> <code>float</code> <p>clip upper bound.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>clipped_traces</code> <code>ndarray</code> <p>Clipped pixel traces.</p> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>@staticmethod\ndef _clip_pixel_traces(\n    pixel_traces: Union[np.ndarray, cp.ndarray],\n    clip_lower: float = 0.0,\n    clip_upper: float = 1.0,\n    gpu_id: int = 0\n) -&gt; cp.ndarray:\n    \"\"\"Clip pixel traces to a range.\n\n    Parameters\n    ----------\n    pixel_traces : Union[np.ndarray, cp.ndarray]\n        Pixel traces to clip.\n    clip_lower : float, default 0.0\n        clip lower bound.\n    clip_upper : float, default 1.0\n        clip upper bound.\n\n    Returns\n    -------\n    clipped_traces : cp.ndarray\n        Clipped pixel traces.\n    \"\"\"\n    with cp.cuda.Device(gpu_id):\n\n        clipped = cp.clip(pixel_traces, clip_lower, clip_upper, pixel_traces)\n        cp.cuda.Stream.null.synchronize()\n        cp.get_default_memory_pool().free_all_blocks()\n        cp.get_default_pinned_memory_pool().free_all_blocks()\n        return clipped\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._decode_pixels","title":"<code>_decode_pixels(distance_threshold=0.5176, magnitude_threshold=(1.1, 2.0), gpu_id=0)</code>","text":"<p>Decode pixels using the decoding matrix.</p> <p>Parameters:</p> Name Type Description Default <code>distance_threshold</code> <code>float</code> <p>Distance threshold for decoding. The default is for a 4-bit, 4-distance Hamming codebook.</p> <code>0.5176.</code> <code>magnitude_threshold</code> <code>Sequence[float]</code> <p>Magnitude threshold for decoding.</p> <code>(1.1, 2.0).</code> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def _decode_pixels(\n    self, distance_threshold: float = 0.5176, \n    magnitude_threshold: Sequence[float] = (1.1, 2.0),\n    gpu_id: int = 0\n):\n    \"\"\"Decode pixels using the decoding matrix.\n\n    Parameters\n    ----------\n    distance_threshold : float, default 0.5176.\n        Distance threshold for decoding. The default is for a 4-bit,\n        4-distance Hamming codebook.\n    magnitude_threshold : Sequence[float], default (1.1, 2.0).\n        Magnitude threshold for decoding. \n    \"\"\"\n\n    with cp.cuda.Device(gpu_id):\n        if self._filter_type == \"lp\":\n            original_shape = self._image_data_lp.shape\n            self._decoded_image = np.zeros((original_shape[1:]), dtype=np.int16)\n            self._magnitude_image = np.zeros((original_shape[1:]), dtype=np.float16)\n            self._scaled_pixel_images = np.zeros((original_shape), dtype=np.float16)\n            self._distance_image = np.zeros((original_shape[1:]), dtype=np.float16)\n        else:\n            original_shape = self._image_data.shape\n            self._decoded_image = np.zeros((original_shape[1:]), dtype=np.int16)\n            self._magnitude_image = np.zeros((original_shape[1:]), dtype=np.float16)\n            self._scaled_pixel_images = np.zeros((original_shape), dtype=np.float16)\n            self._distance_image = np.zeros((original_shape[1:]), dtype=np.float16)\n\n        if self._verbose &gt; 1:\n            print(\"decode pixels\")\n            iterable_z = tqdm(range(original_shape[1]), desc=\"z\", leave=False)\n        elif self._verbose &gt;= 1:\n            iterable_z = tqdm(range(original_shape[1]), desc=\"decoding\", leave=False)\n        else:\n            iterable_z = range(original_shape[1])\n\n        for z_idx in iterable_z:\n            if self._filter_type == \"lp\":\n                z_plane_shape = self._image_data_lp[:, z_idx, :].shape\n                scaled_pixel_traces = (\n                    cp.asarray(self._image_data_lp[:, z_idx, :])\n                    .reshape(self._n_merfish_bits, -1)\n                    .astype(cp.float32)\n                )\n            else:\n                z_plane_shape = self._image_data[:, z_idx, :].shape\n                scaled_pixel_traces = (\n                    cp.asarray(self._image_data[:, z_idx, :])\n                    .reshape(self._n_merfish_bits, -1)\n                    .astype(cp.float32)\n                )\n\n            if self._iterative_normalization_loaded:\n                scaled_pixel_traces = self._scale_pixel_traces(\n                    scaled_pixel_traces,\n                    self._iterative_background_vector,\n                    self._iterative_normalization_vector,\n                    self._n_merfish_bits,\n                    gpu_id=gpu_id\n                )\n            elif self._global_normalization_loaded:\n                scaled_pixel_traces = self._scale_pixel_traces(\n                    scaled_pixel_traces,\n                    self._global_background_vector,\n                    self._global_normalization_vector,\n                    self._n_merfish_bits,\n                    gpu_id=gpu_id\n                )\n\n            scaled_pixel_traces = self._clip_pixel_traces(scaled_pixel_traces,gpu_id=gpu_id)\n            normalized_pixel_traces, pixel_magnitude_trace = (\n                self._normalize_pixel_traces(scaled_pixel_traces,gpu_id=gpu_id)\n            )\n            distance_trace, codebook_index_trace = self._calculate_distances(\n                normalized_pixel_traces, self._decoding_matrix,gpu_id=gpu_id\n            )\n\n            del normalized_pixel_traces\n            gc.collect()\n            cp.cuda.Stream.null.synchronize()\n            cp.get_default_memory_pool().free_all_blocks()\n            cp.get_default_pinned_memory_pool().free_all_blocks()\n\n            decoded_trace = cp.full(distance_trace.shape[0], -1, dtype=cp.int16)\n            mask_trace = distance_trace &lt; distance_threshold\n            decoded_trace[mask_trace] = codebook_index_trace[mask_trace]\n            decoded_trace[pixel_magnitude_trace &lt; magnitude_threshold[0]] = -1\n            decoded_trace[pixel_magnitude_trace &gt; magnitude_threshold[1]] = -1\n\n            self._decoded_image[z_idx, :] = cp.asnumpy(\n                cp.reshape(cp.round(decoded_trace, 5), z_plane_shape[1:])\n            )\n            self._magnitude_image[z_idx, :] = cp.asnumpy(\n                cp.reshape(cp.round(pixel_magnitude_trace, 5), z_plane_shape[1:])\n            )\n            self._scaled_pixel_images[:, z_idx, :] = cp.asnumpy(\n                cp.reshape(cp.round(scaled_pixel_traces, 5), z_plane_shape)\n            )\n            self._distance_image[z_idx, :] = cp.asnumpy(\n                cp.reshape(cp.round(distance_trace, 5), z_plane_shape[1:])\n            )\n\n            del (\n                decoded_trace,\n                pixel_magnitude_trace,\n                scaled_pixel_traces,\n                distance_trace,\n            )\n            gc.collect()\n            cp.cuda.Stream.null.synchronize()\n            cp.get_default_memory_pool().free_all_blocks()\n            cp.get_default_pinned_memory_pool().free_all_blocks()\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._display_results","title":"<code>_display_results()</code>","text":"<p>Display results using Napari.</p> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def _display_results(self):\n    \"\"\"Display results using Napari.\"\"\"\n\n    import napari\n    from qtpy.QtWidgets import QApplication\n\n    def on_close_callback():\n        viewer.layers.clear()\n        gc.collect()\n\n    viewer = napari.Viewer()\n    app = QApplication.instance()\n\n    app.lastWindowClosed.connect(on_close_callback)\n\n\n    viewer.add_image(\n        self._image_data_lp,\n        scale=[self._axial_step, self._pixel_size, self._pixel_size],\n        name=\"image\",\n    )\n\n    viewer.add_image(\n        self._scaled_pixel_images,\n        scale=[self._axial_step, self._pixel_size, self._pixel_size],\n        name=\"scaled pixels\",\n    )\n\n    viewer.add_image(\n        self._decoded_image,\n        scale=[self._axial_step, self._pixel_size, self._pixel_size], # yes.\n        name=\"decoded\",\n    )\n\n    viewer.add_image(\n        self._magnitude_image,\n        scale=[self._axial_step, self._pixel_size, self._pixel_size],\n        name=\"magnitude\",\n    )\n\n    viewer.add_image(\n        self._distance_image,\n        scale=[self._axial_step, self._pixel_size, self._pixel_size],\n        name=\"distance\",\n    )\n\n    napari.run()\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._extract_barcodes","title":"<code>_extract_barcodes(minimum_pixels=9, maximum_pixels=1000, gpu_id=0)</code>","text":"<p>Extract barcodes from decoded image.</p> <p>Parameters:</p> Name Type Description Default <code>minimum_pixels</code> <code>int</code> <p>Minimum number of pixels for a barcode.</p> <code>9</code> <code>maximum_pixels</code> <code>int</code> <p>Maximum number of pixels for a barcode.</p> <code>1000</code> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def _extract_barcodes(\n    self, \n    minimum_pixels: int = 9, \n    maximum_pixels: int = 1000,\n    gpu_id: int = 0\n):\n    \"\"\"Extract barcodes from decoded image.\n\n    Parameters\n    ----------\n    minimum_pixels : int, default 9\n        Minimum number of pixels for a barcode. \n    maximum_pixels : int, default 1000\n        Maximum number of pixels for a barcode. \n    \"\"\"\n\n    self._df_barcodes = pd.DataFrame()\n\n    with cp.cuda.Device(gpu_id):\n        if self._verbose &gt; 1:\n            print(\"extract barcodes\")\n        if self._verbose &gt;= 1:\n            iterable_barcode = tqdm(\n                range(self._codebook_matrix.shape[0]), desc=\"barcode\", leave=False\n            )\n        else:\n            iterable_barcode = range(self._codebook_matrix.shape[0])\n        decoded_image = cp.asarray(self._decoded_image, dtype=cp.int16)\n        if self._optimize_normalization_weights:\n            if self._filter_type == \"lp\":\n                intensity_image = np.concatenate(\n                    [np.expand_dims(self._distance_image, axis=0), self._image_data_lp],\n                    axis=0,\n                ).transpose(1, 2, 3, 0)\n            else:\n                intensity_image = np.concatenate(\n                    [np.expand_dims(self._distance_image, axis=0), self._image_data],\n                    axis=0,\n                ).transpose(1, 2, 3, 0)\n        else:\n            intensity_image = np.concatenate(\n                [\n                    np.expand_dims(self._distance_image, axis=0),\n                    self._scaled_pixel_images,\n                ],\n                axis=0,\n            ).transpose(1, 2, 3, 0)\n\n        for barcode_index in iterable_barcode:\n            on_bits_indices = np.where(self._codebook_matrix[barcode_index])[0]\n\n            if len(on_bits_indices) == 1 and not(self._smFISH):\n                break\n\n            if self._is_3D:\n                if self._verbose &gt; 1:\n                    print(\"\")\n                    print(\"label image\")\n                labeled_image = label(decoded_image == barcode_index, connectivity=3)\n\n                if self._verbose &gt; 1:\n                    print(\"remove large\")\n                pixel_counts = cp.bincount(labeled_image.ravel())\n                large_labels = cp.where(pixel_counts &gt;= maximum_pixels)[0]\n                large_label_mask = cp.zeros_like(labeled_image, dtype=bool)\n                large_label_mask = cp.isin(labeled_image, large_labels)\n                labeled_image[large_label_mask] = 0\n\n                if self._verbose &gt; 1:\n                    print(\"remove small\")\n                labeled_image = remove_small_objects(\n                    labeled_image, min_size=(minimum_pixels - 1), connectivity=3\n                )\n                if self._verbose &gt; 1:\n                    print(\"regionprops table\")\n\n                labeled_image = cp.asnumpy(labeled_image).astype(np.int64)\n\n                props = regionprops_table(\n                    labeled_image,\n                    intensity_image=intensity_image,\n                    properties=[\n                        \"label\",\n                        \"area\",\n                        \"centroid\",\n                        \"intensity_mean\",\n                        \"inertia_tensor_eigvals\",\n                    ]\n                )\n                df_barcode = pd.DataFrame(props)\n\n                props_magnitude = regionprops_table(\n                    labeled_image,\n                    intensity_image=self._magnitude_image,\n                    properties=[\n                        \"label\",\n                        \"intensity_mean\",\n                    ]\n                )\n                df_magnitude = pd.DataFrame(props_magnitude)\n\n                del labeled_image, props, props_magnitude\n                gc.collect()\n                cp.cuda.Stream.null.synchronize()\n                cp.get_default_memory_pool().free_all_blocks()\n                cp.get_default_pinned_memory_pool().free_all_blocks()\n\n                df_magnitude = df_magnitude.rename(\n                    columns={'intensity_mean': 'magnitude_mean'}\n                )\n                df_barcode = df_barcode.merge(\n                    df_magnitude[[\"label\", \"magnitude_mean\"]],\n                    on=\"label\",\n                    how=\"left\",\n                )\n\n                df_barcode.drop(columns=\"label\", inplace=True)\n                df_barcode = df_barcode[df_barcode[\"area\"] &gt; 0.1].reset_index(drop=True)\n\n\n                if self._smFISH == False:\n                    df_barcode[\"on_bit_1\"] = on_bits_indices[0] + 1\n                    df_barcode[\"on_bit_2\"] = on_bits_indices[1] + 1\n                    df_barcode[\"on_bit_3\"] = on_bits_indices[2] + 1\n                    df_barcode[\"on_bit_4\"] = on_bits_indices[3] + 1\n                else:\n                    df_barcode[\"on_bit_1\"] = on_bits_indices[0] + 1\n                df_barcode[\"barcode_id\"] = df_barcode.apply(\n                    lambda x: (barcode_index + 1), axis=1\n                )\n                df_barcode[\"gene_id\"] = df_barcode.apply(\n                    lambda x: self._gene_ids[barcode_index], axis=1\n                )\n                df_barcode[\"tile_idx\"] = self._tile_idx\n\n                df_barcode.rename(columns={\"centroid-0\": \"z\"}, inplace=True)\n                df_barcode.rename(columns={\"centroid-1\": \"y\"}, inplace=True)\n                df_barcode.rename(columns={\"centroid-2\": \"x\"}, inplace=True)\n\n                if self._z_crop:\n                    df_barcode[\"z\"] = df_barcode[\"z\"] + self._z_range[0]\n\n                df_barcode[\"tile_z\"] = np.round(df_barcode[\"z\"], 0).astype(int)\n                df_barcode[\"tile_y\"] = np.round(df_barcode[\"y\"], 0).astype(int)\n                df_barcode[\"tile_x\"] = np.round(df_barcode[\"x\"], 0).astype(int)\n                pts = df_barcode[[\"z\", \"y\", \"x\"]].to_numpy()\n                for pt_idx, pt in enumerate(pts):\n                    pts[pt_idx, :] = self._warp_pixel(\n                        pts[pt_idx, :].copy(), self._spacing, self._origin, self._affine\n                    )\n\n                df_barcode[\"global_z\"] = np.round(pts[:, 0], 2)\n                df_barcode[\"global_y\"] = np.round(pts[:, 1], 2)\n                df_barcode[\"global_x\"] = np.round(pts[:, 2], 2)\n\n                df_barcode.rename(\n                    columns={\"intensity_mean-0\": \"distance_mean\"}, inplace=True\n                )\n                for i in range(1, self._n_merfish_bits + 1):\n                    df_barcode.rename(\n                        columns={\n                            \"intensity_mean-\" + str(i): \"bit\"\n                            + str(i).zfill(2)\n                            + \"_mean_intensity\"\n                        },\n                        inplace=True,\n                    )\n\n                on_bits = on_bits_indices + np.ones(4)\n\n                signal_mean_columns = [\n                    f\"bit{int(bit):02d}_mean_intensity\" for bit in on_bits\n                ]\n                bkd_mean_columns = [\n                    f\"bit{int(bit):02d}_mean_intensity\"\n                    for bit in range(1, self._n_merfish_bits + 1)\n                    if bit not in on_bits\n                ]\n\n                df_barcode[\"signal_mean\"] = df_barcode[signal_mean_columns].mean(axis=1)\n                df_barcode[\"bkd_mean\"] = df_barcode[bkd_mean_columns].mean(axis=1)\n                df_barcode[\"s-b_mean\"] = (\n                    df_barcode[\"signal_mean\"] - df_barcode[\"bkd_mean\"]\n                )\n\n                if self._verbose &gt; 1:\n                    print(\"dataframe aggregation\")\n                if barcode_index == 0:\n                    self._df_barcodes = df_barcode.copy()\n                else:\n                    if not df_barcode.empty:\n                        self._df_barcodes = pd.concat([self._df_barcodes, df_barcode])\n                        self._df_barcodes.reset_index(drop=True, inplace=True)\n\n                del df_barcode\n                gc.collect()\n            else:\n                if self._verbose &gt; 1:\n                    print(\"\")\n                    print(\"label image\")\n\n                from cupyx.scipy import ndimage as cpx_ndi\n                structure = cp.zeros((3, 3, 3), dtype=cp.uint8)\n                structure[1, :, :] = 1  # only same-Z neighbors are connected\n                structure[1, 0, 0] = 0\n                structure[1, 0, 2] = 0\n                structure[1, 2, 0] = 0\n                structure[1, 2, 2] = 0\n                labeled_image, _ = cpx_ndi.label(decoded_image == barcode_index, structure=structure)\n\n                if self._verbose &gt; 1:\n                    print(\"remove large\")\n                pixel_counts = cp.bincount(labeled_image.ravel())\n                large_labels = cp.where(pixel_counts &gt; maximum_pixels)[0]\n                large_label_mask = cp.zeros_like(labeled_image, dtype=bool)\n                large_label_mask = cp.isin(labeled_image, large_labels)\n                labeled_image[large_label_mask] = 0\n\n                if self._verbose &gt; 1:\n                    print(\"remove small\")\n                labeled_image = remove_small_objects(\n                    labeled_image, min_size=minimum_pixels\n                )\n                if self._verbose &gt; 1:\n                    print(\"regionprops table\")\n\n                labeled_image = cp.asnumpy(labeled_image).astype(np.int64)\n                props = regionprops_table(\n                    labeled_image,\n                    intensity_image=intensity_image,\n                    properties=[\n                        \"label\",\n                        \"area\",\n                        \"centroid\",\n                        \"intensity_mean\",\n                        \"inertia_tensor_eigvals\",\n                    ],\n                )\n                df_barcode = pd.DataFrame(props)\n\n                props_magnitude = regionprops_table(\n                    labeled_image,\n                    intensity_image=self._magnitude_image,\n                    properties=[\n                        \"label\",\n                        \"intensity_mean\",\n                    ]\n                )\n                df_magnitude = pd.DataFrame(props_magnitude)\n\n                del labeled_image, props, props_magnitude\n                gc.collect()\n                cp.cuda.Stream.null.synchronize()\n                cp.get_default_memory_pool().free_all_blocks()\n                cp.get_default_pinned_memory_pool().free_all_blocks()\n\n                if not (df_magnitude.index.empty):\n                    df_magnitude = df_magnitude.rename(\n                        columns={'intensity_mean': 'magnitude_mean'}\n                    )\n                    df_barcode = df_barcode.merge(\n                        df_magnitude[[\"label\", \"magnitude_mean\"]],\n                        on=\"label\",\n                        how=\"left\",\n                    )\n                    df_barcode.drop(columns=\"label\", inplace=True)\n\n                df_barcode = df_barcode[df_barcode[\"area\"] &gt; 0.1].reset_index(drop=True)\n\n                if self._smFISH == False:\n                    df_barcode[\"on_bit_1\"] = on_bits_indices[0] + 1\n                    df_barcode[\"on_bit_2\"] = on_bits_indices[1] + 1\n                    df_barcode[\"on_bit_3\"] = on_bits_indices[2] + 1\n                    df_barcode[\"on_bit_4\"] = on_bits_indices[3] + 1\n                else:\n                    df_barcode[\"on_bit_1\"] = on_bits_indices[0] + 1\n                df_barcode[\"barcode_id\"] = df_barcode.apply(\n                    lambda x: (barcode_index + 1), axis=1\n                )\n                df_barcode[\"gene_id\"] = df_barcode.apply(\n                    lambda x: self._gene_ids[barcode_index], axis=1\n                )\n                df_barcode[\"tile_idx\"] = self._tile_idx\n\n\n                df_barcode.rename(columns={\"centroid-0\": \"z\"}, inplace=True)\n                df_barcode.rename(columns={\"centroid-1\": \"y\"}, inplace=True)\n                df_barcode.rename(columns={\"centroid-2\": \"x\"}, inplace=True)\n\n                if self._z_crop:\n                    df_barcode[\"z\"] = df_barcode[\"z\"] + self._z_range[0]\n\n                df_barcode[\"tile_z\"] = np.round(df_barcode[\"z\"], 0).astype(int)\n                df_barcode[\"tile_y\"] = np.round(df_barcode[\"y\"], 0).astype(int)\n                df_barcode[\"tile_x\"] = np.round(df_barcode[\"x\"], 0).astype(int)\n\n                pts = df_barcode[[\"z\", \"y\", \"x\"]].to_numpy()\n                for pt_idx, pt in enumerate(pts):\n                    pts[pt_idx, :] = self._warp_pixel(\n                        pts[pt_idx, :].copy(),\n                        self._spacing,\n                        self._origin,\n                        self._affine,\n                    )\n\n                df_barcode[\"global_z\"] = np.round(pts[:, 0], 2)\n                df_barcode[\"global_y\"] = np.round(pts[:, 1], 2)\n                df_barcode[\"global_x\"] = np.round(pts[:, 2], 2)\n\n                df_barcode.rename(\n                    columns={\"intensity_mean-0\": \"distance_mean\"}, inplace=True\n                )\n                for i in range(1, self._n_merfish_bits + 1):\n                    df_barcode.rename(\n                        columns={\n                            \"intensity_mean-\" + str(i): \"bit\"\n                            + str(i).zfill(2)\n                            + \"_mean_intensity\"\n                        },\n                        inplace=True,\n                    )\n\n                on_bits = on_bits_indices + np.ones(4)\n\n                signal_mean_columns = [\n                    f\"bit{int(bit):02d}_mean_intensity\" for bit in on_bits\n                ]\n                bkd_mean_columns = [\n                    f\"bit{int(bit):02d}_mean_intensity\"\n                    for bit in range(1, self._n_merfish_bits + 1)\n                    if bit not in on_bits\n                ]\n\n                df_barcode[\"signal_mean\"] = df_barcode[signal_mean_columns].mean(\n                    axis=1\n                )\n                df_barcode[\"bkd_mean\"] = df_barcode[bkd_mean_columns].mean(axis=1)\n                df_barcode[\"s-b_mean\"] = (\n                    df_barcode[\"signal_mean\"] - df_barcode[\"bkd_mean\"]\n                )\n\n                if self._verbose &gt; 1:\n                    print(\"dataframe aggregation\")\n                if barcode_index == 0:\n                    self._df_barcodes = df_barcode.copy()\n                else:\n                    if not df_barcode.empty:\n                        self._df_barcodes = pd.concat([self._df_barcodes, df_barcode])\n                        self._df_barcodes.reset_index(drop=True, inplace=True)\n\n                del df_barcode\n                gc.collect()\n\n        del decoded_image, intensity_image\n        gc.collect()\n        cp.cuda.Stream.null.synchronize()\n        cp.get_default_memory_pool().free_all_blocks()\n        cp.get_default_pinned_memory_pool().free_all_blocks()\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._filter_all_barcodes","title":"<code>_filter_all_barcodes(fdr_target=0.05)</code>","text":"<p>Filter barcodes using a classifier and FDR target.</p> <p>Uses a MLP classifier to predict whether a barcode is a blank or not.</p> <p>TO DO: evaluate other classifiers.</p> <p>Parameters:</p> Name Type Description Default <code>fdr_target</code> <code>float</code> <p>False discovery rate target.</p> <code>0.05</code> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def _filter_all_barcodes(self, fdr_target: float = 0.05):\n    \"\"\"Filter barcodes using a classifier and FDR target.\n\n    Uses a MLP classifier to predict whether a barcode is a blank or not.\n\n    TO DO: evaluate other classifiers.\n\n    Parameters\n    ----------\n    fdr_target : float, default 0.05\n        False discovery rate target. \n    \"\"\"\n\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.neural_network import MLPClassifier\n    from sklearn.metrics import classification_report\n    from imblearn.over_sampling import SMOTE\n\n    self._df_barcodes_loaded[\"X\"] = ~self._df_barcodes_loaded[\n        \"gene_id\"\n    ].str.startswith(\"Blank\")\n    if self._is_3D:\n        columns = [\n            \"X\",\n            \"signal_mean\",\n            \"s-b_mean\",\n            \"distance_mean\",\n            \"magnitude_mean\",\n            \"inertia_tensor_eigvals-0\",\n            \"inertia_tensor_eigvals-1\",\n            \"inertia_tensor_eigvals-2\",\n        ]\n    else:\n        columns = [\n            \"X\",\n            \"signal_mean\",\n            \"s-b_mean\",\n            \"distance_mean\",\n            \"magnitude_mean\",\n            \"inertia_tensor_eigvals-0\",\n            \"inertia_tensor_eigvals-1\",\n        ]\n    df_true = self._df_barcodes_loaded[self._df_barcodes_loaded[\"X\"] == True][ #noqa\n        columns\n    ]  # noqa\n    df_false = self._df_barcodes_loaded[self._df_barcodes_loaded[\"X\"] == False][ #noqa\n        columns\n    ]  # noqa\n\n    if len(df_false) &gt; 0:\n        df_true_sampled = df_true.sample(n=len(df_false), random_state=42)\n        df_combined = pd.concat([df_true_sampled, df_false])\n        x = df_combined.drop(\"X\", axis=1)\n        y = df_combined[\"X\"]\n        X_train, X_test, y_train, y_test = train_test_split(\n            x, y, test_size=0.1, random_state=42\n        )\n\n        if self._verbose &gt; 1:\n            print(\"generating synthetic samples for class balance\")\n        smote = SMOTE(random_state=42)\n        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\n        if self._verbose &gt; 1:\n            print(\"scaling features\")\n        scaler = StandardScaler()\n        X_train_scaled = scaler.fit_transform(X_train_resampled)\n        X_test_scaled = scaler.transform(X_test)\n\n        if self._verbose &gt; 1:\n            print(\"training classifier\")\n        # logistic = LogisticRegression(solver='liblinear', random_state=42)\n        mlp = MLPClassifier(solver=\"adam\", max_iter=10000, random_state=42)\n        mlp.fit(X_train_scaled, y_train_resampled)\n        predictions = mlp.predict(X_test_scaled)\n\n        if self._verbose &gt; 1:\n            print(classification_report(y_test, predictions))\n\n        if self._verbose &gt; 1:\n            print(\"predicting on full data\")\n\n        full_data_scaled = scaler.transform(self._df_barcodes_loaded[columns[1:]])\n        self._df_barcodes_loaded[\"predicted_probability\"] = mlp.predict_proba(\n            full_data_scaled\n        )[:, 1]\n\n        if self._verbose &gt; 1:\n            print(\"filtering blanks\")\n\n        coarse_threshold = 0\n        for threshold in np.arange(0, 1, 0.1):  # Coarse step: 0.1\n            fdr = self.calculate_fdr(\n                self._df_barcodes_loaded,\n                threshold,\n                self._blank_count,\n                self._barcode_count,\n                self._verbose,\n            )\n            if fdr &lt;= fdr_target:\n                coarse_threshold = threshold\n                break\n\n        fine_threshold = coarse_threshold\n        for threshold in np.arange(\n            coarse_threshold - 0.1, coarse_threshold + 0.1, 0.01\n        ):\n            fdr = self.calculate_fdr(\n                self._df_barcodes_loaded,\n                threshold,\n                self._blank_count,\n                self._barcode_count,\n                self._verbose,\n            )\n            if fdr &lt;= fdr_target:\n                fine_threshold = threshold\n                break\n\n        df_above_threshold = self._df_barcodes_loaded[\n            self._df_barcodes_loaded[\"predicted_probability\"] &gt; fine_threshold\n        ]\n        self._df_filtered_barcodes = df_above_threshold[\n            [\n                \"tile_idx\",\n                \"gene_id\",\n                \"global_z\",\n                \"global_y\",\n                \"global_x\",\n                \"distance_mean\",\n            ]\n        ].copy()\n        self._df_filtered_barcodes[\"cell_id\"] = -1\n        self._barcodes_filtered = True\n\n        if self._verbose &gt; 1:\n            print(f\"fdr : {fdr}\")\n            print(f\"retained barcodes: {len(self._df_filtered_barcodes)}\")\n\n        del df_above_threshold, full_data_scaled\n        del (\n            mlp,\n            predictions,\n            X_train,\n            X_test,\n            y_test,\n            y_train,\n            X_train_scaled,\n            X_test_scaled,\n        )\n        del df_true, df_false, df_true_sampled, df_combined\n        gc.collect()\n    else:\n        self._df_filtered_barcodes = self._df_barcodes_loaded.copy()\n        self._df_filtered_barcodes[\"cell_id\"] = -1\n        self._df_filtered_barcodes.drop(\"X\", axis=1, inplace=True)\n        self._barcodes_filtered = True\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._filter_all_barcodes_LR","title":"<code>_filter_all_barcodes_LR(fdr_target=0.05)</code>","text":"<p>Filter barcodes using a classifier and FDR target.</p> <p>Uses a logistic regression classifier to predict whether a barcode is a blank or not.</p> <p>Parameters:</p> Name Type Description Default <code>fdr_target</code> <code>float</code> <p>False discovery rate target.</p> <code>0.05</code> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def _filter_all_barcodes_LR(self, fdr_target: float = 0.05):\n    \"\"\"Filter barcodes using a classifier and FDR target.\n\n    Uses a logistic regression classifier to predict whether a barcode is a blank or not.\n\n    Parameters\n    ----------\n    fdr_target : float, default 0.05\n        False discovery rate target. \n    \"\"\"\n\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import classification_report\n    from imblearn.over_sampling import SMOTE\n\n    self._df_barcodes_loaded[\"X\"] = ~self._df_barcodes_loaded[\n        \"gene_id\"\n    ].str.startswith(\"Blank\")\n\n    if self._is_3D:\n        columns = [\n            \"X\",\n            \"area\",\n            \"signal_mean\",\n            \"s-b_mean\",\n            \"distance_mean\",\n            \"magnitude_mean\",\n            \"inertia_tensor_eigvals-0\",\n            \"inertia_tensor_eigvals-1\",\n            \"inertia_tensor_eigvals-2\",\n        ]\n    else:\n        columns = [\n            \"X\",\n            \"area\",\n            \"signal_mean\",\n            \"s-b_mean\",\n            \"distance_mean\",\n            \"magnitude_mean\",\n            \"inertia_tensor_eigvals-0\",\n            \"inertia_tensor_eigvals-1\",\n        ]\n\n    df_true = self._df_barcodes_loaded[self._df_barcodes_loaded[\"X\"] == True][columns] #noqa\n    df_false = self._df_barcodes_loaded[self._df_barcodes_loaded[\"X\"] == False][columns] #noqa\n    if len(df_false) &gt; 1:\n        df_true_sampled = df_true.sample(n=len(df_false), random_state=42)\n        df_combined = pd.concat([df_true_sampled, df_false])\n        x = df_combined.drop(\"X\", axis=1)\n        y = df_combined[\"X\"]\n        X_train, X_test, y_train, y_test = train_test_split(\n            x, y, test_size=0.1, random_state=42\n        )\n\n        if self._verbose &gt; 1:\n            print(\"generating synthetic samples for class balance\")\n        smote = SMOTE(random_state=42)\n        #X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n        X_train_resampled = X_train.copy()\n        y_train_resampled = y_train.copy()\n\n        if self._verbose &gt; 1:\n            print(\"scaling features\")\n        scaler = StandardScaler()\n        X_train_scaled = scaler.fit_transform(X_train_resampled)\n        X_test_scaled = scaler.transform(X_test)\n\n        if self._verbose &gt; 1:\n            print(\"training classifier\")\n        logistic = LogisticRegression(solver='liblinear', random_state=42)\n        logistic.fit(X_train_scaled, y_train_resampled)\n        predictions = logistic.predict(X_test_scaled)\n\n        if self._verbose &gt; 1:\n            print(classification_report(y_test, predictions))\n\n        if self._verbose &gt; 1:\n            print(\"predicting on full data\")\n\n        full_data_scaled = scaler.transform(self._df_barcodes_loaded[columns[1:]])\n        self._df_barcodes_loaded[\"predicted_probability\"] = logistic.predict_proba(\n            full_data_scaled\n        )[:, 1]\n\n        if self._verbose &gt; 1:\n            print(\"filtering blanks\")\n\n        coarse_threshold = 0\n        for threshold in np.arange(0, 1, 0.1):\n            fdr = self.calculate_fdr(\n                self._df_barcodes_loaded,\n                threshold,\n                self._blank_count,\n                self._barcode_count,\n                self._verbose,\n            )\n            if fdr &lt;= fdr_target:\n                coarse_threshold = threshold\n                break\n\n        fine_threshold = coarse_threshold\n        for threshold in np.arange(\n            coarse_threshold - 0.1, coarse_threshold + 0.1, 0.01\n        ):\n            fdr = self.calculate_fdr(\n                self._df_barcodes_loaded,\n                threshold,\n                self._blank_count,\n                self._barcode_count,\n                self._verbose,\n            )\n            if fdr &lt;= fdr_target:\n                fine_threshold = threshold\n                break\n\n        df_above_threshold = self._df_barcodes_loaded[\n            self._df_barcodes_loaded[\"predicted_probability\"] &gt; fine_threshold\n        ]\n        self._df_filtered_barcodes = df_above_threshold[\n            [\n                \"tile_idx\",\n                \"gene_id\",\n                \"global_z\",\n                \"global_y\",\n                \"global_x\",\n                \"distance_mean\",\n            ]\n        ].copy()\n        self._df_filtered_barcodes[\"cell_id\"] = -1\n        self._barcodes_filtered = True\n\n        if self._verbose &gt; 1:\n            print(f\"fdr : {fdr}\")\n            print(f\"retained barcodes: {len(self._df_filtered_barcodes)}\")\n\n        del df_above_threshold, full_data_scaled\n        del (\n            logistic,\n            predictions,\n            X_train,\n            X_test,\n            y_test,\n            y_train,\n            X_train_scaled,\n            X_test_scaled,\n        )\n        del df_true, df_false, df_true_sampled, df_combined\n        gc.collect()\n    else:\n        self._df_filtered_barcodes = self._df_barcodes_loaded.copy()\n        self._df_filtered_barcodes[\"cell_id\"] = -1\n        self._df_filtered_barcodes.drop(\"X\", axis=1, inplace=True)\n        self._barcodes_filtered = True\n        if self._verbose &gt;= 1:\n            print(\"Insufficient Blank barcodes called for filtering.\")\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._global_normalization_vectors","title":"<code>_global_normalization_vectors(low_percentile_cut=10.0, high_percentile_cut=90.0, hot_pixel_threshold=50000, gpu_id=0)</code>","text":"<p>Calculate global normalization and background vectors.</p> <p>Parameters:</p> Name Type Description Default <code>low_percentile_cut</code> <code>float</code> <p>Lower percentile cut for background estimation.</p> <code>10.0</code> <code>high_percentile_cut</code> <code>float</code> <p>Upper percentile cut for normalization estimation.</p> <code>90.0</code> <code>hot_pixel_threshold</code> <code>int</code> <p>Threshold for hot pixel removal.</p> <code>50000</code> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def _global_normalization_vectors(\n    self,\n    low_percentile_cut: float = 10.0,\n    high_percentile_cut: float = 90.0,\n    hot_pixel_threshold: int = 50000,\n    gpu_id: int = 0\n):\n    \"\"\"Calculate global normalization and background vectors.\n\n    Parameters\n    ----------\n    low_percentile_cut : float, default 10.0\n        Lower percentile cut for background estimation.\n    high_percentile_cut : float, default 90.0\n        Upper percentile cut for normalization estimation.\n    hot_pixel_threshold : int, default 50000\n        Threshold for hot pixel removal.\n    \"\"\"\n\n    with cp.cuda.Device(gpu_id):\n        if len(self._datastore.tile_ids) &gt; 5:\n            random_tiles = sample(self._datastore.tile_ids, 5)\n        else:\n            random_tiles = self._datastore.tile_ids\n\n        normalization_vector = cp.ones(len(self._datastore.bit_ids), dtype=cp.float32)\n        background_vector = cp.zeros(len(self._datastore.bit_ids), dtype=cp.float32)\n\n        if self._verbose &gt;= 1:\n            print(\"calculate normalizations\")\n            iterable_bits = enumerate(\n                tqdm(self._datastore.bit_ids, desc=\"bit\", leave=False)\n            )\n        else:\n            iterable_bits = enumerate(self._datastore.bit_ids)\n\n        for bit_idx, bit_id in iterable_bits:\n            all_images = []\n\n            if self._verbose &gt;= 1:\n                iterable_tiles = tqdm(random_tiles, desc=\"loading tiles\", leave=False)\n            else:\n                iterable_tiles = random_tiles\n\n            for tile_id in iterable_tiles:\n                decon_image = self._datastore.load_local_registered_image(\n                    tile=tile_id, bit=bit_id, return_future=False\n                )\n                ufish_image = self._datastore.load_local_ufish_image(\n                    tile=tile_id, bit=bit_id, return_future=False\n                )\n\n                current_image = cp.where(\n                    cp.asarray(ufish_image, dtype=cp.float32) &gt; 0.1,\n                    cp.asarray(decon_image, dtype=cp.float32),\n                    0.0,\n                )\n                current_image[current_image &gt; hot_pixel_threshold] = cp.median(\n                    current_image[current_image.shape[0] // 2, :, :]\n                ).astype(cp.float32)\n                if self._z_crop:\n                    all_images.append(\n                        cp.asnumpy(\n                            current_image[self._z_range[0] : self._z_range[1], :]\n                        ).astype(np.float32)\n                    )\n                else:\n                    all_images.append(cp.asnumpy(current_image).astype(np.float32))\n                del current_image\n                cp.get_default_memory_pool().free_all_blocks()\n                gc.collect()\n\n            all_images = np.array(all_images)\n\n            if self._verbose &gt;= 1:\n                iterable_tiles = enumerate(\n                    tqdm(random_tiles, desc=\"background est.\", leave=False)\n                )\n            else:\n                iterable_tiles = enumerate(random_tiles)\n\n            low_pixels = []\n            for tile_idx, tile_id in iterable_tiles:\n                current_image = cp.asarray(all_images[tile_idx, :], dtype=cp.float32)\n                low_cutoff = cp.percentile(current_image, low_percentile_cut)\n                low_pixels.append(\n                    current_image[current_image &lt; low_cutoff]\n                    .flatten()\n                    .astype(cp.float32)\n                )\n                del current_image\n                cp.get_default_memory_pool().free_all_blocks()\n                gc.collect()\n\n            low_pixels = cp.concatenate(low_pixels, axis=0)\n            if low_pixels.shape[0] &gt; 0:\n                background_vector[bit_idx] = cp.median(low_pixels)\n            else:\n                background_vector[bit_idx] = 0\n\n            del low_pixels\n            cp.get_default_memory_pool().free_all_blocks()\n            gc.collect()\n\n            if self._verbose &gt;= 1:\n                iterable_tiles = enumerate(\n                    tqdm(random_tiles, desc=\"normalization est.\", leave=False)\n                )\n            else:\n                iterable_tiles = enumerate(random_tiles)\n\n            high_pixels = []\n            for tile_idx, tile_id in iterable_tiles:\n                current_image = (\n                    cp.asarray(all_images[tile_idx, :], dtype=cp.float32)\n                    - background_vector[bit_idx]\n                )\n                current_image[current_image &lt; 0] = 0\n                high_cutoff = cp.percentile(current_image, high_percentile_cut)\n                high_pixels.append(\n                    current_image[current_image &gt; high_cutoff]\n                    .flatten()\n                    .astype(cp.float32)\n                )\n\n                del current_image\n                cp.get_default_memory_pool().free_all_blocks()\n                gc.collect()\n\n            high_pixels = cp.concatenate(high_pixels, axis=0)\n            if high_pixels.shape[0] &gt; 0:\n                normalization_vector[bit_idx] = cp.median(high_pixels)\n            else:\n                normalization_vector[bit_idx] = 1\n\n            del high_pixels\n            cp.get_default_memory_pool().free_all_blocks()\n            gc.collect()\n\n        self._datastore.global_normalization_vector = (\n            cp.asnumpy(normalization_vector).astype(np.float32).tolist()\n        )\n        self._datastore.global_background_vector = (\n            cp.asnumpy(background_vector).astype(np.float32).tolist()\n        )\n\n        self._global_background_vector = background_vector\n        self._global_normalization_vector = normalization_vector\n        self._global_normalization_loaded = True\n\n        cp.cuda.Stream.null.synchronize()\n        cp.get_default_memory_pool().free_all_blocks()\n        cp.get_default_pinned_memory_pool().free_all_blocks()\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._iterative_normalization_vectors","title":"<code>_iterative_normalization_vectors(gpu_id=0)</code>","text":"<p>Calculate iterative normalization and background vectors.</p> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def _iterative_normalization_vectors(self, gpu_id: int = 0):\n    \"\"\"Calculate iterative normalization and background vectors.\"\"\"\n    with cp.cuda.Device(gpu_id):\n\n        keep = ~self._df_barcodes_loaded[\"gene_id\"].astype(\"string\").str.startswith(\"Blank\", na=False)\n\n        df_barcodes_loaded_no_blanks = self._df_barcodes_loaded[keep]\n\n        bit_columns = [\n            col\n            for col in df_barcodes_loaded_no_blanks.columns\n            if col.startswith(\"bit\") and col.endswith(\"_mean_intensity\")\n        ]\n\n        barcode_intensities = []\n        barcode_background = []\n        for index, row in df_barcodes_loaded_no_blanks.iterrows():\n\n            if self._smFISH == False:\n                selected_columns = [\n                    f'bit{int(row[\"on_bit_1\"]):02d}_mean_intensity',\n                    f'bit{int(row[\"on_bit_2\"]):02d}_mean_intensity',\n                    f'bit{int(row[\"on_bit_3\"]):02d}_mean_intensity',\n                    f'bit{int(row[\"on_bit_4\"]):02d}_mean_intensity',\n                ]\n            else:\n                selected_columns = [\n                    f'bit{int(row[\"on_bit_1\"]):02d}_mean_intensity'\n                ]\n\n            selected_dict = {\n                col: (row[col] if col in selected_columns else None)\n                for col in bit_columns\n            }\n            not_selected_dict = {\n                col: (row[col] if col not in selected_columns else None)\n                for col in bit_columns\n            }\n\n            barcode_intensities.append(selected_dict)\n            barcode_background.append(not_selected_dict)\n\n        df_barcode_intensities = pd.DataFrame(barcode_intensities)\n        df_barcode_background = pd.DataFrame(barcode_background)\n\n        df_barcode_intensities = df_barcode_intensities.reindex(\n            sorted(df_barcode_intensities.columns), axis=1\n        )\n        df_barcode_background = df_barcode_background.reindex(\n            sorted(df_barcode_background.columns), axis=1\n        )\n\n        barcode_based_normalization_vector = np.round(\n            df_barcode_intensities.median(skipna=True).to_numpy(\n                dtype=np.float32, copy=True\n            ),\n            1,\n        )\n        barcode_based_background_vector = np.round(\n            df_barcode_background.median(skipna=True).to_numpy(\n                dtype=np.float32, copy=True\n            ),\n            1,\n        )\n\n        barcode_based_normalization_vector = np.nan_to_num(\n            barcode_based_normalization_vector, 1.0\n        )\n        barcode_based_normalization_vector = np.where(\n            barcode_based_normalization_vector == 0.0,\n            1.0,\n            barcode_based_normalization_vector,\n        )\n        barcode_based_background_vector = np.nan_to_num(\n            barcode_based_background_vector, 0.0\n        )\n\n        if (\n            self._iterative_background_vector is None\n            and self._iterative_normalization_vector is None\n        ):\n            old_iterative_background_vector = np.round(\n                cp.asnumpy(self._global_background_vector[0 : self._n_merfish_bits]), 1\n            )\n            old_iterative_normalization_vector = np.round(\n                cp.asnumpy(self._global_normalization_vector[0 : self._n_merfish_bits]),\n                1,\n            )\n        else:\n            old_iterative_background_vector = np.asarray(\n                cp.asnumpy(self._iterative_background_vector)\n            )\n            old_iterative_normalization_vector = np.asarray(\n                cp.asnumpy(self._iterative_normalization_vector)\n            )\n\n        diff_iterative_background_vector = np.round(\n            np.abs(barcode_based_background_vector - old_iterative_background_vector), 1\n        )\n        diff_iterative_normalization_vector = np.round(\n            np.abs(\n                barcode_based_normalization_vector - old_iterative_normalization_vector\n            ),\n            1,\n        )\n        self._datastore.iterative_background_vector = (\n            barcode_based_background_vector.astype(np.float32)\n        )\n        self._datastore.iterative_normalization_vector = (\n            barcode_based_normalization_vector.astype(np.float32)\n        )\n\n        if self._verbose &gt; 1:\n            print(time_stamp(), \"Normalizations updated.\")\n            print(\"---\")\n            print(f\"Background delta: {diff_iterative_background_vector}\")\n            print(f\"Background estimate: {barcode_based_background_vector}\")\n            print(\"---\")\n            print(f\"Foreground delta: {diff_iterative_normalization_vector}\")\n            print(f\"Foreground estimate: {barcode_based_normalization_vector}\")\n            print(\"---\")\n            print(f\"Num. barcodes: {len(df_barcodes_loaded_no_blanks)}\")\n            print(\"---\")\n\n        self._iterative_normalization_vector = barcode_based_normalization_vector\n        self._iterative_background_vector = barcode_based_background_vector\n        self._datastore.iterative_normalization_vector = (\n            barcode_based_normalization_vector\n        )\n        self._datastore.iterative_background_vector = barcode_based_background_vector\n\n        self._iterative_normalization_loaded = True\n\n        del df_barcodes_loaded_no_blanks\n        gc.collect()\n        cp.cuda.Stream.null.synchronize()\n        cp.get_default_memory_pool().free_all_blocks()\n        cp.get_default_pinned_memory_pool().free_all_blocks()\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._load_all_barcodes","title":"<code>_load_all_barcodes()</code>","text":"<p>Load all barcodes from datastore.</p> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def _load_all_barcodes(self):\n    \"\"\"Load all barcodes from datastore.\"\"\"\n\n    if self._optimize_normalization_weights:\n        decoded_dir_path = self._temp_dir\n\n        tile_files = decoded_dir_path.glob(\"*.parquet\")\n        tile_files = sorted(tile_files, key=lambda x: x.name)\n\n        if self._verbose &gt;= 1:\n            iterable_files = tqdm(tile_files, desc=\"tile\", leave=False)\n        else:\n            iterable_files = tile_files\n\n        tile_data = [\n            pd.read_parquet(parquet_file) for parquet_file in iterable_files\n        ]\n        self._df_barcodes_loaded = pd.concat(tile_data)\n    elif self._load_tile_decoding:\n        tile_data = []\n        for tile_id in self._datastore.tile_ids:\n            tile_data.append(self._datastore.load_local_decoded_spots(tile_id))\n        self._df_barcodes_loaded = pd.concat(tile_data)\n    else:\n        self._df_filtered_barcodes = (\n            self._datastore.load_global_filtered_decoded_spots()\n        )\n        self._barcodes_filtered = True\n\n    self._df_barcodes_loaded = self._df_barcodes_loaded[self._df_barcodes_loaded[\"gene_id\"].notna() &amp; self._df_barcodes_loaded[\"gene_id\"].astype(str).str.strip().ne(\"\")]\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._load_bit_data","title":"<code>_load_bit_data(ufish_threshold=0.1)</code>","text":"<p>Load raw data for all bits in the tile.</p> <p>Parameters:</p> Name Type Description Default <code>ufish_threshold</code> <code>Optional[float]</code> <p>Threshold for ufish image.</p> <code>0.5</code> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def _load_bit_data(self, ufish_threshold: Optional[float] = 0.1):\n    \"\"\"Load raw data for all bits in the tile.\n\n    Parameters\n    ----------\n    ufish_threshold : Optional[float], default 0.5\n        Threshold for ufish image.\n    \"\"\"\n\n    if self._verbose &gt; 1:\n        print(\"load raw data\")\n        iterable_bits = tqdm(\n            self._datastore.bit_ids[0 : self._n_merfish_bits],\n            desc=\"bit\",\n            leave=False,\n        )\n    elif self._verbose &gt;= 1:\n        iterable_bits = tqdm(\n            self._datastore.bit_ids[0 : self._n_merfish_bits],\n            desc=\"loading\",\n            leave=False,\n        )\n    else:\n        iterable_bits = self._datastore.bit_ids[0 : self._n_merfish_bits]\n\n    images = []\n    self._em_wvl = []\n    for bit_id in iterable_bits:\n        decon_image = self._datastore.load_local_registered_image(\n            tile=self._tile_idx,\n            bit=bit_id,\n        )\n        ufish_image = self._datastore.load_local_ufish_image(\n            tile=self._tile_idx,\n            bit=bit_id,\n        )\n\n        if self._z_crop:\n            current_mask = np.asarray(\n                ufish_image[self._z_range[0] : self._z_range[1], :].result(),\n                dtype=np.float32,\n            )\n            images.append(\n                np.where(\n                    current_mask &gt; ufish_threshold,\n                    np.asarray(\n                        decon_image[\n                            self._z_range[0] : self._z_range[1], :\n                        ].result(),\n                        dtype=np.float32,\n                    ),\n                    0,\n                )\n            )\n        else:\n            current_mask = np.asarray(ufish_image.result(), dtype=np.float32)\n            images.append(\n                np.where(\n                    current_mask &gt; ufish_threshold,\n                    np.asarray(decon_image.result(), dtype=np.float32),\n                    0,\n                )\n            )\n        self._em_wvl.append(\n            self._datastore.load_local_wavelengths_um(\n                tile=self._tile_idx,\n                bit=bit_id,\n            )[1]\n        )\n\n    self._image_data = np.stack(images, axis=0)\n    voxel_size_zyx_um = self._datastore.voxel_size_zyx_um\n    self._pixel_size = voxel_size_zyx_um[1]\n    self._axial_step = voxel_size_zyx_um[0]\n\n    affine, origin, spacing = self._datastore.load_global_coord_xforms_um(\n        tile=self._tile_idx\n    )\n    if affine is None or origin is None or spacing is None:\n        if self._is_3D:\n            affine = np.eye(4)\n            origin = self._datastore.load_local_stage_position_zyx_um(\n                tile=self._tile_idx, round=0\n            )\n            spacing = self._datastore.voxel_size_zyx_um\n        else:\n            affine = np.eye(4)\n            origin = self._datastore.load_local_stage_position_zyx_um(\n                tile=self._tile_idx, round=0\n            )\n            origin = [0, origin[0], origin[1]]\n            spacing = self._datastore.voxel_size_zyx_um\n\n    self._affine = affine\n    self._origin = origin\n    self._spacing = spacing\n\n    del images\n    gc.collect()\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._load_codebook","title":"<code>_load_codebook()</code>","text":"<p>Load and parse codebook into gene_id and codeword matrix.</p> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def _load_codebook(self):\n    \"\"\"Load and parse codebook into gene_id and codeword matrix.\"\"\"\n\n    self._df_codebook = self._datastore.codebook.copy()\n    self._df_codebook.fillna(0, inplace=True)\n\n    self._blank_count = (\n        self._df_codebook[\"gene_id\"].str.lower().str.startswith(\"blank\").sum()\n    )\n\n    if not (self._include_blanks):\n        self._df_codebook.drop(\n            self._df_codebook[self._df_codebook[0].str.startswith(\"Blank\")].index,\n            inplace=True,\n        )\n\n    self._codebook_matrix = self._df_codebook.iloc[:, 1:].to_numpy().astype(int)\n    self._gene_ids = self._df_codebook.iloc[:, 0].tolist()\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._load_global_normalization_vectors","title":"<code>_load_global_normalization_vectors(gpu_id=0)</code>","text":"<p>Load or calculate global normalization and background vectors.</p> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def _load_global_normalization_vectors(self,gpu_id: int = 0):\n    \"\"\"Load or calculate global normalization and background vectors.\"\"\"\n    with cp.cuda.Device(gpu_id):\n        normalization_vector = self._datastore.global_normalization_vector\n        background_vector = self._datastore.global_background_vector\n        if (normalization_vector is not None and background_vector is not None):\n            self._global_normalization_vector = cp.asarray(normalization_vector)\n            self._global_background_vector = cp.asarray(background_vector)\n            self._global_normalization_loaded = True\n        else:\n            self._global_normalization_vectors()\n\n        cp.cuda.Stream.null.synchronize()\n        cp.get_default_memory_pool().free_all_blocks()\n        cp.get_default_pinned_memory_pool().free_all_blocks()\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._load_iterative_normalization_vectors","title":"<code>_load_iterative_normalization_vectors(gpu_id=0)</code>","text":"<p>Load or calculate iterative normalization and background vectors.</p> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def _load_iterative_normalization_vectors(self,gpu_id: int = 0):\n    \"\"\"Load or calculate iterative normalization and background vectors.\"\"\"\n    with cp.cuda.Device(gpu_id):\n        normalization_vector = self._datastore.iterative_normalization_vector\n        background_vector = self._datastore.iterative_background_vector\n\n        if normalization_vector is not None and background_vector is not None:\n            background_vector = np.nan_to_num(background_vector, 0.0)\n            normalization_vector = np.nan_to_num(normalization_vector, 1.0)\n            self._iterative_normalization_vector = cp.asarray(normalization_vector)\n            self._iterative_background_vector = cp.asarray(background_vector)\n            self._iterative_normalization_loaded = True\n        else:\n            self._iterative_normalization_vectors()\n\n        cp.cuda.Stream.null.synchronize()\n        cp.get_default_memory_pool().free_all_blocks()\n        cp.get_default_pinned_memory_pool().free_all_blocks()\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._lp_filter","title":"<code>_lp_filter(gpu_id=0, sigma=(3, 1, 1))</code>","text":"<p>Apply low-pass filter to the raw data.</p> <p>Parameters:</p> Name Type Description Default <code>sigma</code> <code>Tuple[int, int, int]</code> <p>Sigma values for Gaussian filter.</p> <code>[3,1,1]</code> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def _lp_filter(self, gpu_id: int = 0, sigma=(3, 1, 1)):\n    \"\"\"Apply low-pass filter to the raw data.\n\n    Parameters\n    ----------\n    sigma : Tuple[int, int, int], default [3,1,1]\n        Sigma values for Gaussian filter.\n    \"\"\"\n\n    with cp.cuda.Device(gpu_id):\n        self._image_data_lp = self._image_data.copy()\n\n        if self._verbose &gt; 1:\n            print(\"lowpass filter\")\n            iterable_lp = tqdm(\n                range(self._image_data_lp.shape[0]), desc=\"bit\", leave=False\n            )\n        elif self._verbose &gt;= 1:\n            iterable_lp = tqdm(\n                range(self._image_data_lp.shape[0]), desc=\"lowpass\", leave=False\n            )\n        else:\n            iterable_lp = range(self._image_data_lp.shape[0])\n\n        for i in iterable_lp:\n            if self._is_3D:\n                image_data_cp = cp.asarray(self._image_data[i, :], dtype=cp.float32)\n                max_image_data = cp.asnumpy(\n                    cp.max(image_data_cp, axis=(0, 1, 2))\n                ).astype(np.float32)\n                if max_image_data == 0:\n                    self._image_data_lp[i, :, :, :] = 0\n                else:\n                    self._image_data_lp[i, :, :, :] = cp.asnumpy(\n                        gaussian_filter(image_data_cp, sigma=sigma)\n                    ).astype(np.float32)\n                    max_image_data_lp = np.max(\n                        self._image_data_lp[i, :, :, :], axis=(0, 1, 2)\n                    )\n                    self._image_data_lp[i, :, :, :] = self._image_data_lp[\n                        i, :, :, :\n                    ] * (max_image_data / max_image_data_lp)\n            else:\n                for z_idx in range(self._image_data.shape[1]):\n                    image_data_cp = cp.asarray(\n                        self._image_data[i, z_idx, :], dtype=cp.float32\n                    )\n                    max_image_data = cp.asnumpy(\n                        cp.max(image_data_cp, axis=(0, 1))\n                    ).astype(np.float32)\n                    if max_image_data == 0:\n                        self._image_data_lp[i, z_idx, :, :] = 0\n                    else:\n                        self._image_data_lp[i, z_idx, :, :] = cp.asnumpy(\n                            gaussian_filter(image_data_cp, sigma=(sigma[1], sigma[2]))\n                        ).astype(np.float32)\n                        max_image_data_lp = np.max(\n                            self._image_data_lp[i, z_idx, :, :], axis=(0, 1)\n                        )\n                        self._image_data_lp[i, z_idx, :, :] = self._image_data_lp[\n                            i, z_idx, :, :\n                        ] * (max_image_data / max_image_data_lp)\n\n        self._filter_type = \"lp\"\n\n        del image_data_cp\n        del self._image_data\n        gc.collect()\n        cp.cuda.Stream.null.synchronize()\n        cp.get_default_memory_pool().free_all_blocks()\n        cp.get_default_pinned_memory_pool().free_all_blocks()\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._normalize_codebook","title":"<code>_normalize_codebook(gpu_id=0, include_errors=False)</code>","text":"<p>Normalize each codeword by L2 norm.</p> <p>Parameters:</p> Name Type Description Default <code>include_errors</code> <code>bool</code> <p>Include single-bit errors as unique barcodes in the decoding matrix.</p> <code>False</code> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def _normalize_codebook(self, gpu_id: int = 0, include_errors: bool = False):\n    \"\"\"Normalize each codeword by L2 norm.\n\n    Parameters\n    ----------\n    include_errors : bool, default False\n        Include single-bit errors as unique barcodes in the decoding matrix.\"\"\"\n\n    with cp.cuda.Device(gpu_id):\n        self._barcode_set = cp.asarray(\n            self._codebook_matrix[:, 0 : self._n_merfish_bits]\n        )\n        magnitudes = cp.linalg.norm(self._barcode_set, axis=1, keepdims=True)\n        magnitudes[magnitudes == 0] = 1  # ensure with smFISH rounds have magnitude 1\n\n        if not include_errors:\n            # Normalize directly using broadcasting\n            normalized_barcodes = self._barcode_set / magnitudes\n            return cp.asnumpy(normalized_barcodes)\n        else:\n            # Pre-compute the normalized barcodes\n            normalized_barcodes = self._barcode_set / magnitudes\n\n            # Initialize an empty list to hold all barcodes with single errors\n            barcodes_with_single_errors = [normalized_barcodes]\n\n            # Generate single-bit errors\n            for bit_index in range(self._barcode_set.shape[1]):\n                flipped_barcodes = self._barcode_set.copy()\n                flipped_barcodes[:, bit_index] = 1 - flipped_barcodes[:, bit_index]\n                flipped_magnitudes = cp.sqrt(cp.sum(flipped_barcodes**2, axis=1))\n                flipped_magnitudes = cp.where(\n                    flipped_magnitudes == 0, 1, flipped_magnitudes\n                )\n                normalized_flipped = flipped_barcodes / flipped_magnitudes\n                barcodes_with_single_errors.append(normalized_flipped)\n\n            # Stack all barcodes (original normalized + with single errors)\n            all_barcodes = cp.vstack(barcodes_with_single_errors)\n\n            cp.cuda.Stream.null.synchronize()\n            cp.get_default_memory_pool().free_all_blocks()\n            cp.get_default_pinned_memory_pool().free_all_blocks()\n\n            return cp.asnumpy(all_barcodes)\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._normalize_pixel_traces","title":"<code>_normalize_pixel_traces(pixel_traces, gpu_id=0)</code>  <code>staticmethod</code>","text":"<p>Normalize pixel traces by L2 norm.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_traces</code> <code>Union[ndarray, ndarray]</code> <p>Pixel traces to normalize.</p> required <p>Returns:</p> Name Type Description <code>normalized_traces</code> <code>ndarray</code> <p>Normalized pixel traces.</p> <code>norms</code> <code>ndarray</code> <p>L2 norms of pixel traces.</p> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>@staticmethod\ndef _normalize_pixel_traces(\n    pixel_traces: Union[np.ndarray, cp.ndarray],\n    gpu_id: int = 0\n) -&gt; Tuple[cp.ndarray, cp.ndarray]:\n    \"\"\"Normalize pixel traces by L2 norm.\n\n    Parameters\n    ----------\n    pixel_traces : Union[np.ndarray, cp.ndarray]\n        Pixel traces to normalize.\n\n    Returns\n    -------\n    normalized_traces : cp.ndarray\n        Normalized pixel traces.\n    norms : cp.ndarray\n        L2 norms of pixel traces.    \n    \"\"\"\n\n    with cp.cuda.Device(gpu_id):\n        if isinstance(pixel_traces, np.ndarray):\n            pixel_traces = cp.asarray(pixel_traces, dtype=cp.float32)\n\n        norms = cp.linalg.norm(pixel_traces, axis=0)\n        norms = cp.where(norms == 0, np.inf, norms)\n        normalized_traces = pixel_traces / norms\n        norms = cp.where(norms == np.inf, -1, norms)\n\n        cp.cuda.Stream.null.synchronize()\n        cp.get_default_memory_pool().free_all_blocks()\n        cp.get_default_pinned_memory_pool().free_all_blocks()\n\n        return normalized_traces, norms\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._reformat_barcodes_for_baysor","title":"<code>_reformat_barcodes_for_baysor()</code>","text":"<p>Reformat barcodes for Baysor and save to datastore.</p> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def _reformat_barcodes_for_baysor(self):\n    \"\"\"Reformat barcodes for Baysor and save to datastore.\"\"\"\n\n    if self._barcodes_filtered:\n        missing_columns = [\n            col\n            for col in [\n                \"gene_id\",\n                \"global_z\",\n                \"global_y\",\n                \"global_x\",\n                \"cell_id\",\n                \"tile_idx\",\n                \"distance_mean\",\n            ]\n            if col not in self._df_filtered_barcodes.columns\n        ]\n        if missing_columns:\n            print(f\"The following columns are missing: {missing_columns}\")\n        baysor_df = self._df_filtered_barcodes[\n            [\n                \"gene_id\",\n                \"global_z\",\n                \"global_y\",\n                \"global_x\",\n                \"cell_id\",\n                \"tile_idx\",\n                \"distance_mean\",\n            ]\n        ].copy()\n        baysor_df.rename(\n            columns={\n                \"gene_id\": \"feature_name\",\n                \"global_x\": \"x_location\",\n                \"global_y\": \"y_location\",\n                \"global_z\": \"z_location\",\n                \"barcode_id\": \"codeword_index\",\n                \"tile_idx\": \"fov_name\",\n                \"distance_mean\": \"qv\",\n            },\n            inplace=True,\n        )\n\n        baysor_df[\"cell_id\"] = baysor_df[\"cell_id\"] + 1\n        baysor_df[\"transcript_id\"] = pd.util.hash_pandas_object(\n            baysor_df, index=False\n        )\n        baysor_df[\"is_gene\"] = ~baysor_df[\"feature_name\"].str.contains(\n            \"Blank\", na=False\n        )\n        self._datastore.save_spots_prepped_for_baysor(baysor_df)\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._remove_duplicates_in_tile_overlap","title":"<code>_remove_duplicates_in_tile_overlap(radius=0.75)</code>","text":"<p>Remove duplicates in tile overlap.</p> <p>Parameters:</p> Name Type Description Default <code>radius</code> <code>float</code> <p>3D radius, in microns, for duplicate removal.</p> <code>0.75 </code> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def _remove_duplicates_in_tile_overlap(self, radius: float = 0.75):\n    \"\"\"Remove duplicates in tile overlap.\n\n    Parameters\n    ----------\n    radius : float, default 0.75 \n        3D radius, in microns, for duplicate removal. \n    \"\"\"\n\n    self._df_filtered_barcodes.reset_index(drop=True, inplace=True)\n\n    coords = self._df_filtered_barcodes[[\"global_z\", \"global_y\", \"global_x\"]].values\n    tile_idxs = self._df_filtered_barcodes[\"tile_idx\"].values\n\n    tree = cKDTree(coords)\n    pairs = tree.query_pairs(radius)\n\n    rows_to_drop = set()\n    distances = []\n    for i, j in pairs:\n        if tile_idxs[i] != tile_idxs[j]:\n            if (\n                self._df_filtered_barcodes.loc[i, \"distance_mean\"]\n                &lt;= self._df_filtered_barcodes.loc[j, \"distance_mean\"]\n            ):\n                rows_to_drop.add(j)\n                distances.append(self._df_filtered_barcodes.loc[j, \"distance_mean\"])\n            else:\n                rows_to_drop.add(i)\n                distances.append(self._df_filtered_barcodes.loc[i, \"distance_mean\"])\n\n    self._df_filtered_barcodes.drop(rows_to_drop, inplace=True)\n    self._df_filtered_barcodes.reset_index(drop=True, inplace=True)\n\n    avg_distance = np.mean(distances) if distances else 0\n    dropped_count = len(rows_to_drop)\n\n    if self._verbose &gt; 1:\n        print(\n            \"Average distance metric of dropped points (overlap): \"\n            + str(avg_distance)\n        )\n        print(\"Dropped points: \" + str(dropped_count))\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._remove_duplicates_within_tile","title":"<code>_remove_duplicates_within_tile(radius_xy=0.75, radius_z=0.5)</code>","text":"<p>Collapse near-duplicate detections within each tile and same gene_id.</p> <p>Two rows are considered neighbors if and only if: 1) They belong to the same tile (<code>tile_idx</code>), 2) Their XY separation is within <code>radius_xy</code> (microns), 3) Their absolute Z separation is within <code>radius_z</code> (microns), and 4) Their identity matches (<code>gene_id</code> is equal).</p> <p>For each connected component (cluster) under this neighbor relation, keep exactly one row: the one with the smallest <code>distance_mean</code>. Ties on <code>distance_mean</code> are broken deterministically by the original row index (lower index wins).</p> <p>Parameters:</p> Name Type Description Default <code>radius_xy</code> <code>float</code> <p>Neighborhood radius in the XY plane, in microns.</p> <code>0.75</code> <code>radius_z</code> <code>float</code> <p>Neighborhood half-extent along Z, in microns.</p> <code>0.50</code> Modifies <p>self._df_filtered_barcodes : pandas.DataFrame     Drops non-winning rows per cluster; resets index at the end.</p> Notes <p>Expected columns: <code>global_z</code>, <code>global_y</code>, <code>global_x</code>, <code>tile_idx</code>, <code>gene_id</code>, <code>distance_mean</code>.</p> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def _remove_duplicates_within_tile(\n    self,\n    radius_xy: float = 0.75,\n    radius_z: float = 0.50,\n) -&gt; None:\n    \"\"\"Collapse near-duplicate detections within each tile *and same gene_id*.\n\n    Two rows are considered neighbors if and only if:\n    1) They belong to the same tile (``tile_idx``),\n    2) Their XY separation is within ``radius_xy`` (microns),\n    3) Their absolute Z separation is within ``radius_z`` (microns), and\n    4) Their identity matches (``gene_id`` is equal).\n\n    For each connected component (cluster) under this neighbor relation,\n    keep exactly one row: the one with the smallest ``distance_mean``.\n    Ties on ``distance_mean`` are broken deterministically by the original row\n    index (lower index wins).\n\n    Parameters\n    ----------\n    radius_xy : float, default 0.75\n        Neighborhood radius in the XY plane, in microns.\n    radius_z : float, default 0.50\n        Neighborhood half-extent along Z, in microns.\n\n    Modifies\n    --------\n    self._df_filtered_barcodes : pandas.DataFrame\n        Drops non-winning rows per cluster; resets index at the end.\n\n    Notes\n    -----\n    Expected columns: ``global_z``, ``global_y``, ``global_x``,\n    ``tile_idx``, ``gene_id``, ``distance_mean``.\n    \"\"\"\n    try:\n        df = self._df_filtered_barcodes\n        filtered = True\n    except:\n        df = self._df_barcodes_loaded\n        filtered = False\n    if df.empty or len(df) &lt; 2:\n        return\n\n    # Stable order &amp; deterministic tie-breaks\n    df.reset_index(drop=True, inplace=True)\n\n    coords = df[[\"global_z\", \"global_y\", \"global_x\"]].to_numpy(dtype=float, copy=False)\n    tiles = df[\"tile_idx\"].to_numpy()\n    genes = df[\"gene_id\"].to_numpy()  # dtype can be int/str/object; equality works elementwise\n    dmean = df[\"distance_mean\"].to_numpy(dtype=float, copy=False)\n\n    rows_to_drop: set[int] = set()\n\n    # Union\u2013Find (Disjoint Set)\n    def uf_find(parent: np.ndarray, x: int) -&gt; int:\n        while parent[x] != x:\n            parent[x] = parent[parent[x]]\n            x = parent[x]\n        return x\n\n    def uf_union(parent: np.ndarray, rank: np.ndarray, a: int, b: int) -&gt; None:\n        ra, rb = uf_find(parent, a), uf_find(parent, b)\n        if ra == rb:\n            return\n        if rank[ra] &lt; rank[rb]:\n            parent[ra] = rb\n        elif rank[ra] &gt; rank[rb]:\n            parent[rb] = ra\n        else:\n            parent[rb] = ra\n            rank[ra] += 1\n\n    # Process each tile independently\n    for t in np.unique(tiles):\n        local_idx = np.flatnonzero(tiles == t)\n        if local_idx.size &lt; 2:\n            continue\n\n        sub = coords[local_idx]\n        z_local = sub[:, 0]\n        xy_local = sub[:, 1:3]       # (Y, X)\n        genes_local = genes[local_idx]\n\n        # 1) XY-near candidate pairs\n        tree = cKDTree(xy_local)\n        pairs_local = tree.query_pairs(r=radius_xy)\n        if not pairs_local:\n            continue\n\n        # 2) Filter by Z window *and same gene_id*\n        filtered_pairs = [\n            (i, j)\n            for (i, j) in pairs_local\n            if (abs(z_local[i] - z_local[j]) &lt;= radius_z) and (genes_local[i] == genes_local[j])\n        ]\n        if not filtered_pairs:\n            continue\n\n        # 3) Union\u2013Find over local nodes\n        n = local_idx.size\n        parent = np.arange(n)\n        rank = np.zeros(n, dtype=np.int8)\n        for i_loc, j_loc in filtered_pairs:\n            uf_union(parent, rank, i_loc, j_loc)\n\n        # 4) Gather components\n        comps: dict[int, list[int]] = {}\n        for i_loc in range(n):\n            r = uf_find(parent, i_loc)\n            comps.setdefault(r, []).append(i_loc)\n\n        # 5) Keep exactly one best per multi-member component\n        for members in comps.values():\n            if len(members) &lt; 2:\n                continue\n            glob_members = local_idx[np.asarray(members)]\n            # Lexicographic: primary key is distance_mean, tie-breaker is original index\n            best_global = glob_members[np.lexsort((glob_members, dmean[glob_members]))][0]\n            for g in glob_members:\n                if g != best_global:\n                    rows_to_drop.add(g)\n\n    if rows_to_drop:\n        df.drop(index=list(rows_to_drop), inplace=True)\n        df.reset_index(drop=True, inplace=True)\n\n    if getattr(self, \"_verbose\", 0) &gt; 1:\n        dropped = dmean[list(rows_to_drop)] if rows_to_drop else np.array([], dtype=float)\n        avg = float(dropped.mean()) if dropped.size else 0.0\n        print(\n            \"Average distance metric of dropped points (within-tile, same gene, clusters): \"\n            + str(avg)\n        )\n        print(\"Dropped points: \" + str(len(rows_to_drop)))\n\n    if filtered:\n        del self._df_filtered_barcodes\n        self._df_filtered_barcodes = df.copy()\n    else:\n        del self._df_barcodes_loaded\n        self._df_barcodes_loaded = df.copy()\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._save_barcodes","title":"<code>_save_barcodes()</code>","text":"<p>Save barcodes to datastore.</p> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def _save_barcodes(self):\n    \"\"\"Save barcodes to datastore.\"\"\"\n\n    if self._verbose &gt; 1:\n        print(\"save barcodes\")\n\n    if self._optimize_normalization_weights:\n        decoded_dir_path = self._temp_dir\n        decoded_dir_path.mkdir(parents=True, exist_ok=True)\n        temp_decoded_path = decoded_dir_path / Path(\n            \"tile\" + str(self._tile_idx).zfill(3) + \"_temp_decoded.parquet\"\n        )\n        self._df_barcodes.to_parquet(temp_decoded_path)\n    else:\n        if not (self._barcodes_filtered):\n            self._datastore.save_local_decoded_spots(\n                self._df_barcodes, tile=self._tile_idx\n            )\n        else:\n            self._datastore.save_global_filtered_decoded_spots(\n                self._df_filtered_barcodes\n            )\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._scale_pixel_traces","title":"<code>_scale_pixel_traces(pixel_traces, background_vector, normalization_vector, merfish_bits=16, gpu_id=0)</code>  <code>staticmethod</code>","text":"<p>Scale pixel traces using background and normalization vectors.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_traces</code> <code>Union[ndarray, ndarray]</code> <p>Pixel traces to scale.</p> required <code>background_vector</code> <code>Union[ndarray, ndarray]</code> <p>Background vector.</p> required <code>normalization_vector</code> <code>Union[ndarray, ndarray]</code> <p>Normalization vector.</p> required <code>merfish_bits</code> <code>int = 16</code> <p>Number of MERFISH bits. Default 16. Assume MERFISH bits are [0, merfish_bits].</p> <code>16</code> <p>Returns:</p> Name Type Description <code>scaled_traces</code> <code>ndarray</code> <p>Scaled pixel traces.</p> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>@staticmethod\ndef _scale_pixel_traces(\n    pixel_traces: Union[np.ndarray, cp.ndarray],\n    background_vector: Union[np.ndarray, cp.ndarray],\n    normalization_vector: Union[np.ndarray, cp.ndarray],\n    merfish_bits=16,\n    gpu_id: int = 0\n) -&gt; cp.ndarray:\n    \"\"\"Scale pixel traces using background and normalization vectors.\n\n    Parameters\n    ----------\n    pixel_traces : Union[np.ndarray, cp.ndarray]\n        Pixel traces to scale.\n    background_vector : Union[np.ndarray, cp.ndarray]\n        Background vector.\n    normalization_vector : Union[np.ndarray, cp.ndarray]\n        Normalization vector.\n    merfish_bits : int = 16\n        Number of MERFISH bits. Default 16. Assume MERFISH bits are [0, merfish_bits].\n\n    Returns\n    -------\n    scaled_traces : cp.ndarray\n        Scaled pixel traces.\n    \"\"\"\n\n    with cp.cuda.Device(gpu_id):\n        if isinstance(pixel_traces, np.ndarray):\n            pixel_traces = cp.asarray(pixel_traces, dtype=cp.float32)\n        if isinstance(background_vector, np.ndarray):\n            background_vector = cp.asarray(background_vector, dtype=cp.float32)\n        if isinstance(normalization_vector, np.ndarray):\n            normalization_vector = cp.asarray(normalization_vector, dtype=cp.float32)\n\n        background_vector = background_vector[0:merfish_bits]\n        normalization_vector = normalization_vector[0:merfish_bits]\n\n        cp.cuda.Stream.null.synchronize()\n        cp.get_default_memory_pool().free_all_blocks()\n        cp.get_default_pinned_memory_pool().free_all_blocks()\n\n        return (pixel_traces - background_vector[:, cp.newaxis]) / normalization_vector[\n            :, cp.newaxis\n        ]\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder._warp_pixel","title":"<code>_warp_pixel(pixel_space_point, spacing, origin, affine)</code>  <code>staticmethod</code>","text":"<p>Warp pixel space point to physical space point.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_space_point</code> <code>ndarray</code> <p>Pixel space point.</p> required <code>spacing</code> <code>ndarray</code> <p>Spacing.</p> required <code>origin</code> <code>ndarray</code> <p>Origin.</p> required <code>affine</code> <code>ndarray</code> <p>Affine transformation matrix.</p> required <p>Returns:</p> Name Type Description <code>registered_space_point</code> <code>ndarray</code> <p>Registered space point.</p> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>@staticmethod\ndef _warp_pixel(\n    pixel_space_point: np.ndarray,\n    spacing: np.ndarray,\n    origin: np.ndarray,\n    affine: np.ndarray,\n) -&gt; np.ndarray:\n    \"\"\"Warp pixel space point to physical space point.\n\n    Parameters\n    ----------\n    pixel_space_point : np.ndarray\n        Pixel space point.\n    spacing : np.ndarray\n        Spacing.\n    origin : np.ndarray\n        Origin.\n    affine : np.ndarray \n        Affine transformation matrix.\n\n    Returns\n    -------\n    registered_space_point : np.ndarray\n        Registered space point.\n    \"\"\"\n\n    physical_space_point = pixel_space_point * spacing + origin\n    registered_space_point = (\n        np.array(affine) @ np.array(list(physical_space_point) + [1])\n    )[:-1]\n\n\n\n    return registered_space_point\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder.calculate_fdr","title":"<code>calculate_fdr(df, threshold, blank_count, barcode_count, verbose=False)</code>  <code>staticmethod</code>","text":"<p>Calculate false discovery rate.</p> <p>(# noncoding found ) / (# noncoding in codebook) / (# coding found) / (# coding in codebook)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe containing decoded spots.</p> required <code>threshold</code> <code>float</code> <p>Threshold for predicted probability.</p> required <code>blank_count</code> <code>int</code> <p>Number of blank barcodes.</p> required <code>barcode_count</code> <code>int</code> <p>Number of barcodes.</p> required <code>verbose</code> <code>bool = False</code> <p>Verbose output. Default False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>fdr</code> <code>float</code> <p>False discovery rate.</p> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>@staticmethod\ndef calculate_fdr(\n    df: pd.DataFrame, \n    threshold: float, \n    blank_count: int, \n    barcode_count: int, \n    verbose: bool = False) -&gt; float:\n    \"\"\"Calculate false discovery rate.\n\n    (# noncoding found ) / (# noncoding in codebook) / (# coding found) / (# coding in codebook)\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Dataframe containing decoded spots.\n    threshold : float\n        Threshold for predicted probability.\n    blank_count : int\n        Number of blank barcodes.\n    barcode_count : int\n        Number of barcodes.\n    verbose : bool = False\n        Verbose output. Default False.\n\n    Returns\n    -------\n    fdr : float\n        False discovery rate.\n    \"\"\"\n\n    if threshold &gt;= 0:\n        df[\"prediction\"] = df[\"predicted_probability\"] &gt; threshold\n\n        coding = df[\n            (~df[\"gene_id\"].str.startswith(\"Blank\"))\n            &amp; (df[\"predicted_probability\"] &gt; threshold)\n        ].shape[0]\n        noncoding = df[\n            (df[\"gene_id\"].str.startswith(\"Blank\"))\n            &amp; (df[\"predicted_probability\"] &gt; threshold)\n        ].shape[0]\n    else:\n        coding = df[(~df[\"gene_id\"].str.startswith(\"Blank\"))].shape[0]\n        noncoding = df[(df[\"gene_id\"].str.startswith(\"Blank\"))].shape[0]\n\n    if coding &gt; 0:\n        fdr = (noncoding / blank_count) / (coding / (barcode_count - blank_count))\n    else:\n        fdr = np.inf\n\n    if verbose &gt; 1:\n        print(f\"threshold: {threshold}\")\n        print(f\"coding: {coding}\")\n        print(f\"noncoding: {noncoding}\")\n        print(f\"fdr: {fdr}\")\n\n    return fdr\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder.decode_all_tiles","title":"<code>decode_all_tiles(assign_to_cells=True, prep_for_baysor=True, distance_threshold=0.5176, lowpass_sigma=(3, 1, 1), magnitude_threshold=(0.9, 10.0), minimum_pixels=2.0, ufish_threshold=0.1, fdr_target=0.05)</code>","text":"<p>Optimize normalization by decoding.</p> <p>Helper function to iteratively optimize normalization by decoding.</p> <p>Parameters:</p> Name Type Description Default <code>n_random_tiles</code> <code>int</code> <p>Number of random tiles.</p> <code>10</code> <code>n_iterations</code> <code>int</code> <p>Number of iterations.</p> <code>10</code> <code>minimum_pixels</code> <code>float</code> <p>Minimum number of pixels for a barcode.</p> <code>3.0</code> <code>ufish_threshold</code> <code>float</code> <p>Ufish threshold.</p> <code>0.25</code> <code>lowpass_sigma</code> <code>Optional[Sequence[float]]</code> <p>Lowpass sigma.</p> <code>(3, 1, 1)</code> <code>magnitude_threshold</code> <code>Optional[Sequence[float]]</code> <p>L2-norm threshold</p> <code>(0.9, 10.0)</code> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def decode_all_tiles(\n    self,\n    assign_to_cells: bool = True,\n    prep_for_baysor: bool = True,\n    distance_threshold: Optional[float] = 0.5176,\n    lowpass_sigma: Optional[Sequence[float]] = (3, 1, 1),\n    magnitude_threshold: Optional[Sequence[float]] = (0.9,10.0),\n    minimum_pixels: Optional[float] = 2.0,\n    ufish_threshold: Optional[float] = 0.1,\n    fdr_target: Optional[float] = 0.05,\n):\n    \"\"\"Optimize normalization by decoding.\n\n    Helper function to iteratively optimize normalization by decoding.\n\n    Parameters\n    ----------\n    n_random_tiles : int, default 10\n        Number of random tiles. \n    n_iterations : int, default 10\n        Number of iterations. \n    minimum_pixels : float, default 3.0\n        Minimum number of pixels for a barcode. \n    ufish_threshold : float, default 0.25\n        Ufish threshold. \n    lowpass_sigma : Optional[Sequence[float]], default (3, 1, 1)\n        Lowpass sigma.\n    magnitude_threshold: Optional[Sequence[float], default (1.1,2.0)\n        L2-norm threshold\n    \"\"\"\n\n    if self._num_gpus &lt; 1:\n        raise RuntimeError(\"No GPUs allocated.\")\n    all_tiles = list(range(len(self._datastore.tile_ids)))\n    chunk_size = (len(all_tiles) + self._num_gpus - 1) // self._num_gpus\n\n    self._distance_threshold = distance_threshold\n\n    processes = []\n    for gpu in range(self._num_gpus):\n        start = gpu * chunk_size\n        end = min(start + chunk_size, len(all_tiles))\n        subset = all_tiles[start:end]\n        if not subset:\n            continue\n        p = mp.Process(\n            target=decode_tiles_worker,\n            args=(\n                self._datastore_path,\n                subset,\n                gpu,\n                self._n_merfish_bits,\n                lowpass_sigma,\n                distance_threshold,\n                magnitude_threshold,\n                minimum_pixels,\n                ufish_threshold,\n                self._smFISH\n            ),\n        )\n        p.start()\n        processes.append(p)\n\n    for p in processes:\n        p.join()\n\n    # load all barcodes and filter\n    self._load_tile_decoding = True\n    self._load_all_barcodes()\n    self._filter_all_barcodes_LR(fdr_target=fdr_target)\n    if not(self._is_3D):\n        radius_z = self._datastore.voxel_size_zyx_um[0]\n        self._remove_duplicates_within_tile(radius_z=radius_z)\n\n    if len(all_tiles) &gt; 1:\n        self._remove_duplicates_in_tile_overlap()\n    if assign_to_cells:\n        self._assign_cells()\n    self._save_barcodes()\n    if self._verbose &gt;=1 :\n        print(f\"Number of retained barcodes: {len(self._df_filtered_barcodes)}\")\n    if prep_for_baysor:\n        self._reformat_barcodes_for_baysor()\n    self._cleanup()\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder.decode_one_tile","title":"<code>decode_one_tile(tile_idx=0, gpu_id=0, display_results=False, return_results=False, lowpass_sigma=(3, 1, 1), magnitude_threshold=(0.9, 10.0), minimum_pixels=2.0, use_normalization=True, ufish_threshold=0.1)</code>","text":"<p>Decode one tile.</p> <p>Helper function to decode one tile. Can also display results in napari or return results as np.ndarray.</p> <p>Parameters:</p> Name Type Description Default <code>tile_idx</code> <code>int</code> <p>Tile index.</p> <code>0</code> <code>gpu_id</code> <code>int</code> <p>GPU ID to use for decoding.</p> <code>0</code> <code>display_results</code> <code>bool</code> <p>Display results in napari.</p> <code>False</code> <code>return_results</code> <code>bool</code> <p>Return results as np.ndarray</p> <code>False</code> <code>lowpass_sigma</code> <code>Optional[Sequence[float]]</code> <p>Lowpass sigma.</p> <code>(3, 1, 1)</code> <code>magnitude_threshold</code> <code>Optional[list[float, float]]</code> <p>L2-norm threshold</p> <code>(0.9, 10.0)</code> <code>minimum_pixels</code> <code>Optional[float]</code> <p>Minimum number of pixels for a barcode.</p> <code>3.0</code> <code>use_normalization</code> <code>Optional[bool]</code> <p>Use normalization.</p> <code>True</code> <code>ufish_threshold</code> <code>Optional[float]</code> <p>Ufish threshold.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>Optional[tuple[ndarray, ...]]</code> <p>If return_results is True, returns a tuple of np.ndarray containing the following: 1. Image data (filtered or unfiltered). 2. Scaled pixel images. 3. Magnitude image. 4. Distance image. 5. Decoded image.</p> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def decode_one_tile(\n    self,\n    tile_idx: int = 0,\n    gpu_id: int = 0, \n    display_results: bool = False,\n    return_results: bool = False,\n    lowpass_sigma: Optional[Sequence[float]] = (3, 1, 1),\n    magnitude_threshold: Optional[list[float,float]] = (0.9,10.0),\n    minimum_pixels: Optional[float] = 2.0,\n    use_normalization: Optional[bool] = True,\n    ufish_threshold: Optional[float] = 0.1,\n) -&gt; Optional[tuple[np.ndarray, ...]]:\n    \"\"\"Decode one tile.\n\n    Helper function to decode one tile. Can also display results in napari or return results as np.ndarray.\n\n    Parameters\n    ----------\n    tile_idx : int, default 0\n        Tile index.\n    gpu_id : int, default 0\n        GPU ID to use for decoding.\n    display_results : bool, default False\n        Display results in napari.\n    return_results : bool, default False\n        Return results as np.ndarray\n    lowpass_sigma : Optional[Sequence[float]], default (3, 1, 1)\n        Lowpass sigma.\n    magnitude_threshold: Optional[Sequence[float]], default (1.1, 2.0)\n        L2-norm threshold\n    minimum_pixels : Optional[float], default 3.0\n        Minimum number of pixels for a barcode. \n    use_normalization : Optional[bool], default True\n        Use normalization. \n    ufish_threshold : Optional[float], default 0.5\n        Ufish threshold.\n\n    Returns\n    -------\n    Optional[tuple[np.ndarray,...]]\n        If return_results is True, returns a tuple of np.ndarray containing the following:\n        1. Image data (filtered or unfiltered).\n        2. Scaled pixel images.\n        3. Magnitude image.\n        4. Distance image.\n        5. Decoded image.\n    \"\"\"\n\n    with cp.cuda.Device(gpu_id):\n\n        if use_normalization:\n            self._load_iterative_normalization_vectors(gpu_id=gpu_id)\n\n        self._tile_idx = tile_idx\n        self._load_bit_data(ufish_threshold=ufish_threshold)\n        if not (np.any(lowpass_sigma == 0)):\n            self._lp_filter(sigma=lowpass_sigma,gpu_id=gpu_id)\n        self._decode_pixels(\n            distance_threshold=self._distance_threshold,\n            magnitude_threshold=magnitude_threshold,\n            gpu_id=gpu_id\n        )\n        self._extract_barcodes(minimum_pixels=minimum_pixels,gpu_id=gpu_id)\n\n        if display_results:\n            if not(self._df_barcodes.empty):\n                print(f\"Number of extracted barcodes: {len(self._df_barcodes)}\")\n            else:\n                print(\"No barcodes extracted.\")\n            self._display_results()\n        if return_results:\n            if self._filter_type == \"lp\":\n                return (\n                    self._image_data_lp, \n                    self._scaled_pixel_images, \n                    self._magnitude_image, \n                    self._distance_image, \n                    self._decoded_image\n                )\n            else:\n                return (\n                    self._image_data, \n                    self._scaled_pixel_images, \n                    self._magnitude_image, \n                    self._distance_image, \n                    self._decoded_image\n                )\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder.optimize_filtering","title":"<code>optimize_filtering(assign_to_cells=False, prep_for_baysor=True, fdr_target=0.05)</code>","text":"<p>Optimize filtering.</p> <p>Helper function to opimize filtering for already decoded spots.</p> <p>Parameters:</p> Name Type Description Default <code>assign_to_cells</code> <code>bool</code> <p>Assign barcodes to cells.</p> <code>False</code> <code>prep_for_baysor</code> <code>bool</code> <p>Prepare barcodes for Baysor.</p> <code>True</code> <code>fdr_target</code> <code>Optional[float]</code> <p>False discovery rate target.</p> <code>0.05</code> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def optimize_filtering(\n    self,\n    assign_to_cells: bool = False,\n    prep_for_baysor: bool = True,\n    fdr_target: Optional[float] = 0.05,\n):\n    \"\"\"Optimize filtering.\n\n    Helper function to opimize filtering for already decoded spots.\n\n    Parameters\n    ----------\n    assign_to_cells : bool, default False\n        Assign barcodes to cells. \n    prep_for_baysor : bool, default True\n        Prepare barcodes for Baysor. \n    fdr_target : Optional[float], default 0.05\n        False discovery rate target. \n    \"\"\"\n\n    self._load_tile_decoding = True\n    self._load_all_barcodes()\n    self._load_tile_decoding = False\n    all_tiles = list(range(len(self._datastore.tile_ids)))\n    if not(self._verbose == 0):\n        self._verbose = 2\n    if len(all_tiles) or not(self._is_3D):\n        if not(self._is_3D):\n            radius_z = self._datastore.voxel_size_zyx_um[0]*2\n            self._remove_duplicates_within_tile(radius_z=radius_z)\n        else:\n            self._remove_duplicates_in_tile_overlap()\n    self._filter_all_barcodes(fdr_target=fdr_target)\n    if not(self._verbose == 0):\n        self._verbose = 1\n\n    if assign_to_cells:\n        self._assign_cells()\n    self._save_barcodes()\n    if prep_for_baysor:\n        self._reformat_barcodes_for_baysor()\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder.optimize_normalization_by_decoding","title":"<code>optimize_normalization_by_decoding(n_random_tiles=5, n_iterations=10, distance_threshold=0.52, minimum_pixels=2.0, ufish_threshold=0.1, lowpass_sigma=(3, 1, 1), magnitude_threshold=(0.9, 10.0))</code>","text":"<p>Optimize normalization by decoding.</p> <p>Helper function to iteratively optimize normalization by decoding.</p> <p>Parameters:</p> Name Type Description Default <code>n_random_tiles</code> <code>int</code> <p>Number of random tiles.</p> <code>10</code> <code>n_iterations</code> <code>int</code> <p>Number of iterations.</p> <code>10</code> <code>minimum_pixels</code> <code>float</code> <p>Minimum number of pixels for a barcode.</p> <code>3.0</code> <code>ufish_threshold</code> <code>float</code> <p>Ufish threshold.</p> <code>0.1</code> <code>lowpass_sigma</code> <code>Optional[Sequence[float]]</code> <p>Lowpass sigma.</p> <code>(3, 1, 1)</code> <code>magnitude_threshold</code> <code>Optional[Sequence[float]]</code> <p>L2-norm threshold</p> <code>(0.9, 10.0)</code> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def optimize_normalization_by_decoding(\n    self,\n    n_random_tiles: int = 5,\n    n_iterations: int = 10,\n    distance_threshold: Optional[float] = 0.52,\n    minimum_pixels: Optional[float] = 2.0,\n    ufish_threshold: Optional[float] = 0.1,\n    lowpass_sigma: Optional[Sequence[float]] = (3, 1, 1),\n    magnitude_threshold: Optional[Sequence[float]] = (0.9, 10.0)\n):\n    \"\"\"Optimize normalization by decoding.\n\n    Helper function to iteratively optimize normalization by decoding.\n\n    Parameters\n    ----------\n    n_random_tiles : int, default 10\n        Number of random tiles. \n    n_iterations : int, default 10\n        Number of iterations. \n    minimum_pixels : float, default 3.0\n        Minimum number of pixels for a barcode. \n    ufish_threshold : float, default 0.1\n        Ufish threshold. \n    lowpass_sigma : Optional[Sequence[float]], default (3, 1, 1)\n        Lowpass sigma.\n    magnitude_threshold: Optional[Sequence[float], default (0.9,10.0)\n        L2-norm threshold\n    \"\"\"\n    if self._num_gpus &lt; 1:\n        raise RuntimeError(\"No GPUs allocated.\")\n    all_tiles = list(range(len(self._datastore.tile_ids)))\n\n    # preload global normalization once\n    self._distance_threshold = distance_threshold\n    self._iterative_background_vector = None\n    self._iterative_normalization_vector = None\n    self._global_background_vector = None\n    self._optimize_normalization_weights = True\n    self._load_global_normalization_vectors(gpu_id=0)\n    temp_dir = Path(tempfile.mkdtemp())\n    self._temp_dir = temp_dir\n\n    # split the same set of random tiles each iteration\n    if len(all_tiles) &gt; n_random_tiles:\n        random_tiles = sample(all_tiles, n_random_tiles)\n    else:\n        random_tiles = all_tiles\n    chunk_size = (len(random_tiles) + self._num_gpus - 1) // self._num_gpus\n\n    if self._verbose &gt;= 1:\n        iterator = trange(n_iterations,desc=\"Iterative normalization\")\n    else:\n        iterator = range(n_iterations)\n\n    for iteration in iterator:\n\n        # launch one process per GPU\n        processes = []\n        for gpu in range(self._num_gpus):\n            start = gpu * chunk_size\n            end = min(start + chunk_size, len(random_tiles))\n            subset = random_tiles[start:end]\n            if not subset:\n                continue\n            p = mp.Process(\n                target=_optimize_norm_worker,\n                args=(\n                    self._datastore_path,\n                    subset,\n                    gpu,\n                    self._n_merfish_bits,\n                    temp_dir,\n                    iteration,\n                    lowpass_sigma,\n                    distance_threshold,\n                    magnitude_threshold,\n                    minimum_pixels,\n                    ufish_threshold,\n                    self._smFISH\n                ),\n            )\n            p.start()\n            processes.append(p)\n\n        for p in processes:\n            p.join()\n\n        with cp.cuda.Device(0):\n        # gather results and update\n            self._load_all_barcodes()\n            if not(self._is_3D):\n                radius_z = self._datastore.voxel_size_zyx_um[0]*2\n                self._remove_duplicates_within_tile(radius_z=radius_z)\n            self._load_global_normalization_vectors(gpu_id=0)\n            if not(self._verbose == 0):\n                self._verbose = 2\n            self._iterative_normalization_vectors(gpu_id=0)\n            if not(self._verbose == 0):\n                self._verbose = 1\n            del self._global_background_vector, self._global_normalization_vector\n            gc.collect()\n            cp.cuda.Stream.null.synchronize()\n            cp.get_default_memory_pool().free_all_blocks()\n            cp.get_default_pinned_memory_pool().free_all_blocks()\n\n    # cleanup temp files, etc.\n    self._cleanup()\n    self._optimize_normalization_weights = False\n    shutil.rmtree(self._temp_dir)\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder._optimize_norm_worker","title":"<code>_optimize_norm_worker(datastore_path, tile_indices, gpu_id, merfish_bits, temp_dir, iteration, lowpass_sigma, distance_threshold, magnitude_threshold, minimum_pixels, ufish_threshold, smFISH=False)</code>","text":"<p>Worker that runs one iteration of normalization\u2010by\u2010decoding on a GPU.</p> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def _optimize_norm_worker(\n    datastore_path: Path,\n    tile_indices: Sequence[int],\n    gpu_id: int,\n    merfish_bits: int,\n    temp_dir: Path,\n    iteration: int,\n    lowpass_sigma: Sequence[float],\n    distance_threshold: float,\n    magnitude_threshold: Sequence[float],\n    minimum_pixels: float,\n    ufish_threshold: float,\n    smFISH: bool = False\n):\n    \"\"\"Worker that runs one iteration of normalization\u2010by\u2010decoding on a GPU.\"\"\"\n    import cupy as cp\n    import torch\n    torch.cuda.set_device(gpu_id)\n    cp.cuda.Device(gpu_id).use()\n    cp.cuda.Stream.null.synchronize()\n\n    local_datastore = qi2labDataStore(datastore_path)\n    local_decoder = PixelDecoder(\n        datastore=local_datastore, \n        use_mask=False, \n        merfish_bits=merfish_bits, \n        num_gpus=1,\n        verbose=0,\n        smFISH=smFISH\n    )\n\n    local_decoder._distance_threshold = distance_threshold\n\n    local_decoder._load_global_normalization_vectors(gpu_id=gpu_id)\n    local_decoder._optimize_normalization_weights = True\n    local_decoder._temp_dir = temp_dir\n\n    # if iteration==0, skip use_normalization\n    use_norm = iteration &gt; 0\n    for tile_idx in tile_indices:\n        local_decoder.decode_one_tile(\n            tile_idx=tile_idx,\n            display_results=False,\n            return_results=False,\n            lowpass_sigma=lowpass_sigma,\n            magnitude_threshold=magnitude_threshold,\n            minimum_pixels=minimum_pixels,\n            ufish_threshold=ufish_threshold,\n            use_normalization=use_norm,\n            gpu_id=gpu_id,\n        )\n        local_decoder._save_barcodes()\n\n\n    local_decoder._optimize_normalization_weights = False\n\n    cp.cuda.Stream.null.synchronize()\n    cp.get_default_memory_pool().free_all_blocks()\n    cp.get_default_pinned_memory_pool().free_all_blocks()\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.decode_tiles_worker","title":"<code>decode_tiles_worker(datastore_path, tile_indices, gpu_id, merfish_bits, lowpass_sigma, distance_threshold, magnitude_threshold, minimum_pixels, ufish_threshold, smFISH=False)</code>","text":"<p>Worker that runs decode_one_tile on a subset of tiles under one GPU.</p> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def decode_tiles_worker(\n    datastore_path: Path,\n    tile_indices: Sequence[int],\n    gpu_id: int,\n    merfish_bits: int,\n    lowpass_sigma: Sequence[float],\n    distance_threshold: float,\n    magnitude_threshold: Sequence[float],\n    minimum_pixels: float,\n    ufish_threshold: float,\n    smFISH: bool = False\n):\n    \"\"\"Worker that runs decode_one_tile on a subset of tiles under one GPU.\"\"\"\n    import cupy as cp\n    import torch\n    torch.cuda.set_device(gpu_id)\n    cp.cuda.Device(gpu_id).use()\n    cp.cuda.Stream.null.synchronize()\n\n    local_datastore = qi2labDataStore(datastore_path)\n    local_decoder = PixelDecoder(\n        datastore=local_datastore, \n        use_mask=False, \n        merfish_bits=merfish_bits, \n        num_gpus=1,\n        verbose=0,\n        smFISH=smFISH\n    )\n\n    local_decoder._distance_threshold = distance_threshold\n\n    local_decoder._load_global_normalization_vectors(gpu_id=gpu_id)\n    local_decoder._load_iterative_normalization_vectors(gpu_id=gpu_id)\n    local_decoder._optimize_normalization_weights = False\n\n    for tile_tracker, tile_idx in enumerate(tile_indices):\n        local_decoder.decode_one_tile(\n            tile_idx=tile_idx,\n            display_results=False,\n            return_results=False,\n            lowpass_sigma=lowpass_sigma,\n            magnitude_threshold=magnitude_threshold,\n            minimum_pixels=minimum_pixels,\n            ufish_threshold=ufish_threshold,\n            use_normalization=True,\n            gpu_id=gpu_id,\n        )\n\n        local_decoder._save_barcodes()\n        local_decoder._cleanup()\n        print(time_stamp(), f\"GPU {gpu_id}: decoded and saved tile {tile_tracker+1} of out {len(tile_indices)} (tile index: {tile_idx}).\")\n\n    cp.cuda.Stream.null.synchronize()\n    cp.get_default_memory_pool().free_all_blocks()\n    cp.get_default_pinned_memory_pool().free_all_blocks()\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/","title":"DataStore Class","text":"<p>Interface to qi2lab MERFISH datastore.</p> <p>This module provides methods and attributes to create or interact with the qi2lab MERFISH datastore. The filestore structure is further described in the merfish3d-analysis documentation.</p> History: <ul> <li>2024/12: Refactored repo structure.</li> <li>2024/12: Updated docstrings and exception types.</li> <li>2024/07: Initial commit.</li> </ul> <p>Classes:</p> Name Description <code>qi2labDataStore</code> <p>API to qi2lab MERFISH store.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore","title":"<code>qi2labDataStore</code>","text":"<p>API to qi2lab MERFISH store.</p> <p>Parameters:</p> Name Type Description Default <code>datastore_path</code> <code>Union[str, Path]</code> <p>Path to qi2lab MERFISH store</p> required <p>Methods:</p> Name Description <code>initialize_tile</code> <p>Initialize directory structure for a tile.</p> <code>load_codebook_parsed</code> <p>Load and split codebook into gene_ids and codebook matrix.</p> <code>load_coord_of_xform_px</code> <p>Local fidicual optical flow matrix for one round and tile.</p> <code>load_global_baysor_filtered_spots</code> <p>Load Baysor re-assigned decoded RNA.</p> <code>load_global_baysor_outlines</code> <p>Load Baysor cell outlines.</p> <code>load_global_cellpose_outlines</code> <p>Load Cellpose max projection cell outlines.</p> <code>load_global_cellpose_segmentation_image</code> <p>Load Cellpose max projection, downsampled segmentation image.</p> <code>load_global_coord_xforms_um</code> <p>Load global registration transform for one tile.</p> <code>load_global_fidicual_image</code> <p>Load downsampled, fused fidicual image.</p> <code>load_global_filtered_decoded_spots</code> <p>Load all decoded and filtered spots.</p> <code>load_local_bit_linker</code> <p>Load readout bits linked to fidicual round for one tile.</p> <code>load_local_corrected_image</code> <p>Load gain and offset corrected image for fiducial OR readout bit for one tile.</p> <code>load_local_decoded_spots</code> <p>Load decoded spots and features for one tile.</p> <code>load_local_registered_image</code> <p>Local registered, deconvolved image for fidiculial OR readout bit for one tile.</p> <code>load_local_rigid_xform_xyz_px</code> <p>Load calculated rigid registration transform for one round and tile.</p> <code>load_local_round_linker</code> <p>Load fidicual round linked to readout bit for one tile.</p> <code>load_local_stage_position_zyx_um</code> <p>Load tile stage position for one tile.</p> <code>load_local_ufish_image</code> <p>Load readout bit U-FISH prediction image for one tile.</p> <code>load_local_ufish_spots</code> <p>Load U-FISH spot localizations and features for one tile.</p> <code>load_local_wavelengths_um</code> <p>Load wavelengths for fidicual OR readout bit for one tile.</p> <code>reformat_baysor_3D_oultines</code> <p>Reformat baysor 3D json file into ImageJ ROIs.</p> <code>reprocess_and_save_filtered_spots_with_baysor_outlines</code> <p>Reprocess filtered spots using baysor cell outlines, then save.</p> <code>run_baysor</code> <p>Run Baysor\"</p> <code>save_coord_of_xform_px</code> <p>Save fidicual optical flow matrix for one round and tile.</p> <code>save_global_cellpose_segmentation_image</code> <p>Save Cellpose max projection, downsampled segmentation image.</p> <code>save_global_coord_xforms_um</code> <p>Save global registration transform for one tile.</p> <code>save_global_fidicual_image</code> <p>Save downsampled, fused fidicual image.</p> <code>save_global_filtered_decoded_spots</code> <p>Save all decoded and filtered spots.</p> <code>save_local_bit_linker</code> <p>Save readout bits linked to fidicual round for one tile.</p> <code>save_local_corrected_image</code> <p>Save gain and offset corrected image.</p> <code>save_local_decoded_spots</code> <p>Save decoded spots and features for one tile.</p> <code>save_local_registered_image</code> <p>Save registered, deconvolved image.</p> <code>save_local_rigid_xform_xyz_px</code> <p>Save calculated rigid registration transform for one round and tile.</p> <code>save_local_round_linker</code> <p>Save fidicual round linker attribute to readout bit for one tile.</p> <code>save_local_stage_position_zyx_um</code> <p>Save tile stage position for one tile.</p> <code>save_local_ufish_image</code> <p>Save U-FISH prediction image.</p> <code>save_local_ufish_spots</code> <p>Save U-FISH localizations and features.</p> <code>save_local_wavelengths_um</code> <p>Save wavelengths for fidicual OR readout bit for one tile.</p> <code>save_mtx</code> <p>Save mtx file for downstream analysis. Assumes Baysor has been run.</p> <code>save_spots_prepped_for_baysor</code> <p>Save spots prepped for Baysor.</p> <p>Attributes:</p> Name Type Description <code>baysor_options</code> <code>Union[Path, str]</code> <p>Baysor options</p> <code>baysor_path</code> <code>Union[Path, str]</code> <p>Baysor path</p> <code>binning</code> <code>Optional[int]</code> <p>Camera binning.</p> <code>bit_ids</code> <code>Optional[Collection[str]]</code> <p>Bit IDs.</p> <code>camera_model</code> <code>Optional[str]</code> <p>Camera model.</p> <code>channel_psfs</code> <code>Optional[ArrayLike]</code> <p>Channel point spread functions (PSF).</p> <code>channel_shading_maps</code> <code>Optional[ArrayLike]</code> <p>Channel shaiding images.</p> <code>channels_in_data</code> <code>Optional[Collection[int]]</code> <p>Channel indices.</p> <code>codebook</code> <code>Optional[DataFrame]</code> <p>Codebook.</p> <code>datastore_state</code> <code>Optional[dict]</code> <p>Datastore state.</p> <code>e_per_ADU</code> <code>Optional[float]</code> <p>Electrons per camera ADU.</p> <code>experiment_order</code> <code>Optional[DataFrame]</code> <p>Round and bit order.</p> <code>global_background_vector</code> <code>Optional[ArrayLike]</code> <p>Global background vector.</p> <code>global_normalization_vector</code> <code>Optional[ArrayLike]</code> <p>Global normalization vector.</p> <code>iterative_background_vector</code> <code>Optional[ArrayLike]</code> <p>Iterative background vector.</p> <code>iterative_normalization_vector</code> <code>Optional[ArrayLike]</code> <p>Iterative normalization vector.</p> <code>julia_threads</code> <code>int</code> <p>Julia thread number</p> <code>microscope_type</code> <code>Optional[str]</code> <p>Microscope type.</p> <code>na</code> <code>Optional[float]</code> <p>Detection objective numerical aperture (NA).</p> <code>noise_map</code> <code>Optional[ArrayLike]</code> <p>Camera noise image.</p> <code>num_bits</code> <code>int</code> <p>Number of bits.</p> <code>num_rounds</code> <code>Optional[int]</code> <p>Number of rounds.</p> <code>num_tiles</code> <code>Optional[int]</code> <p>Number of tiles.</p> <code>ri</code> <code>Optional[float]</code> <p>Detection objective refractive index (RI).</p> <code>round_ids</code> <code>Optional[Collection[str]]</code> <p>Round IDs.</p> <code>tile_ids</code> <code>Optional[Collection[str]]</code> <p>Tile IDs.</p> <code>tile_overlap</code> <code>Optional[float]</code> <p>XY tile overlap.</p> <code>voxel_size_zyx_um</code> <code>Optional[ArrayLike]</code> <p>Voxel size, zyx order (microns).</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>class qi2labDataStore:\n    \"\"\"API to qi2lab MERFISH store.\n\n    Parameters\n    ----------\n    datastore_path : Union[str, Path]\n        Path to qi2lab MERFISH store\n\n    \"\"\"\n\n    def __init__(self, datastore_path: Union[str, Path]):\n        compressor = {\n            \"id\": \"blosc\",\n            \"cname\": \"zstd\",\n            \"clevel\": 5,\n            \"shuffle\": 2,\n        }\n        self._zarrv2_spec = {\n            \"driver\": \"zarr\",\n            \"kvstore\": None,\n            \"metadata\": {\"compressor\": compressor},\n            \"open\": True,\n            \"assume_metadata\": False,\n            \"create\": True,\n            \"delete_existing\": False,\n        }\n\n        self._datastore_path = Path(datastore_path)\n        if self._datastore_path.exists():\n            self._parse_datastore()\n        else:\n            self._init_datastore()\n\n    @property\n    def datastore_state(self) -&gt; Optional[dict]:\n        \"\"\"Datastore state.\n\n        Returns\n        -------\n        datastore_state : Optional[dict]\n            Datastore state.\n        \"\"\"\n\n        return getattr(self, \"_datastore_state\", None)\n\n    @datastore_state.setter\n    def datastore_state(self, value: dict):\n        \"\"\"Set the datastore state.\n\n        Parameters\n        ----------\n        value : dict\n            New datastore state.\n        \"\"\"\n\n        if not hasattr(self, \"_datastore_state\") or self._datastore_state is None:\n            self._datastore_state = value\n        else:\n            self._datastore_state.update(value)\n        self._save_to_json(self._datastore_state, self._datastore_state_json_path)\n\n    @property\n    def microscope_type(self) -&gt; Optional[str]:\n        \"\"\"Microscope type.\n\n        Returns\n        -------\n        microscope_type : Optional[str]\n            Microscope type.\n        \"\"\"\n\n        return getattr(self, \"_microscope_type\", None)\n\n    @microscope_type.setter\n    def microscope_type(self, value: str):\n        \"\"\"Set the microscope type.\n\n        Parameters\n        ----------\n        value : str\n            New microscope type.\n        \"\"\"\n\n        self._microscope_type = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"microscope_type\"] = value\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def camera_model(self) -&gt; Optional[str]:\n        \"\"\"Camera model.\n\n        Returns\n        -------\n        camera_model : Optional[str]\n            Camera model.\n        \"\"\"\n\n        return getattr(self, \"_camera_model\", None)\n\n    @camera_model.setter\n    def camera_model(self, value: str):\n        \"\"\"Set the camera model.\n\n        Parameters\n        ----------\n        value : str\n            New camera model.\n        \"\"\"\n        self._camera_model = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"camera_model\"] = value\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def num_rounds(self) -&gt; Optional[int]:\n        \"\"\"Number of rounds.\n\n        Returns\n        -------\n        num_rounds : int\n            Number of rounds.\n        \"\"\"\n\n        return getattr(self, \"_num_rounds\", None)\n\n    @num_rounds.setter\n    def num_rounds(self, value: int):\n        \"\"\"Set the number of rounds.\n\n        Parameters\n        ----------\n        value : int\n            New number of rounds.\n        \"\"\"\n\n        self._num_rounds = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"num_rounds\"] = value\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def num_bits(self) -&gt; int:\n        \"\"\"Number of bits.\n\n        Returns\n        -------\n        num_bits : int\n            Number of bits.\n        \"\"\"\n        return getattr(self, \"_num_bits\", None)\n\n    @num_bits.setter\n    def num_bits(self, value: int):\n        \"\"\"Set the number of bits.\n\n        Parameters\n        -------\n        num_bits : int\n            Number of bits.\n        \"\"\"\n        self._num_bits = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"num_bits\"] = value\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def num_tiles(self) -&gt; Optional[int]:\n        \"\"\"Number of tiles.\n\n        Returns\n        -------\n        num_tiles : int\n            Number of tiles.\n        \"\"\"\n\n        return getattr(self, \"_num_tiles\", None)\n\n    @num_tiles.setter\n    def num_tiles(self, value: int):\n        \"\"\"Set the number of tiles.\n\n        Parameters\n        ----------\n        value : int\n            New number of tiles.\n        \"\"\"\n\n        self._num_tiles = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"num_tiles\"] = value\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n        self._tile_ids = []\n        for tile_idx in range(value):\n            self._tile_ids.append(\"tile\" + str(tile_idx).zfill(4))\n\n    @property\n    def channels_in_data(self) -&gt; Optional[Collection[int]]:\n        \"\"\"Channel indices.\n\n        Returns\n        -------\n        channels_in_data : Collection[int]\n            Channel indices.\n        \"\"\"\n\n        return getattr(self, \"_channels_in_data\", None)\n\n    @channels_in_data.setter\n    def channels_in_data(self, value: Collection[int]):\n        \"\"\"Set the channels in the data.\n\n        Parameters\n        ----------\n        value : Collection[int]\n            New channels in data (int values starting from zero).\n        \"\"\"\n\n        self._channels_in_data = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"channels_in_data\"] = value\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def tile_overlap(self) -&gt; Optional[float]:\n        \"\"\"XY tile overlap.\n\n        Returns\n        -------\n        tile_overlap : float\n            XY tile overlap.\n        \"\"\"\n\n        return getattr(self, \"_tile_overlap\", None)\n\n    @tile_overlap.setter\n    def tile_overlap(self, value: float):\n        \"\"\"Set the tile overlap.\n\n        Parameters\n        ----------\n        value : float\n            New tile overlap.\n        \"\"\"\n\n        self._tile_overlap = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"tile_overlap\"] = value\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def binning(self) -&gt; Optional[int]:\n        \"\"\"Camera binning.\n\n        Returns\n        -------\n        binning : int\n            Camera binning.\n        \"\"\"\n\n        return getattr(self, \"_binning\", None)\n\n    @binning.setter\n    def binning(self, value: int):\n        \"\"\"Set the camera binning.\n\n        Parameters\n        ----------\n        value : int\n            New camera binning.\n        \"\"\"\n\n        self._binning = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"binning\"] = value\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def e_per_ADU(self) -&gt; Optional[float]:\n        \"\"\"Electrons per camera ADU.\n\n        Returns\n        -------\n        e_per_ADU : float\n            Electrons per camera ADU.\"\"\"\n\n        return getattr(self, \"_e_per_ADU\", None)\n\n    @e_per_ADU.setter\n    def e_per_ADU(self, value: float):\n        \"\"\"Set the camera conversion (e- per ADU).\n\n        Parameters\n        ----------\n        value : float\n            New camera conversion (e- per ADU).\n        \"\"\"\n\n        self._e_per_ADU = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"e_per_ADU\"] = value\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def na(self) -&gt; Optional[float]:\n        \"\"\"Detection objective numerical aperture (NA).\n\n        Returns\n        -------\n        na : float\n            Detection objective numerical aperture (NA).\n        \"\"\"\n\n        return getattr(self, \"_na\", None)\n\n    @na.setter\n    def na(self, value: float):\n        \"\"\"Set detection objective numerical aperture (NA).\n\n        Parameters\n        ----------\n        value: float\n            New detection objective numerical aperture (NA)\n        \"\"\"\n\n        self._na = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"na\"] = value\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def ri(self) -&gt; Optional[float]:\n        \"\"\"Detection objective refractive index (RI).\n\n        Returns\n        -------\n        ri : float\n            Detection objective refractive index (RI).\n        \"\"\"\n\n        return getattr(self, \"_ri\", None)\n\n    @ri.setter\n    def ri(self, value: float):\n        \"\"\"Set detection objective refractive index (RI).\n\n        Parameters\n        ----------\n        value: float\n            New detection objective refractive index (RI)\n        \"\"\"\n\n        self._ri = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"ri\"] = value\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def noise_map(self) -&gt; Optional[ArrayLike]:\n        \"\"\"Camera noise image.\n\n        Returns\n        -------\n        noise_map : ArrayLike\n            Camera noise image.\n        \"\"\"\n\n        return getattr(self, \"_noise_map\", None)\n\n    @noise_map.setter\n    def noise_map(self, value: ArrayLike):\n        \"\"\"Set the camera noise image.\n\n        Parameters\n        ----------\n        value : ArrayLike\n            New camera noise image.\n        \"\"\"\n\n        self._noise_map = value\n        current_local_zarr_path = str(self._calibrations_zarr_path / Path(\"noise_map\"))\n\n        try:\n            self._save_to_zarr_array(\n                value,\n                self._get_kvstore_key(current_local_zarr_path),\n                self._zarrv2_spec,\n                return_future=False,\n            )\n        except (IOError, OSError, ZarrError):\n            print(r\"Could not access calibrations.zarr/noise_map\")\n\n    @property\n    def channel_shading_maps(self) -&gt; Optional[ArrayLike]:\n        \"\"\"Channel shaiding images.\n\n        Returns\n        -------\n        channel_shading_maps : ArrayLike\n            Channel shading images.\n        \"\"\"\n\n        return getattr(self, \"_shading_maps\", None)\n\n    @channel_shading_maps.setter\n    def channel_shading_maps(self, value: ArrayLike):\n        \"\"\"Set the channel shading images.\n\n        Parameters\n        ----------\n        value : ArrayLike\n            New channel shading images.\n        \"\"\"\n\n        self._shading_maps = value\n        current_local_zarr_path = str(\n            self._calibrations_zarr_path / Path(\"shading_maps\")\n        )\n\n        try:\n            self._save_to_zarr_array(\n                value,\n                self._get_kvstore_key(current_local_zarr_path),\n                self._zarrv2_spec,\n                return_future=False,\n            )\n        except (IOError, OSError, ZarrError):\n            print(r\"Could not access calibrations.zarr/shading_maps\")\n\n    @property\n    def channel_psfs(self) -&gt; Optional[ArrayLike]:\n        \"\"\"Channel point spread functions (PSF).\n\n        Return\n        ------\n        channel_psfs : ArrayLike\n            Channel point spread functions (PSF).\n        \"\"\"\n\n        return getattr(self, \"_psfs\", None)\n\n    @channel_psfs.setter\n    def channel_psfs(self, value: ArrayLike):\n        \"\"\"Set the channel point spread functions (PSF).\n\n        Parameters\n        ----------\n        value : ArrayLike\n            New channel point spread functions (PSF).\n        \"\"\"\n\n        self._psfs = value\n        current_local_zarr_path = str(self._calibrations_zarr_path / Path(\"psf_data\"))\n\n        try:\n            self._save_to_zarr_array(\n                value,\n                self._get_kvstore_key(current_local_zarr_path),\n                self._zarrv2_spec.copy(),\n                return_future=False,\n            )\n        except (IOError, ValueError):\n            print(r\"Could not access calibrations.zarr/psf_data\")\n\n    @property\n    def experiment_order(self) -&gt; Optional[pd.DataFrame]:\n        \"\"\"Round and bit order.\n\n        Returns\n        -------\n        experiment_order : pd.DataFrame\n            Round and bit order.\n        \"\"\"\n\n        return getattr(self, \"_experiment_order\", None)\n\n    @experiment_order.setter\n    def experiment_order(self, value: Union[ArrayLike, pd.DataFrame]):\n        \"\"\"Set the round and bit order.\n\n        Parameters\n        ----------\n        value : Union[ArrayLike, pd.DataFrame]\n            New round and bit order.\n        \"\"\"\n\n        if isinstance(value, pd.DataFrame):\n            self._experiment_order = value\n        else:\n            channel_list = []\n            for idx in range(len(self._channels_in_data)):\n                channel_list.append(str(self._channels_in_data[idx]))\n            self._experiment_order = pd.DataFrame(\n                value, columns=channel_list, dtype=\"int64\"\n            )\n\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"exp_order\"] = self._experiment_order.values.tolist()\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n        if self.num_rounds is None: \n            self.num_rounds = int(value[-1, 0])\n        else:\n            assert self.num_rounds == int(value[-1, 0]), \"Number of rounds does not match experiment order file.\"\n\n        if self.num_bits is None:\n            self.num_bits = int(np.max(value[:, 1:]))\n        else:\n            assert self.num_bits == int(np.max(value[:, 1:])), \"Number of bits does not match experiment order file.\"\n\n        self._round_ids = []\n        for round_idx in range(self.num_rounds):\n            self._round_ids.append(\"round\" + str(round_idx + 1).zfill(3))\n\n        self._bit_ids = []\n        for bit_idx in range(self.num_bits):\n            self._bit_ids.append(\"bit\" + str(bit_idx + 1).zfill(3))\n\n    @property\n    def codebook(self) -&gt; Optional[pd.DataFrame]:\n        \"\"\"Codebook.\n\n        Returns\n        -------\n        codebook : pd.DataFrame\n            Codebook.\n        \"\"\"\n\n        data = getattr(self, \"_codebook\", None)\n\n        if data is None:\n            return None\n        num_columns = len(data[0]) if data else 0\n        columns = [\"gene_id\"] + [f\"bit{i:02d}\" for i in range(1, num_columns)]\n\n        return pd.DataFrame(data, columns=columns)\n\n    @codebook.setter\n    def codebook(self, value: pd.DataFrame):\n        \"\"\"Set the codebook.\n\n        Parameters\n        ----------\n        value : pd.DataFrame\n            New codebook.\n        \"\"\"\n\n        self._codebook = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"codebook\"] = self._codebook.values.tolist()\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def voxel_size_zyx_um(self) -&gt; Optional[ArrayLike]:\n        \"\"\"Voxel size, zyx order (microns).\n\n        Returns\n        -------\n        voxel_size_zyx_um : ArrayLike\n            Voxel size, zyx order (microns).\n        \"\"\"\n\n        return getattr(self, \"_voxel_size_zyx_um\", None)\n\n    @voxel_size_zyx_um.setter\n    def voxel_size_zyx_um(self, value: ArrayLike):\n        \"\"\"Set the voxel size, zyx order (microns).\n\n        Parameters\n        ----------\n        value : ArrayLike\n            New voxel size, zyx order (microns).\n        \"\"\"\n\n        self._voxel_size_zyx_um = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"voxel_size_zyx_um\"] = value\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def baysor_path(self) -&gt; Union[Path,str]:\n        \"\"\"Baysor path\n\n        Returns\n        -------\n        baysor_path : Union[Path,str]\n            Baysor path.\n        \"\"\"\n\n        return getattr(self,\"_baysor_path\",None)\n\n    @baysor_path.setter\n    def baysor_path(self, value: Union[Path,str]):\n        \"\"\"Set the baysor path.\n\n        Parameters\n        ----------\n        value : Union[Path,str]\n            New baysor path.\n        \"\"\"\n\n        if value is None:\n            self._baysor_path = None\n            self._datastore_state[\"BaysorPath\"] = None\n        else:\n            self._baysor_path = Path(value)\n            self._datastore_state[\"BaysorPath\"] = str(self._baysor_path)\n        self._save_to_json(self._datastore_state, self._datastore_state_json_path)\n\n    @property\n    def baysor_options(self) -&gt; Union[Path,str]:\n        \"\"\"Baysor options\n\n        Returns\n        -------\n        baysor_options : Union[Path,str]\n            Baysor options.\n        \"\"\"\n        return getattr(self,\"_baysor_options\",None)\n\n    @baysor_options.setter\n    def baysor_options(self, value: Union[Path,str]):\n        \"\"\"Set the baysor options.\n\n        Parameters\n        ----------\n        value : Union[Path,str]\n            New baysor options.\n        \"\"\"\n\n        if value is None:\n            self._baysor_path = None\n            self._datastore_state[\"BaysorPath\"] = None\n        else:\n            self._baysor_options = Path(value)\n            self._datastore_state[\"BaysorOptions\"] = str(self._baysor_options)\n        self._save_to_json(self._datastore_state, self._datastore_state_json_path)\n\n    @property\n    def julia_threads(self) -&gt; int:\n        \"\"\"Julia thread number\n\n        Returns\n        -------\n        julia_threads : int\n            Julia thread number.\n        \"\"\"\n\n        return getattr(self,\"_julia_threads\",None)\n\n    @julia_threads.setter\n    def julia_threads(self, value: int):\n        \"\"\"Set the julia thread number.\n\n        Parameters\n        ----------\n        value : int\n            New julia thread number.\n        \"\"\"\n\n        self._julia_threads = value\n        self._datastore_state[\"JuliaThreads\"] = str(self._julia_threads)\n        self._save_to_json(self._datastore_state, self._datastore_state_json_path)\n\n    @property\n    def global_normalization_vector(self) -&gt; Optional[ArrayLike]:\n        \"\"\"Global normalization vector.\n\n        Returns\n        -------\n        global_normalization_vector : ArrayLike\n            Global normalization vector.\n        \"\"\"\n\n        value = getattr(self, \"_global_normalization_vector\", None)\n        if value is None:\n            zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n            calib_zattrs = self._load_from_json(zattrs_path)\n\n            try:\n                value = np.asarray(\n                    calib_zattrs[\"global_normalization_vector\"], dtype=np.float32\n                )\n                return value\n            except KeyError:\n                print(\"Global normalization vector not calculated.\")\n                return None\n        else:\n            return value\n\n    @global_normalization_vector.setter\n    def global_normalization_vector(self, value: ArrayLike):\n        \"\"\"Set the global normalization vector.\n\n        Parameters\n        ----------\n        value : ArrayLike\n            New global normalization vector.\n        \"\"\"\n\n        self._global_normalization_vector = np.asarray(value, dtype=np.float32)\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"global_normalization_vector\"] = (\n            self._global_normalization_vector.tolist()\n        )\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def global_background_vector(self) -&gt; Optional[ArrayLike]:\n        \"\"\"Global background vector.\n\n        Returns\n        -------\n        global_background_vector : ArrayLike\n            Global background vector.\n        \"\"\"\n\n        value = getattr(self, \"_global_background_vector\", None)\n        if value is None:\n            zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n            calib_zattrs = self._load_from_json(zattrs_path)\n            try:\n                value = np.asarray(\n                    calib_zattrs[\"global_background_vector\"], dtype=np.float32\n                )\n                return value\n            except KeyError:\n                print(\"Global background vector not calculated.\")\n                return None\n        else:\n            return value\n\n    @global_background_vector.setter\n    def global_background_vector(self, value: ArrayLike):\n        \"\"\"Set the global background vector.\n\n        Parameters\n        ----------\n        value : ArrayLike\n            New global background vector.\n        \"\"\"\n\n        self._global_background_vector = np.asarray(value, dtype=np.float32)\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"global_background_vector\"] = (\n            self._global_background_vector.tolist()\n        )\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def iterative_normalization_vector(self) -&gt; Optional[ArrayLike]:\n        \"\"\"Iterative normalization vector.\n\n        Returns\n        -------\n        iterative_normalization_vector : ArrayLike\n            Iterative normalization vector.\n        \"\"\"\n\n        value = getattr(self, \"_iterative_normalization_vector\", None)\n        if value is None:\n            zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n            calib_zattrs = self._load_from_json(zattrs_path)\n            try:\n                value = np.asarray(\n                    calib_zattrs[\"iterative_normalization_vector\"], dtype=np.float32\n                )\n            except KeyError:\n                value = None\n\n            if value is None:\n                print(\"Iterative normalization vector not calculated.\")\n                return None\n\n            return value\n        else:\n            return value\n\n    @iterative_normalization_vector.setter\n    def iterative_normalization_vector(self, value: ArrayLike):\n        \"\"\"Set the iterative normalization vector.\n\n        Parameters\n        ----------\n        value : ArrayLike\n            New iterative normalization vector.\n        \"\"\"\n\n        self._iterative_normalization_vector = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"iterative_normalization_vector\"] = (\n            self._iterative_normalization_vector.tolist()\n        )\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def iterative_background_vector(self) -&gt; Optional[ArrayLike]:\n        \"\"\"Iterative background vector.\n\n        Returns\n        -------\n        iterative_background_vector : ArrayLike\n            Iterative background vector.\n        \"\"\"\n\n        value = getattr(self, \"_iterative_background_vector\", None)\n        if value is None:\n            zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n            calib_zattrs = self._load_from_json(zattrs_path)\n            try:\n                value = np.asarray(\n                    calib_zattrs[\"iterative_background_vector\"], dtype=np.float32\n                )\n            except KeyError:\n                value = None\n            if value is None:\n                print(\"Iterative background vector not calculated.\")\n                return None\n\n            return value\n        else:\n            return value\n\n    @iterative_background_vector.setter\n    def iterative_background_vector(self, value: ArrayLike):\n        \"\"\"Set the iterative background vector.\n\n        Parameters\n        ----------\n        value : ArrayLike\n            New iterative background vector.\n        \"\"\"\n\n        self._iterative_background_vector = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"iterative_background_vector\"] = (\n            self._iterative_background_vector.tolist()\n        )\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def tile_ids(self) -&gt; Optional[Collection[str]]:\n        \"\"\"Tile IDs.\n\n        Returns\n        -------\n        tile_ids : Collection[str]\n            Tile IDs.\n        \"\"\"\n\n        return getattr(self, \"_tile_ids\", None)\n\n    @property\n    def round_ids(self) -&gt; Optional[Collection[str]]:\n        \"\"\"Round IDs.\n\n        Returns\n        -------\n        round_ids : Collection[str]\n            Round IDs.\n        \"\"\"\n\n        return getattr(self, \"_round_ids\", None)\n\n    @property\n    def bit_ids(self) -&gt; Optional[Collection[str]]:\n        \"\"\"Bit IDs.\n\n        Returns\n        -------\n        bit_ids : Collection[str]\n            Bit IDs.\n        \"\"\"\n\n        return getattr(self, \"_bit_ids\", None)\n\n    def _init_datastore(self):\n        \"\"\"Initialize datastore.\n\n        Create directory structure and initialize datastore state.\n        \"\"\"\n\n        self._datastore_path.mkdir(parents=True)\n        self._calibrations_zarr_path = self._datastore_path / Path(r\"calibrations.zarr\")\n        self._calibrations_zarr_path.mkdir()\n        calibrations_zattrs_path = self._calibrations_zarr_path / Path(r\".zattrs\")\n        empty_zattrs = {}\n        self._save_to_json(empty_zattrs, calibrations_zattrs_path)\n        self._polyDT_root_path = self._datastore_path / Path(r\"polyDT\")\n        self._polyDT_root_path.mkdir()\n        self._readouts_root_path = self._datastore_path / Path(r\"readouts\")\n        self._readouts_root_path.mkdir()\n        self._ufish_localizations_root_path = self._datastore_path / Path(\n            r\"ufish_localizations\"\n        )\n        self._ufish_localizations_root_path.mkdir()\n        self._decoded_root_path = self._datastore_path / Path(r\"decoded\")\n        self._decoded_root_path.mkdir()\n        self._fused_root_path = self._datastore_path / Path(r\"fused\")\n        self._fused_root_path.mkdir()\n        self._segmentation_root_path = self._datastore_path / Path(r\"segmentation\")\n        self._segmentation_root_path.mkdir()\n        self._mtx_output_root_path = self._datastore_path / Path(r\"mtx_output\")\n        self._mtx_output_root_path.mkdir()\n        self._baysor_path = r\"\"\n        self._baysor_options = r\"\"\n        self._julia_threads = 0\n\n        # initialize datastore state\n        self._datastore_state_json_path = self._datastore_path / Path(\n            r\"datastore_state.json\"\n        )\n        self._datastore_state = {\n            \"Version\": 0.3,\n            \"Initialized\": True,\n            \"Calibrations\": False,\n            \"Corrected\": False,\n            \"LocalRegistered\": False,\n            \"GlobalRegistered\": False,\n            \"Fused\": False,\n            \"SegmentedCells\": False,\n            \"DecodedSpots\": False,\n            \"FilteredSpots\": False,\n            \"RefinedSpots\": False,\n            \"mtxOutput\": False,\n            \"BaysorPath\": str(self._baysor_path),\n            \"BaysorOptions\": str(self._baysor_options),\n            \"JuliaThreads\": str(self._julia_threads)\n        }\n\n        self._save_to_json(self._datastore_state, self._datastore_state_json_path)\n\n    @staticmethod\n    def _get_kvstore_key(path: Union[Path, str]) -&gt; dict:\n        \"\"\"Convert datastore location to tensorstore kvstore key.\n\n        Parameters\n        ----------\n        path : Union[Path, str]\n            Datastore location.\n\n        Returns\n        -------\n        kvstore_key : dict\n            Tensorstore kvstore key.\n        \"\"\"\n\n        path_str = str(path)\n        if path_str.startswith(\"s3://\") or \"s3.amazonaws.com\" in path_str:\n            return {\"driver\": \"s3\", \"path\": path_str}\n        elif path_str.startswith(\"gs://\") or \"storage.googleapis.com\" in path_str:\n            return {\"driver\": \"gcs\", \"path\": path_str}\n        elif path_str.startswith(\"azure://\") or \"blob.core.windows.net\" in path_str:\n            return {\"driver\": \"azure\", \"path\": path_str}\n        elif path_str.startswith(\"http://\") or path_str.startswith(\"https://\"):\n            raise ValueError(\"Unsupported cloud storage provider in URL\")\n        else:\n            return {\"driver\": \"file\", \"path\": path_str}\n\n    @staticmethod\n    def _load_from_json(dictionary_path: Union[Path, str]) -&gt; dict:\n        \"\"\"Load json as dictionary.\n\n        Parameters\n        ----------\n        dictionary_path : Union[Path, str]\n            Path to json file.\n\n        Returns\n        -------\n        dictionary : dict\n            Dictionary from json file.\n        \"\"\"\n\n        try:\n            with open(dictionary_path, \"r\") as f:\n                dictionary = json.load(f)\n        except (FileNotFoundError, json.JSONDecodeError):\n            dictionary = {}\n        return dictionary\n\n    @staticmethod\n    def _save_to_json(dictionary: dict, dictionary_path: Union[Path, str]):\n        \"\"\"Save dictionary to json.\n\n        Parameters\n        ----------\n        dictionary : dict\n            The data to be saved.\n        dictionary_path : Union[Path,str]\n            The path to the JSON file where the data will be saved.\n        \"\"\"\n\n        with open(dictionary_path, \"w\") as file:\n            json.dump(dictionary, file, indent=4)\n\n    @staticmethod\n    def _load_from_microjson(dictionary_path: Union[Path, str]) -&gt; dict:\n        \"\"\"Load cell outlines outlines microjson as dictionary.\n\n        Parameters\n        ----------\n        dictionary_path : Union[Path, str]\n            Path to microjson file.\n\n        Returns\n        -------\n        outlines : dict\n            Cell outlines dictionary.\n        \"\"\"\n\n        try:\n            with open(dictionary_path, \"r\") as f:\n                data = json.load(f)\n                outlines = {}\n                for feature in data[\"features\"]:\n                    cell_id = feature[\"properties\"][\"cell_id\"]\n                    coordinates = feature[\"geometry\"][\"coordinates\"][0]\n                    outlines[cell_id] = np.array(coordinates)\n        except (FileNotFoundError, json.JSONDecodeError, KeyError, TypeError, ValueError):\n            outlines = {}\n        return outlines\n\n    @staticmethod\n    def _check_for_zarr_array(kvstore: Union[Path, str], spec: dict):\n        \"\"\"Check if zarr array exists using Tensortore.\n\n        Parameters\n        ----------\n        kvstore : Union[Path, str]\n            Datastore location.\n        spec : dict\n            Zarr specification.\n        \"\"\"\n\n        current_zarr = ts.open(\n            {\n                **spec,\n                \"kvstore\": kvstore,\n            }\n        ).result()\n\n        del current_zarr\n\n    @staticmethod\n    def _load_from_zarr_array(\n        kvstore: dict, spec: dict, return_future=True\n    ) -&gt; ArrayLike:\n        \"\"\"Return tensorstore array from zarr\n\n        Defaults to returning future result.\n\n        Parameters\n        ----------\n        kvstore : dict\n            Tensorstore kvstore specification.\n        spec : dict\n            Tensorstore zarr specification.\n        return_future : bool\n            Return future (True) or immediately read (False).\n\n        Returns\n        -------\n        array : ArrayLike\n            Delayed (future) or immediate array.\n        \"\"\"\n\n        current_zarr = ts.open(\n            {\n                **spec,\n                \"kvstore\": kvstore,\n            }\n        ).result()\n\n        read_future = current_zarr.read()\n\n        if return_future:\n            return read_future\n        else:\n            return read_future.result()\n\n    @staticmethod\n    def _save_to_zarr_array(\n        array: ArrayLike,\n        kvstore: dict,\n        spec: dict,\n        return_future: Optional[bool] = False,\n    ) -&gt; Optional[ArrayLike]:\n        \"\"\"Save array to zarr using tensorstore.\n\n        Defaults to returning future result.\n\n        Parameters\n        ----------\n        array : ArrayLike\n            Array to save.\n        kvstore : dict\n            Tensorstore kvstore specification.\n        spec : dict\n            Tensorstore zarr specification.\n        return_future : Optional[bool]\n            Return future (True) or immediately write (False).\n\n        Returns\n        -------\n        write_future : Optional[ArrayLike]\n            Delayed (future) if return_future is True.\n        \"\"\"\n\n        # check datatype\n        if str(array.dtype) == \"uint8\":\n            array_dtype = \"&lt;u1\"\n        elif str(array.dtype) == \"uint16\":\n            array_dtype = \"&lt;u2\"\n        elif str(array.dtype) == \"float16\":\n            array_dtype = \"&lt;f2\"\n        elif str(array.dtype) == \"float32\":\n            array_dtype = \"&lt;f4\"\n        else:\n            print(\"Unsupported data type: \" + str(array.dtype))\n            return None\n\n        # check array dimension\n        spec[\"metadata\"][\"shape\"] = array.shape\n        if len(array.shape) == 2:\n            spec[\"metadata\"][\"chunks\"] = [array.shape[0], array.shape[1]]\n        elif len(array.shape) == 3:\n            spec[\"metadata\"][\"chunks\"] = [1, array.shape[1], array.shape[2]]\n        elif len(array.shape) == 4:\n            spec[\"metadata\"][\"chunks\"] = [1, 1, array.shape[1], array.shape[2]]\n        spec[\"metadata\"][\"dtype\"] = array_dtype\n\n        try:\n            current_zarr = ts.open(\n                {\n                    **spec,\n                    \"kvstore\": kvstore,\n                }\n            ).result()\n\n            write_future = current_zarr.write(array)\n\n            if return_future:\n                return write_future\n            else:\n                write_future.result()\n                return None\n        except (IOError, OSError, TimeoutError):\n            print(\"Error writing zarr array.\")\n\n    @staticmethod\n    def _load_from_parquet(parquet_path: Union[Path, str]) -&gt; pd.DataFrame:\n        \"\"\"Load dataframe from parquet.\n\n        Parameters\n        ----------\n        parquet_path : Union[Path, str]\n            Path to parquet file.\n\n        Returns\n        -------\n        df : pd.DataFrame\n            Dataframe from parquet file.\n        \"\"\"\n\n        return pd.read_parquet(parquet_path)\n\n    @staticmethod\n    def _save_to_parquet(df: pd.DataFrame, parquet_path: Union[Path, str]):\n        \"\"\"Save dataframe to parquet.\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n            Dataframe to save.\n        parquet_path : Union[Path, str]\n            Path to parquet file.\n        \"\"\"\n\n        # df.to_parquet(\n        #     parquet_path,\n        #     engine=\"pyarrow\",\n        #     version=\"1.0\",\n        #     write_statistics=False\n        # )\n\n        df.to_parquet(\n            parquet_path,\n            engine=\"fastparquet\", \n            index=False\n        )\n\n    def _parse_datastore(self):\n        \"\"\"Parse datastore to discover available components.\"\"\"\n\n        # directory structure as defined by qi2lab spec\n        self._calibrations_zarr_path = self._datastore_path / Path(r\"calibrations.zarr\")\n        self._polyDT_root_path = self._datastore_path / Path(r\"polyDT\")\n        self._readouts_root_path = self._datastore_path / Path(r\"readouts\")\n        self._ufish_localizations_root_path = self._datastore_path / Path(\n            r\"ufish_localizations\"\n        )\n        self._decoded_root_path = self._datastore_path / Path(r\"decoded\")\n        self._fused_root_path = self._datastore_path / Path(r\"fused\")\n        self._segmentation_root_path = self._datastore_path / Path(r\"segmentation\")\n        self._mtx_output_root_path = self._datastore_path / Path(r\"mtx_output\")\n        self._datastore_state_json_path = self._datastore_path / Path(\n            r\"datastore_state.json\"\n        )\n\n        # read in .json in root directory that indicates what steps have been run\n        with open(self._datastore_state_json_path, \"r\") as json_file:\n            self._datastore_state = json.load(json_file)\n\n        # validate calibrations.zarr\n        if self._datastore_state[\"Calibrations\"]:\n            if not (self._calibrations_zarr_path.exists()):\n                print(\"Calibration data error.\")\n            try:\n                zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n                attributes = self._load_from_json(zattrs_path)\n            except (FileNotFoundError, json.JSONDecodeError):\n                print(\"Calibration attributes not found\")\n\n            keys_to_check = [\n                \"num_rounds\",\n                \"num_tiles\",\n                \"channels_in_data\",\n                \"tile_overlap\",\n                \"binning\",\n                \"e_per_ADU\",\n                \"na\",\n                \"ri\",\n                \"exp_order\",\n                \"codebook\",\n                \"num_bits\"\n            ]\n            if self._datastore_state[\"Version\"] == 0.3:\n                keys_to_check.append(\"microscope_type\")\n                keys_to_check.append(\"camera_model\")\n                keys_to_check.append(\"voxel_size_zyx_um\")\n            for key in keys_to_check:\n                if key not in attributes.keys():\n                    raise KeyError(\"Calibration attributes incomplete\")\n                else:\n                    setattr(self, \"_\" + key, attributes[key])\n\n            current_local_zarr_path = str(\n                self._calibrations_zarr_path / Path(\"psf_data\")\n            )\n\n            try:\n                self._psfs = (\n                    self._load_from_zarr_array(\n                        kvstore=self._get_kvstore_key(current_local_zarr_path),\n                        spec=self._zarrv2_spec.copy(),\n                    )\n                ).result()\n            except (IOError, OSError, ZarrError):\n                print(\"Calibration psfs missing.\")\n\n            del current_local_zarr_path\n\n            # current_local_zarr_path = str(\n            #     self._calibrations_zarr_path / Path(\"noise_map\")\n            # )\n\n            # try:\n            #     self._noise_map = (\n            #         self._load_from_zarr_array(\n            #             kvstore=self._get_kvstore_key(current_local_zarr_path),\n            #             spec=self._zarrv2_spec,\n            #         )\n            #     ).result()\n            # except Exception:\n            #     print(\"Calibration noise map missing.\")\n\n        # validate polyDT and readout bits data\n        if self._datastore_state[\"Corrected\"]:\n            if not (self._polyDT_root_path.exists()):\n                raise FileNotFoundError(\"PolyDT directory not initialized\")\n            else:\n                polyDT_tile_ids = sorted(\n                    [\n                        entry.name\n                        for entry in self._polyDT_root_path.iterdir()\n                        if entry.is_dir()\n                    ],\n                    key=lambda x: int(x.split(\"tile\")[1].split(\".zarr\")[0]),\n                )\n                current_tile_dir_path = self._polyDT_root_path / Path(\n                    polyDT_tile_ids[0]\n                )\n                self._round_ids = sorted(\n                    [\n                        entry.name.split(\".\")[0]\n                        for entry in current_tile_dir_path.iterdir()\n                        if entry.is_dir()\n                    ],\n                    key=lambda x: int(x.split(\"round\")[1].split(\".zarr\")[0]),\n                )\n            if not (self._readouts_root_path.exists()):\n                raise FileNotFoundError(\"Readout directory not initialized\")\n            else:\n                readout_tile_ids = sorted(\n                    [\n                        entry.name\n                        for entry in self._readouts_root_path.iterdir()\n                        if entry.is_dir()\n                    ],\n                    key=lambda x: int(x.split(\"tile\")[1].split(\".zarr\")[0]),\n                )\n                current_tile_dir_path = self._readouts_root_path / Path(\n                    readout_tile_ids[0]\n                )\n                self._bit_ids = sorted(\n                    [\n                        entry.name.split(\".\")[0]\n                        for entry in current_tile_dir_path.iterdir()\n                        if entry.is_dir()\n                    ],\n                    key=lambda x: int(x.split(\"bit\")[1].split(\".zarr\")[0]),\n                )\n            assert (\n                polyDT_tile_ids == readout_tile_ids\n            ), \"polyDT and readout tile ids do not match. Conversion error.\"\n            self._tile_ids = polyDT_tile_ids.copy()\n            del polyDT_tile_ids, readout_tile_ids\n\n            for tile_id, round_id in product(self._tile_ids, self._round_ids):\n                try:\n                    zattrs_path = str(\n                        self._polyDT_root_path\n                        / Path(tile_id)\n                        / Path(round_id + \".zarr\")\n                        / Path(\".zattrs\")\n                    )\n                    attributes = self._load_from_json(zattrs_path)\n                except (FileNotFoundError, json.JSONDecodeError):\n                    print(\"polyDT tile attributes not found\")\n\n                keys_to_check = [\n                    \"stage_zyx_um\",\n                    \"excitation_um\",\n                    \"emission_um\",\n                    \"bit_linker\",\n                    # \"exposure_ms\",\n                    \"psf_idx\",\n                ]\n\n                for key in keys_to_check:\n                    if key not in attributes.keys():\n                        print(tile_id, round_id, key)\n                        raise KeyError(\"Corrected polyDT attributes incomplete\")\n\n                current_local_zarr_path = str(\n                    self._polyDT_root_path\n                    / Path(tile_id)\n                    / Path(round_id + \".zarr\")\n                    / Path(\"corrected_data\")\n                )\n\n                try:\n                    self._check_for_zarr_array(\n                        self._get_kvstore_key(current_local_zarr_path),\n                        self._zarrv2_spec.copy(),\n                    )\n                except (IOError, OSError, ZarrError):\n                    print(tile_id, round_id)\n                    print(\"Corrected polyDT data missing.\")\n\n            for tile_id, bit_id in product(self._tile_ids, self._bit_ids):\n                try:\n                    zattrs_path = str(\n                        self._readouts_root_path\n                        / Path(tile_id)\n                        / Path(bit_id + \".zarr\")\n                        / Path(\".zattrs\")\n                    )\n                    attributes = self._load_from_json(zattrs_path)\n                except (FileNotFoundError, json.JSONDecodeError):\n                    print(\"Readout tile attributes not found\")\n\n                keys_to_check = [\n                    \"excitation_um\",\n                    \"emission_um\",\n                    \"round_linker\",\n                    # \"exposure_ms\",\n                    \"psf_idx\",\n                ]\n                for key in keys_to_check:\n                    if key not in attributes.keys():\n                        raise KeyError(\"Corrected readout attributes incomplete\")\n\n                current_local_zarr_path = str(\n                    self._readouts_root_path\n                    / Path(tile_id)\n                    / Path(bit_id + \".zarr\")\n                    / Path(\"corrected_data\")\n                )\n\n                try:\n                    self._check_for_zarr_array(\n                        self._get_kvstore_key(current_local_zarr_path),\n                        self._zarrv2_spec.copy(),\n                    )\n                except (IOError, OSError, ZarrError):\n                    print(tile_id, bit_id)\n                    print(\"Corrected readout data missing.\")\n\n        # check and validate local registered data\n        if self._datastore_state[\"LocalRegistered\"]:\n            for tile_id, round_id in product(self._tile_ids, self._round_ids):\n                if round_id is not self._round_ids[0]:\n                    try:\n                        zattrs_path = str(\n                            self._polyDT_root_path\n                            / Path(tile_id)\n                            / Path(round_id + \".zarr\")\n                            / Path(\".zattrs\")\n                        )\n                        with open(zattrs_path, \"r\") as f:\n                            attributes = json.load(f)\n                    except (FileNotFoundError, json.JSONDecodeError):\n                        print(\"polyDT tile attributes not found\")\n\n                    keys_to_check = [\"rigid_xform_xyz_px\"]\n\n                    for key in keys_to_check:\n                        if key not in attributes.keys():\n                            raise KeyError(f\"{round_id,tile_id} Rigid registration missing\")\n\n                    current_local_zarr_path = str(\n                        self._polyDT_root_path\n                        / Path(tile_id)\n                        / Path(round_id + \".zarr\")\n                        / Path(\"opticalflow_xform_px\")\n                    )\n\n                    try:\n                        self._check_for_zarr_array(\n                            self._get_kvstore_key(current_local_zarr_path),\n                            self._zarrv2_spec.copy(),\n                        )\n                    except (IOError, OSError, ZarrError):\n                        #print(tile_id, round_id)\n                        #print(\"Optical flow registration data missing.\")\n                        pass\n\n                current_local_zarr_path = str(\n                    self._polyDT_root_path\n                    / Path(tile_id)\n                    / Path(round_id + \".zarr\")\n                    / Path(\"registered_decon_data\")\n                )\n                if round_id is self._round_ids[0]:\n                    try:\n                        self._check_for_zarr_array(\n                            self._get_kvstore_key(current_local_zarr_path),\n                            self._zarrv2_spec.copy(),\n                        )\n                    except (IOError, OSError, ZarrError):\n                        print(tile_id, round_id)\n                        print(\"Registered polyDT data missing.\")\n\n            for tile_id, bit_id in product(self._tile_ids, self._bit_ids):\n                current_local_zarr_path = str(\n                    self._readouts_root_path\n                    / Path(tile_id)\n                    / Path(bit_id + \".zarr\")\n                    / Path(\"registered_decon_data\")\n                )\n\n                try:\n                    self._check_for_zarr_array(\n                        self._get_kvstore_key(current_local_zarr_path),\n                        self._zarrv2_spec.copy(),\n                    )\n                except (IOError, OSError, ZarrError):\n                    print(tile_id, round_id)\n                    print(\"Registered readout data missing.\")\n\n                current_local_zarr_path = str(\n                    self._readouts_root_path\n                    / Path(tile_id)\n                    / Path(bit_id + \".zarr\")\n                    / Path(\"registered_ufish_data\")\n                )\n\n                try:\n                    self._check_for_zarr_array(\n                        self._get_kvstore_key(current_local_zarr_path),\n                        self._zarrv2_spec.copy(),\n                    )\n                except (IOError, OSError, ZarrError):\n                    print(tile_id, round_id)\n                    print(\"Registered ufish prediction missing.\")\n\n            for tile_id, bit_id in product(self._tile_ids, self._bit_ids):\n                current_ufish_path = (\n                    self._ufish_localizations_root_path\n                    / Path(tile_id)\n                    / Path(bit_id + \".parquet\")\n                )\n                if not (current_ufish_path.exists()):\n                    raise FileNotFoundError(\n                        tile_id + \" \" + bit_id + \" ufish localization missing\"\n                    )\n\n        # check and validate global registered data\n        if self._datastore_state[\"GlobalRegistered\"]:\n            for tile_id in self._tile_ids:\n                try:\n                    zattrs_path = str(\n                        self._polyDT_root_path\n                        / Path(tile_id)\n                        / Path(self._round_ids[0] + \".zarr\")\n                        / Path(\".zattrs\")\n                    )\n                    with open(zattrs_path, \"r\") as f:\n                        attributes = json.load(f)\n                except (FileNotFoundError, json.JSONDecodeError):\n                    print(\"polyDT tile attributes not found\")\n\n                keys_to_check = [\"affine_zyx_um\", \"origin_zyx_um\", \"spacing_zyx_um\"]\n\n                for key in keys_to_check:\n                    if key not in attributes.keys():\n                        raise KeyError(\"Global registration missing\")\n\n        # check and validate fused\n        if self._datastore_state[\"Fused\"]:\n            try:\n                zattrs_path = str(\n                    self._fused_root_path\n                    / Path(\"fused.zarr\")\n                    / Path(\"fused_polyDT_iso_zyx\")\n                    / Path(\".zattrs\")\n                )\n                with open(zattrs_path, \"r\") as f:\n                    attributes = json.load(f)\n            except (FileNotFoundError, json.JSONDecodeError):\n                print(\"Fused image attributes not found\")\n\n            keys_to_check = [\"affine_zyx_um\", \"origin_zyx_um\", \"spacing_zyx_um\"]\n\n            for key in keys_to_check:\n                if key not in attributes.keys():\n                    raise KeyError(\"Fused image metadata missing\")\n\n            current_local_zarr_path = str(\n                self._fused_root_path\n                / Path(\"fused.zarr\")\n                / Path(\"fused_polyDT_iso_zyx\")\n            )\n\n            try:\n                self._check_for_zarr_array(\n                    self._get_kvstore_key(current_local_zarr_path),\n                    self._zarrv2_spec.copy(),\n                )\n            except (IOError, OSError, ZarrError):\n                print(\"Fused data missing.\")\n\n        # check and validate cellpose segmentation\n        if self._datastore_state[\"SegmentedCells\"]:\n            current_local_zarr_path = str(\n                self._segmentation_root_path\n                / Path(\"cellpose\")\n                / Path(\"cellpose.zarr\")\n                / Path(\"masks_polyDT_iso_zyx\")\n            )\n\n            try:\n                self._check_for_zarr_array(\n                    self._get_kvstore_key(current_local_zarr_path),\n                    self._zarrv2_spec.copy(),\n                )\n            except (IOError, OSError, ZarrError):\n                print(\"Cellpose data missing.\")\n\n            cell_outlines_path = (\n                self._segmentation_root_path\n                / Path(\"cellpose\")\n                / Path(\"imagej_rois\")\n                / Path(\"global_coords_rois.zip\")\n            )\n            if not (cell_outlines_path.exists()):\n                raise FileNotFoundError(\"Cellpose cell outlines missing.\")\n\n        # check and validate decoded spots\n        if self._datastore_state[\"DecodedSpots\"]:\n            for tile_id in self._tile_ids:\n                decoded_path = self._decoded_root_path / Path(\n                    tile_id + \"_decoded_features.parquet\"\n                )\n\n                if not (decoded_path.exists()):\n                    raise FileNotFoundError(tile_id + \" decoded spots missing.\")\n\n        # check and validate filtered decoded spots\n        if self._datastore_state[\"FilteredSpots\"]:\n            filtered_path = self._decoded_root_path / Path(\n                \"all_tiles_filtered_decoded_features.parquet\"\n            )\n\n            if not (filtered_path.exists()):\n                raise FileNotFoundError(\"filtered decoded spots missing.\")\n\n        if self._datastore_state[\"RefinedSpots\"]:\n            baysor_spots_path = (\n                self._segmentation_root_path\n                / Path(\"baysor\")\n                / Path(\"segmentation.csv\")\n            )\n\n            if not (baysor_spots_path.exists()):\n                raise FileNotFoundError(\"Baysor filtered decoded spots missing.\")\n\n        # check and validate mtx\n        if self._datastore_state[\"mtxOutput\"]:\n            mtx_barcodes_path = self._mtx_output_root_path / Path(\"barcodes.tsv.gz\")\n            mtx_features_path = self._mtx_output_root_path / Path(\"features.tsv.gz\")\n            mtx_matrix_path = self._mtx_output_root_path / Path(\"matrix.tsv.gz\")\n\n            if (\n                not (mtx_barcodes_path.exists())\n                or not (mtx_features_path.exists())\n                or not (mtx_matrix_path.exists())\n            ):\n                raise FileNotFoundError(\"mtx output missing.\")\n\n        try:\n            self._baysor_path = Path(str(self._datastore_state[\"BaysorPath\"]))\n            self._baysor_options = Path(str(self._datastore_state[\"BaysorOptions\"]))\n            self._julia_threads = int(self._datastore_state[\"JuliaThreads\"])\n        except KeyError:\n            self._baysor_path = r\"\"\n            self._baysor_options = r\"\"\n            self._julia_threads = 1\n\n    def load_codebook_parsed(\n        self,\n    ) -&gt; Optional[tuple[Collection[str], ArrayLike]]:\n        \"\"\"Load and split codebook into gene_ids and codebook matrix.\n\n        Returns\n        -------\n        gene_ids : Collection[str]\n            Gene IDs.\n        codebook_matrix : ArrayLike\n            Codebook matrix.\n        \"\"\"\n\n        try:\n            data = getattr(self, \"_codebook\", None)\n\n            if data is None:\n                return None\n            num_columns = len(data[0]) if data else 0\n            columns = [\"gene_id\"] + [f\"bit{i:02d}\" for i in range(1, num_columns)]\n            codebook_df = pd.DataFrame(data, columns=columns)\n\n            gene_ids = codebook_df.iloc[:, 0].tolist()\n            codebook_matrix = codebook_df.iloc[:, 1:].to_numpy().astype(int)\n            del data, codebook_df\n            return gene_ids, codebook_matrix\n        except (KeyError, ValueError, TypeError):\n            print(\"Error parsing codebook.\")\n            return None\n\n    def initialize_tile(\n        self,\n        tile: Union[int, str],\n    ):\n        \"\"\"Initialize directory structure for a tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n        \"\"\"\n\n        if getattr(self, \"_experiment_order\", None) is None:\n            print(\"Assign experimental order before creating tiles.\")\n            return None\n\n        if getattr(self, \"_num_tiles\", None) is None:\n            print(\"Assign number of tiles before creating tiles.\")\n            return None\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tile id.\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        try:\n            polyDT_tile_path = self._polyDT_root_path / Path(tile_id)\n            polyDT_tile_path.mkdir()\n            for round_idx, round_id in enumerate(self._round_ids):\n                polyDT_round_path = polyDT_tile_path / Path(round_id + \".zarr\")\n                polyDT_round_path.mkdir()\n                polydt_round_attrs_path = polyDT_round_path / Path(\".zattrs\")\n                round_attrs = {\n                    \"bit_linker\": self._experiment_order.to_numpy()[round_idx, 1:]\n                    .astype(int)\n                    .tolist(),\n                }\n                self._save_to_json(round_attrs, polydt_round_attrs_path)\n        except FileExistsError:\n            print(\"Error creating polyDT tile. Does it exist already?\")\n\n        try:\n            readout_tile_path = self._readouts_root_path / Path(tile_id)\n            readout_tile_path.mkdir()\n            for bit_idx, bit_id in enumerate(self._bit_ids):\n                readout_bit_path = readout_tile_path / Path(bit_id + \".zarr\")\n                readout_bit_path.mkdir()\n                readout_bit_attrs_path = readout_bit_path / Path(\".zattrs\")\n                fiducial_channel = str(self._channels_in_data[0])\n                readout_one_channel = str(self._channels_in_data[1])\n\n                if len(self._channels_in_data) == 3:\n                    readout_two_channel = str(self._channels_in_data[2])\n                    condition_one = self._experiment_order[readout_one_channel] == (\n                        bit_idx + 1\n                    )\n                    condition_two = self._experiment_order[readout_two_channel] == (\n                        bit_idx + 1\n                    )\n                    combined_condition = condition_one | condition_two\n\n                else:\n                    combined_condition = self._experiment_order[\n                        readout_one_channel\n                    ] == (bit_idx + 1)\n                matching_rows = self._experiment_order.loc[combined_condition]\n\n                bit_attrs = {\n                    \"round_linker\": int(matching_rows[fiducial_channel].values[0])\n                }\n                self._save_to_json(bit_attrs, readout_bit_attrs_path)\n        except FileExistsError:\n            print(\"Error creating readout tile. Does it exist already?\")\n\n    def load_local_bit_linker(\n        self,\n        tile: Union[int, str],\n        round: Union[int, str],\n    ) -&gt; Optional[Sequence[int]]:\n        \"\"\"Load readout bits linked to fidicual round for one tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n        round : Union[int, str]\n            Round index or round id.\n\n        Returns\n        -------\n        bit_linker : Optional[Sequence[int]]\n            Readout bits linked to fidicual round for one tile.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id.\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                round_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id.\")\n                return None\n            else:\n                round_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n\n        try:\n            zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(round_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n            attributes = self._load_from_json(zattrs_path)\n            return attributes[\"bits\"][1:]\n        except (FileNotFoundError, json.JSONDecodeError):\n            print(tile_id, round_id)\n            print(\"Bit linker attribute not found.\")\n            return None\n\n    def save_local_bit_linker(\n        self,\n        bit_linker: Sequence[int],\n        tile: Union[int, str],\n        round: Union[int, str],\n    ):\n        \"\"\"Save readout bits linked to fidicual round for one tile.\n\n        Parameters\n        ----------\n        bit_linker : Sequence[int]\n            Readout bits linked to fidicual round for one tile.\n        tile : Union[int, str]\n            Tile index or tile id.\n        round : Union[int, str]\n            Round index or round id.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id.\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                round_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id.\")\n                return None\n            else:\n                round_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n\n        try:\n            zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(round_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n            attributes = self._load_from_json(zattrs_path)\n            attributes[\"bits\"] = bit_linker\n            self._save_to_json(attributes, zattrs_path)\n        except (FileNotFoundError, json.JSONDecodeError):\n            print(tile_id, round_id)\n            print(\"Error writing bit linker attribute.\")\n            return None\n\n    def load_local_round_linker(\n        self,\n        tile: Union[int, str],\n        bit: Union[int, str],\n    ) -&gt; Optional[Sequence[int]]:\n        \"\"\"Load fidicual round linked to readout bit for one tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n        bit : Union[int, str]\n            Bit index or bit id.\n\n        Returns\n        -------\n        round_linker : Optional[Sequence[int]]\n            Fidicual round linked to readout bit for one tile.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id.\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(bit, int):\n            if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                return None\n            else:\n                bit_id = self._bit_ids[bit]\n        elif isinstance(bit, str):\n            if bit not in self._bit_ids:\n                print(\"Set valid bit id.\")\n                return None\n            else:\n                bit_id = bit\n        else:\n            print(\"'bit' must be integer index or string identifier\")\n            return None\n\n        try:\n            zattrs_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(bit_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n            attributes = self._load_from_json(zattrs_path)\n            return int(attributes[\"round_linker\"])\n        except FileNotFoundError:\n            print(tile_id, bit_id)\n            print(\"Round linker attribute not found.\")\n            return None\n\n    def save_local_round_linker(\n        self,\n        round_linker: int,\n        tile: Union[int, str],\n        bit: Union[int, str],\n    ):\n        \"\"\"Save fidicual round linker attribute to readout bit for one tile.\n\n        Parameters\n        ----------\n        round_linker : int\n            Fidicual round linked to readout bit for one tile.\n        tile : Union[int, str]\n            Tile index or tile id.\n        bit : Union[int, str]\n            Bit index or bit id.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id.\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(bit, int):\n            if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                return None\n            else:\n                bit_id = self._bit_ids[bit]\n        elif isinstance(bit, str):\n            if bit not in self._bit_ids:\n                print(\"Set valid bit id.\")\n                return None\n            else:\n                bit_id = bit\n        else:\n            print(\"'bit' must be integer index or string identifier\")\n            return None\n\n        try:\n            zattrs_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(bit_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n            attributes = self._load_from_json(zattrs_path)\n            attributes[\"round\"] = int(round_linker)\n            self._save_to_json(attributes, zattrs_path)\n        except (FileNotFoundError, json.JSONDecodeError):\n            print(tile_id, bit_id)\n            print(\"Error writing round linker attribute.\")\n            return None\n\n    def load_local_stage_position_zyx_um(\n        self,\n        tile: Union[int, str],\n        round: Union[int, str],\n    ) -&gt; Optional[ArrayLike]:\n        \"\"\"Load tile stage position for one tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n        round : Union[int, str]\n            Round index or round id.\n\n        Returns\n        -------\n        stage_zyx_um : Optional[ArrayLike]\n            Tile stage position for one tile.\n        affine_zyx_um: Optional[ArrayLike]\n            Affine transformation between stage and camera\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id.\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                round_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id.\")\n                return None\n            else:\n                round_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n\n        try:\n            zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(round_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n            attributes = self._load_from_json(zattrs_path)\n            return np.asarray(attributes[\"stage_zyx_um\"], dtype=np.float32),\\\n                np.asarray(attributes[\"affine_zyx_px\"], dtype=np.float32)\n        except FileNotFoundError:\n            print(tile_id, round_id)\n            print(\"Stage position attribute not found.\")\n            return None\n\n    def save_local_stage_position_zyx_um(\n        self,\n        stage_zyx_um: ArrayLike,\n        affine_zyx_px: ArrayLike,\n        tile: Union[int, str],\n        round: Union[int, str],\n    ):\n        \"\"\"Save tile stage position for one tile.\n\n        Parameters\n        ----------\n        stage_zyx_um : ArrayLike\n            Tile stage position for one tile.\n        affine_zyx_px; ArrayLike\n            4x4 homogeneous affine matrix for stage transformation\n        tile : Union[int, str]\n            Tile index or tile id.\n        round : Union[int, str]\n            Round index or round id.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id.\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                round_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id.\")\n                return None\n            else:\n                round_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n\n        try:\n            zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(round_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n            attributes = self._load_from_json(zattrs_path)\n            attributes[\"stage_zyx_um\"] = stage_zyx_um.tolist()\n            attributes[\"affine_zyx_px\"] = affine_zyx_px.tolist()\n            self._save_to_json(attributes, zattrs_path)\n        except (FileNotFoundError, json.JSONDecodeError):\n            print(tile_id, round_id)\n            print(\"Error writing stage position attribute.\")\n            return None\n\n    def load_local_wavelengths_um(\n        self,\n        tile: Union[int, str],\n        round: Optional[Union[int, str]] = None,\n        bit: Optional[Union[int, str]] = None,\n    ) -&gt; Optional[tuple[float, float]]:\n        \"\"\"Load wavelengths for fidicual OR readout bit for one tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n        round : Optional[Union[int, str]]   \n            Round index or round id.\n        bit : Optional[Union[int, str]]\n            Bit index or bit id.\n\n        Returns\n        -------\n        wavelengths_um : Optional[tuple[float, float]]\n            Wavelengths for fidicual OR readout bit for one tile.\n        \"\"\"\n\n        if (round is None and bit is None) or (round is not None and bit is not None):\n            print(\"Provide either 'round' or 'bit', but not both\")\n            return None\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if bit is not None:\n            if isinstance(bit, int):\n                if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                    print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                    return None\n                else:\n                    local_id = self._bit_ids[bit]\n            elif isinstance(bit, str):\n                if bit not in self._bit_ids:\n                    print(\"Set valid bit id\")\n                    return None\n                else:\n                    local_id = bit\n            else:\n                print(\"'bit' must be integer index or string identifier\")\n                return None\n            zattrs_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n        else:\n            if isinstance(round, int):\n                if round &lt; 0:\n                    print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                    return None\n                else:\n                    local_id = self._round_ids[round]\n            elif isinstance(round, str):\n                if round not in self._round_ids:\n                    print(\"Set valid round id\")\n                    return None\n                else:\n                    local_id = round\n            else:\n                print(\"'round' must be integer index or string identifier\")\n                return None\n            zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n\n        try:\n            attributes = self._load_from_json(zattrs_path)\n            ex_wavelength_um = attributes[\"excitation_um\"]\n            em_wavelength_um = attributes[\"emission_um\"]\n            return (ex_wavelength_um, em_wavelength_um)\n        except KeyError:\n            print(\"Wavelength attributes not found.\")\n            return None\n\n    def save_local_wavelengths_um(\n        self,\n        wavelengths_um: tuple[float, float],\n        tile: Union[int, str],\n        round: Optional[Union[int, str]] = None,\n        bit: Optional[Union[int, str]] = None,\n    ) -&gt; Optional[tuple[float, float]]:\n        \"\"\"Save wavelengths for fidicual OR readout bit for one tile.\n\n        Parameters\n        ----------\n        wavelengths_um : tuple[float, float]\n            Wavelengths for fidicual OR readout bit for one tile.\n        tile : Union[int, str]\n            Tile index or tile id.\n        round : Optional[Union[int, str]]\n            Round index or round id.\n        bit : Optional[Union[int, str]]\n            Bit index or bit id.\n\n        Returns\n        -------\n        wavelengths_um : Optional[tuple[float, float]]\n            Wavelengths for fidicual OR readout bit for one tile.\n        \"\"\"\n\n        if (round is None and bit is None) or (round is not None and bit is not None):\n            print(\"Provide either 'round' or 'bit', but not both\")\n            return None\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if bit is not None:\n            if isinstance(bit, int):\n                if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                    print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                    return None\n                else:\n                    local_id = self._bit_ids[bit]\n            elif isinstance(bit, str):\n                if bit not in self._bit_ids:\n                    print(\"Set valid bit id\")\n                    return None\n                else:\n                    local_id = bit\n            else:\n                print(\"'bit' must be integer index or string identifier\")\n                return None\n            zattrs_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n        else:\n            if isinstance(round, int):\n                if round &lt; 0:\n                    print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                    return None\n                else:\n                    local_id = self._round_ids[round]\n            elif isinstance(round, str):\n                if round not in self._round_ids:\n                    print(\"Set valid round id\")\n                    return None\n                else:\n                    local_id = round\n            else:\n                print(\"'round' must be integer index or string identifier\")\n                return None\n            zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n\n        try:\n            attributes = self._load_from_json(zattrs_path)\n            attributes[\"excitation_um\"] = float(wavelengths_um[0])\n            attributes[\"emission_um\"] = float(wavelengths_um[1])\n            self._save_to_json(attributes, zattrs_path)\n        except (FileNotFoundError, json.JSONDecodeError):\n            print(\"Error writing wavelength attributes.\")\n            return None\n\n    def load_local_corrected_image(\n        self,\n        tile: Union[int, str],\n        round: Optional[Union[int, str]] = None,\n        bit: Optional[Union[int, str]] = None,\n        return_future: Optional[bool] = True,\n    ) -&gt; Optional[ArrayLike]:\n        \"\"\"Load gain and offset corrected image for fiducial OR readout bit for one tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n        round : Optional[Union[int, str]]\n            Round index or round id.\n        bit : Optional[Union[int, str]]\n            Bit index or bit id.\n        return_future : Optional[bool]\n            Return future array.\n\n        Returns\n        -------\n        corrected_image : Optional[ArrayLike]\n            Gain and offset corrected image for fiducial OR readout bit for one tile.\n        \"\"\"\n\n        if (round is None and bit is None) or (round is not None and bit is not None):\n            print(\"Provide either 'round' or 'bit', but not both\")\n            return None\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if bit is not None:\n            if isinstance(bit, int):\n                if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                    print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                    return None\n                else:\n                    local_id = self._bit_ids[bit]\n            elif isinstance(bit, str):\n                if bit not in self._bit_ids:\n                    print(\"Set valid bit id\")\n                    return None\n                else:\n                    local_id = bit\n            else:\n                print(\"'bit' must be integer index or string identifier\")\n                return None\n            current_local_zarr_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\"corrected_data\")\n            )\n        else:\n            if isinstance(round, int):\n                if round &lt; 0:\n                    print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                    return None\n                else:\n                    local_id = self._round_ids[round]\n            elif isinstance(round, str):\n                if round not in self._round_ids:\n                    print(\"Set valid round id\")\n                    return None\n                else:\n                    local_id = round\n            else:\n                print(\"'round' must be integer index or string identifier\")\n                return None\n            current_local_zarr_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\"corrected_data\")\n            )\n\n        if not Path(current_local_zarr_path).exists():\n            print(\"Corrected image not found.\")\n            return None\n\n        try:\n            spec = self._zarrv2_spec.copy()\n            spec[\"metadata\"][\"dtype\"] = \"&lt;u2\"\n            corrected_image = self._load_from_zarr_array(\n                self._get_kvstore_key(current_local_zarr_path),\n                spec,\n                return_future,\n            )\n            return corrected_image\n        except (IOError, OSError, ZarrError):\n            print(\"Error loading corrected image.\")\n            return None\n\n    def save_local_corrected_image(\n        self,\n        image: ArrayLike,\n        tile: Union[int, str],\n        gain_correction: bool = True,\n        hotpixel_correction: bool = True,\n        shading_correction: bool = False,\n        psf_idx: int = 0,\n        round: Optional[Union[int, str]] = None,\n        bit: Optional[Union[int, str]] = None,\n        return_future: Optional[bool] = False,\n    ):\n        \"\"\"Save gain and offset corrected image.\n\n        Parameters\n        ----------\n        image : ArrayLike\n            Local corrected image.\n        tile : Union[int, str]\n            Tile index or tile id.\n        gain_correction : bool\n            Gain correction applied (True) or not (False).\n        hotpixel_correction : bool\n            Hotpixel correction applied (True) or not (False).\n        shading_correction : bool\n            Shading correction applied (True) or not (False).\n        psf_idx : int\n            PSF index.\n        round : Optional[Union[int, str]]\n            Round index or round id.\n        bit : Optional[Union[int, str]]\n            Bit index or bit id.\n        return_future : Optional[bool]\n            Return future array.\n   \"\"\"\n\n        if (round is None and bit is None) or (round is not None and bit is not None):\n            print(\"Provide either 'round' or 'bit', but not both\")\n            return None\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if bit is not None:\n            if isinstance(bit, int):\n                if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                    print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                    return None\n                else:\n                    local_id = self._bit_ids[bit]\n            elif isinstance(bit, str):\n                if bit not in self._bit_ids:\n                    print(\"Set valid bit id\")\n                    return None\n                else:\n                    local_id = bit\n            else:\n                print(\"'bit' must be integer index or string identifier\")\n                return None\n            current_local_zarr_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\"corrected_data\")\n            )\n            current_local_zattrs_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n        else:\n            if isinstance(round, int):\n                if round &lt; 0:\n                    print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                    return None\n                else:\n                    local_id = self._round_ids[round]\n            elif isinstance(round, str):\n                if round not in self._round_ids:\n                    print(\"Set valid round id\")\n                    return None\n                else:\n                    local_id = round\n            else:\n                print(\"'round' must be integer index or string identifier\")\n                return None\n            current_local_zarr_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\"corrected_data\")\n            )\n            current_local_zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n\n        try:\n            self._save_to_zarr_array(\n                image,\n                self._get_kvstore_key(current_local_zarr_path),\n                self._zarrv2_spec,\n                return_future,\n            )\n            attributes = self._load_from_json(current_local_zattrs_path)\n            attributes[\"gain_correction\"] = (gain_correction,)\n            attributes[\"hotpixel_correction\"] = (hotpixel_correction,)\n            attributes[\"shading_correction\"] = (shading_correction)\n            attributes[\"psf_idx\"] = psf_idx\n            self._save_to_json(attributes, current_local_zattrs_path)\n        except (IOError, OSError, TimeoutError) as e:\n            print(e)\n            print(\"Error saving corrected image.\")\n            return None\n\n    def load_local_rigid_xform_xyz_px(\n        self,\n        tile: Union[int, str],\n        round: Union[int, str],\n    ) -&gt; Optional[ArrayLike]:\n        \"\"\"Load calculated rigid registration transform for one round and tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n        round : Union[int, str]\n            Round index or round id.\n\n        Returns\n        -------\n        rigid_xform_xyz_px : Optional[ArrayLike]\n            Local rigid registration transform for one round and tile.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                round_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id\")\n                return None\n            else:\n                round_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n        try:\n            zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(round_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n            attributes = self._load_from_json(zattrs_path)\n            rigid_xform_xyz_px = np.asarray(\n                attributes[\"rigid_xform_xyz_px\"], dtype=np.float32\n            )\n            return rigid_xform_xyz_px\n        except (FileNotFoundError, json.JSONDecodeError):\n            print(tile_id, round_id)\n            print(\"Rigid transform mapping back to first round not found.\")\n            return None\n\n    def save_local_rigid_xform_xyz_px(\n        self,\n        rigid_xform_xyz_px: ArrayLike,\n        tile: Union[int, str],\n        round: Union[int, str],\n    ) -&gt; Optional[ArrayLike]:\n        \"\"\"Save calculated rigid registration transform for one round and tile.\n\n        Parameters\n        ----------\n        rigid_xform_xyz_px : ArrayLike\n            Local rigid registration transform for one round and tile.\n        tile : Union[int, str]\n            Tile index or tile id.\n        round : Union[int, str]\n            Round index or round id.\n\n        Returns\n        -------\n        rigid_xform_xyz_px : Optional[ArrayLike]\n            Local rigid registration transform for one round and tile.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                round_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id\")\n                return None\n            else:\n                round_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n        try:\n            zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(round_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n            attributes = self._load_from_json(zattrs_path)\n            attributes[\"rigid_xform_xyz_px\"] = rigid_xform_xyz_px.tolist()\n            self._save_to_json(attributes, zattrs_path)\n        except (FileNotFoundError, json.JSONDecodeError):\n            print(\"Error writing rigid transform attribute.\")\n            return None\n\n    def load_coord_of_xform_px(\n        self,\n        tile: Optional[Union[int, str]],\n        round: Optional[Union[int, str]],\n        return_future: Optional[bool] = True,\n    ) -&gt; Optional[tuple[ArrayLike, ArrayLike]]:\n        \"\"\"Local fidicual optical flow matrix for one round and tile.\n\n        Parameters\n        ----------\n        tile : Optional[Union[int, str]]\n            Tile index or tile id.\n        round : Optional[Union[int, str]]\n            Round index or round id.\n        return_future : Optional[bool]\n            Return future array.\n\n        Returns\n        -------\n        of_xform_px : Optional[ArrayLike]\n            Local fidicual optical flow matrix for one round and tile.\n        downsampling : Optional[ArrayLike]\n            Downsampling factor.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                round_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id\")\n                return None\n            else:\n                round_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n\n        current_local_zarr_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(round_id + \".zarr\")\n            / Path(\"opticalflow_xform_px\")\n        )\n        zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(round_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n\n        if not Path(current_local_zarr_path).exists():\n            print(\"Optical flow transform mapping back to first round not found.\")\n            return None\n\n        try:\n            compressor = {\n                \"id\": \"blosc\",\n                \"cname\": \"zstd\",\n                \"clevel\": 5,\n                \"shuffle\": 2,\n            }\n            spec_of = {\n                \"driver\": \"zarr\",\n                \"kvstore\": None,\n                \"metadata\": {\"compressor\": compressor},\n                \"open\": True,\n                \"assume_metadata\": False,\n                \"create\": True,\n                \"delete_existing\": False,\n            }\n            spec_of[\"metadata\"][\"dtype\"] = \"&lt;f4\"\n            of_xform_px = self._load_from_zarr_array(\n                self._get_kvstore_key(current_local_zarr_path),\n                spec_of.copy(),\n                return_future,\n            )\n            attributes = self._load_from_json(zattrs_path)\n            block_size = np.asarray(\n                attributes[\"block_size\"], dtype=np.float32\n            )\n            block_stride = np.asarray(\n                attributes[\"block_stride\"], dtype=np.float32\n            )\n\n            return (of_xform_px, block_size, block_stride)\n        except (IOError, OSError, ZarrError) as e:\n            print(e)\n            print(\"Error loading optical flow transform.\")\n            return None\n\n    def save_coord_of_xform_px(\n        self,\n        of_xform_px: ArrayLike,\n        tile: Union[int, str],\n        block_size: Sequence[float],\n        block_stride: Sequence[float],\n        round: Union[int, str],\n        return_future: Optional[bool] = False,\n    ):\n        \"\"\"Save fidicual optical flow matrix for one round and tile.\n\n        Parameters\n        ----------\n        of_xform_px : ArrayLike\n            Local fidicual optical flow matrix for one round and tile.\n        tile : Union[int, str]\n            Tile index or tile id.\n        block_size : Sequence[float]\n            Block size for pixel warp\n        block_stride: Sequence[float]\n            Block stride for pixel warp\n        round : Union[int, str] \n            Round index or round id.\n        return_future : Optional[bool]\n            Return future array.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                local_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id\")\n                return None\n            else:\n                local_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n        current_local_zarr_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\"opticalflow_xform_px\")\n        )\n        current_local_zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n\n        try:\n            compressor = {\n                \"id\": \"blosc\",\n                \"cname\": \"zstd\",\n                \"clevel\": 5,\n                \"shuffle\": 2,\n            }\n            spec_of = {\n                \"driver\": \"zarr\",\n                \"kvstore\": None,\n                \"metadata\": {\"compressor\": compressor},\n                \"open\": True,\n                \"assume_metadata\": False,\n                \"create\": True,\n                \"delete_existing\": False,\n            }\n            self._save_to_zarr_array(\n                of_xform_px,\n                self._get_kvstore_key(current_local_zarr_path),\n                spec_of.copy(),\n                return_future,\n            )\n            attributes = self._load_from_json(current_local_zattrs_path)\n            attributes[\"block_size\"] = block_size.tolist()\n            attributes[\"block_stride\"] = block_stride.tolist()\n            self._save_to_json(attributes, current_local_zattrs_path)\n        except (IOError, OSError, TimeoutError):\n            print(\"Error saving optical flow transform.\")\n            return None\n\n    def load_local_registered_image(\n        self,\n        tile: Union[int, str],\n        round: Optional[Union[int, str]] = None,\n        bit: Optional[Union[int, str]] = None,\n        return_future: Optional[bool] = True,\n    ) -&gt; Optional[ArrayLike]:\n        \"\"\"Local registered, deconvolved image for fidiculial OR readout bit for one tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n        round : Optional[Union[int, str]]\n            Round index or round id.\n        bit : Optional[Union[int, str]]\n            Bit index or bit id.\n        return_future : Optional[bool]\n            Return future array.\n\n        Returns\n        -------\n        registered_decon_image : Optional[ArrayLike]\n            Registered, deconvolved image for fidiculial OR readout bit for one tile.\n        \"\"\"\n\n        if (round is None and bit is None) or (round is not None and bit is not None):\n            print(\"Provide either 'round' or 'bit', but not both\")\n            return None\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if bit is not None:\n            if isinstance(bit, int):\n                if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                    print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                    return None\n                else:\n                    local_id = self._bit_ids[bit]\n            elif isinstance(bit, str):\n                if bit not in self._bit_ids:\n                    print(\"Set valid bit id\")\n                    return None\n                else:\n                    local_id = bit\n            else:\n                print(\"'bit' must be integer index or string identifier\")\n                return None\n            current_local_zarr_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\"registered_decon_data\")\n            )\n        else:\n            if isinstance(round, int):\n                if round &lt; 0:\n                    print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                    return None\n                else:\n                    local_id = self._round_ids[round]\n            elif isinstance(round, str):\n                if round not in self._round_ids:\n                    print(\"Set valid round id\")\n                    return None\n                else:\n                    local_id = round\n            else:\n                print(\"'round' must be integer index or string identifier\")\n                return None\n            current_local_zarr_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\"registered_decon_data\")\n            )\n\n        if not Path(current_local_zarr_path).exists():\n            #print(\"Registered deconvolved image not found.\")\n            return None\n\n        try:\n            spec = self._zarrv2_spec.copy()\n            spec[\"metadata\"][\"dtype\"] = \"&lt;u2\"\n            registered_decon_image = self._load_from_zarr_array(\n                self._get_kvstore_key(current_local_zarr_path),\n                spec,\n                return_future,\n            )\n            return registered_decon_image\n        except (IOError, OSError, ZarrError) as e:\n            print(e)\n            print(\"Error loading registered deconvolved image.\")\n            return None\n\n    def save_local_registered_image(\n        self,\n        registered_image: ArrayLike,\n        tile: Union[int, str],\n        deconvolution: bool = True,\n        round: Optional[Union[int, str]] = None,\n        bit: Optional[Union[int, str]] = None,\n        return_future: Optional[bool] = False,\n    ):\n        \"\"\"Save registered, deconvolved image.\n\n        Parameters\n        ----------\n        registered_image : ArrayLike\n            Registered, deconvolved image.\n        tile : Union[int, str]\n            Tile index or tile id.\n        deconvolution : bool\n            Deconvolution applied (True) or not (False).\n        round : Optional[Union[int, str]]\n            Round index or round id.\n        bit : Optional[Union[int, str]]\n            Bit index or bit id.\n        return_future : Optional[bool]\n            Return future array.\n        \"\"\"\n\n        if (round is None and bit is None) or (round is not None and bit is not None):\n            print(\"Provide either 'round' or 'bit', but not both\")\n            return None\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if bit is not None:\n            if isinstance(bit, int):\n                if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                    print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                    return None\n                else:\n                    local_id = self._bit_ids[bit]\n            elif isinstance(bit, str):\n                if bit not in self._bit_ids:\n                    print(\"Set valid bit id\")\n                    return None\n                else:\n                    local_id = bit\n            else:\n                print(\"'bit' must be integer index or string identifier\")\n                return None\n            current_local_zarr_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\"registered_decon_data\")\n            )\n            current_local_zattrs_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n        else:\n            if isinstance(round, int):\n                if round &lt; 0:\n                    print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                    return None\n                else:\n                    local_id = self._round_ids[round]\n            elif isinstance(round, str):\n                if round not in self._round_ids:\n                    print(\"Set valid round id\")\n                    return None\n                else:\n                    local_id = round\n            else:\n                print(\"'round' must be integer index or string identifier\")\n                return None\n            current_local_zarr_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\"registered_decon_data\")\n            )\n            current_local_zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n\n        try:\n            spec = self._zarrv2_spec.copy()\n            spec[\"metadata\"][\"dtype\"] = \"&lt;u2\"\n            self._save_to_zarr_array(\n                registered_image,\n                self._get_kvstore_key(current_local_zarr_path),\n                spec,\n                return_future,\n            )\n            attributes = self._load_from_json(current_local_zattrs_path)\n            attributes[\"deconvolution\"] = deconvolution\n            self._save_to_json(attributes, current_local_zattrs_path)\n        except (IOError, OSError, TimeoutError):\n            print(\"Error saving corrected image.\")\n            return None\n\n    def load_local_ufish_image(\n        self,\n        tile: Union[int, str],\n        bit: Union[int, str],\n        return_future: Optional[bool] = True,\n    ) -&gt; Optional[ArrayLike]:\n        \"\"\"Load readout bit U-FISH prediction image for one tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n        bit : Union[int, str]\n            Bit index or bit id.\n        return_future : Optional[bool]\n\n        Returns\n        -------\n        registered_ufish_image : Optional[ArrayLike]\n            U-FISH prediction image for one tile.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(bit, int):\n            if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                return None\n            else:\n                bit_id = self._bit_ids[bit]\n        elif isinstance(bit, str):\n            if bit not in self._bit_ids:\n                print(\"Set valid bit id\")\n                return None\n            else:\n                bit_id = bit\n        else:\n            print(\"'bit' must be integer index or string identifier\")\n            return None\n\n        current_local_zarr_path = str(\n            self._readouts_root_path\n            / Path(tile_id)\n            / Path(bit_id + \".zarr\")\n            / Path(\"registered_ufish_data\")\n        )\n\n        if not Path(current_local_zarr_path).exists():\n            print(\"U-FISH prediction image not found.\")\n            return None\n\n        try:\n            spec = self._zarrv2_spec.copy()\n            spec[\"metadata\"][\"dtype\"] = \"&lt;f4\"\n            registered_ufish_image = self._load_from_zarr_array(\n                self._get_kvstore_key(current_local_zarr_path),\n                spec,\n                return_future,\n            )\n            return registered_ufish_image\n        except (IOError, OSError, ZarrError) as e:\n            print(e)\n            print(\"Error loading U-FISH image.\")\n            return None\n\n    def save_local_ufish_image(\n        self,\n        ufish_image: ArrayLike,\n        tile: Union[int, str],\n        bit: Union[int, str],\n        return_future: Optional[bool] = False,\n    ):\n        \"\"\"Save U-FISH prediction image.\n\n        Parameters\n        ----------\n        ufish_image : ArrayLike\n            U-FISH prediction image.\n        tile : Union[int, str]\n            Tile index or tile id.\n        bit : Union[int, str]\n            Bit index or bit id.\n        return_future : Optional[bool]\n            Return future array.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if bit is not None:\n            if isinstance(bit, int):\n                if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                    print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                    return None\n                else:\n                    local_id = self._bit_ids[bit]\n            elif isinstance(bit, str):\n                if bit not in self._bit_ids:\n                    print(\"Set valid bit id\")\n                    return None\n                else:\n                    local_id = bit\n            else:\n                print(\"'bit' must be integer index or string identifier\")\n                return None\n            current_local_zarr_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\"registered_ufish_data\")\n            )\n\n        try:\n            self._save_to_zarr_array(\n                ufish_image,\n                self._get_kvstore_key(current_local_zarr_path),\n                self._zarrv2_spec.copy(),\n                return_future,\n            )\n        except (IOError, OSError, ZarrError) as e:\n            print(e)\n            print(\"Error saving U-Fish image.\")\n            return None\n\n    def load_local_ufish_spots(\n        self,\n        tile: Union[int, str],\n        bit: Union[int, str],\n    ) -&gt; Optional[pd.DataFrame]:\n        \"\"\"Load U-FISH spot localizations and features for one tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n        bit : Union[int, str]\n            Bit index or bit id.\n\n        Returns\n        -------\n        ufish_localizations : Optional[pd.DataFrame]\n            U-FISH localizations and features for one tile.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(bit, int):\n            if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                return None\n            else:\n                bit_id = self._bit_ids[bit]\n        elif isinstance(bit, str):\n            if bit not in self._bit_ids:\n                print(\"Set valid bit id\")\n                return None\n            else:\n                bit_id = bit\n        else:\n            print(\"'bit' must be integer index or string identifier\")\n            return None\n\n        current_ufish_localizations_path = (\n            self._ufish_localizations_root_path\n            / Path(tile_id)\n            / Path(bit_id + \".parquet\")\n        )\n\n        if not current_ufish_localizations_path.exists():\n            print(\"U-FISH localizations not found.\")\n            return None\n        else:\n            ufish_localizations = self._load_from_parquet(\n                current_ufish_localizations_path\n            )\n            return ufish_localizations\n\n    def save_local_ufish_spots(\n        self,\n        spot_df: pd.DataFrame,\n        tile: Union[int, str],\n        bit: Union[int, str],\n    ):\n        \"\"\"Save U-FISH localizations and features.\n\n        Parameters\n        ----------\n        spot_df : pd.DataFrame\n            U-FISH localizations and features.\n        tile : Union[int, str]\n            Tile index or tile id.\n        bit : Union[int, str]\n            Bit index or bit id.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(bit, int):\n            if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                return None\n            else:\n                bit_id = self._bit_ids[bit]\n        elif isinstance(bit, str):\n            if bit not in self._bit_ids:\n                print(\"Set valid bit id\")\n                return None\n            else:\n                bit_id = bit\n        else:\n            print(\"'bit' must be integer index or string identifier\")\n            return None\n\n        if not (self._ufish_localizations_root_path / Path(tile_id)).exists():\n            (self._ufish_localizations_root_path / Path(tile_id)).mkdir()\n\n        current_ufish_localizations_path = (\n            self._ufish_localizations_root_path\n            / Path(tile_id)\n            / Path(bit_id + \".parquet\")\n        )\n\n        try:\n            self._save_to_parquet(spot_df, current_ufish_localizations_path)\n        except (IOError, OSError) as e:\n            print(e)\n            print(\"Error saving U-FISH localizations.\")\n            return None\n\n    def load_global_coord_xforms_um(\n        self,\n        tile: Union[int, str],\n    ) -&gt; Optional[tuple[ArrayLike, ArrayLike, ArrayLike]]:\n        \"\"\"Load global registration transform for one tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n\n        Returns\n        -------\n        affine_zyx_um : Optional[ArrayLike]\n            Global affine registration transform for one tile.\n        origin_zyx_um : Optional[ArrayLike]\n            Global origin registration transform for one tile.\n        spacing_zyx_um : Optional[ArrayLike]\n            Global spacing registration transform for one tile.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None, None, None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None, None, None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        try:\n            zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(self._round_ids[0] + \".zarr\")\n                / Path(\".zattrs\")\n            )\n            attributes = self._load_from_json(zattrs_path)\n            affine_zyx_um = np.asarray(attributes[\"affine_zyx_um\"], dtype=np.float32)\n            origin_zyx_um = np.asarray(attributes[\"origin_zyx_um\"], dtype=np.float32)\n            spacing_zyx_um = np.asarray(attributes[\"spacing_zyx_um\"], dtype=np.float32)\n            return (affine_zyx_um, origin_zyx_um, spacing_zyx_um)\n        except (FileNotFoundError, json.JSONDecodeError):\n            print(tile_id, self._round_ids[0])\n            print(\"Global coordinate transforms not found\")\n            return None, None, None\n\n    def save_global_coord_xforms_um(\n        self,\n        affine_zyx_um: ArrayLike,\n        origin_zyx_um: ArrayLike,\n        spacing_zyx_um: ArrayLike,\n        tile: Union[int, str],\n    ) -&gt; None:\n        \"\"\"Save global registration transform for one tile.\n\n        Parameters\n        ----------\n        affine_zyx_um : ArrayLike\n            Global affine registration transform for one tile.\n        origin_zyx_um : ArrayLike\n            Global origin registration transform for one tile.\n        spacing_zyx_um : ArrayLike\n            Global spacing registration transform for one tile.\n        tile : Union[int, str]\n            Tile index or tile id.\n        \"\"\"\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        try:\n            zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(self._round_ids[0] + \".zarr\")\n                / Path(\".zattrs\")\n            )\n            attributes = self._load_from_json(zattrs_path)\n            attributes[\"affine_zyx_um\"] = affine_zyx_um.tolist()\n            attributes[\"origin_zyx_um\"] = origin_zyx_um.tolist()\n            attributes[\"spacing_zyx_um\"] = spacing_zyx_um.tolist()\n            self._save_to_json(attributes, zattrs_path)\n        except (FileNotFoundError, json.JSONDecodeError) as e:\n            print(e)\n            print(\"Could not save global coordinate transforms.\")\n\n    def load_global_fidicual_image(\n        self,\n        return_future: Optional[bool] = True,\n    ) -&gt; Optional[tuple[ArrayLike, ArrayLike, ArrayLike, ArrayLike]]:\n        \"\"\"Load downsampled, fused fidicual image.\n\n        Parameters\n        ----------\n        return_future : Optional[bool]\n            Return future array.\n\n        Returns\n        -------\n        fused_image : Optional[ArrayLike]\n            Downsampled, fused fidicual image.\n        affine_zyx_um : Optional[ArrayLike]\n            Global affine registration transform for fused image.\n        origin_zyx_um : Optional[ArrayLike]\n            Global origin registration transform for fused image.\n        spacing_zyx_um : Optional[ArrayLike]\n            Global spacing registration transform for fused image.\n        \"\"\"\n\n        current_local_zarr_path = str(\n            self._fused_root_path / Path(\"fused.zarr\") / Path(\"fused_polyDT_iso_zyx\")\n        )\n\n        if not Path(current_local_zarr_path).exists():\n            print(\"Globally registered, fused image not found.\")\n            return None\n\n        zattrs_path = str(current_local_zarr_path / Path(\".zattrs\"))\n\n        try:\n            fused_image = self._load_from_zarr_array(\n                self._get_kvstore_key(current_local_zarr_path),\n                self._zarrv2_spec.copy(),\n                return_future,\n            )\n            attributes = self._load_from_json(zattrs_path)\n            affine_zyx_um = np.asarray(attributes[\"affine_zyx_um\"], dtype=np.float32)\n            origin_zyx_um = np.asarray(attributes[\"origin_zyx_um\"], dtype=np.float32)\n            spacing_zyx_um = np.asarray(attributes[\"spacing_zyx_um\"], dtype=np.float32)\n            return fused_image, affine_zyx_um, origin_zyx_um, spacing_zyx_um\n        except (IOError, OSError, ZarrError):\n            print(\"Error loading globally registered, fused image.\")\n            return None\n\n    def save_global_fidicual_image(\n        self,\n        fused_image: ArrayLike,\n        affine_zyx_um: ArrayLike,\n        origin_zyx_um: ArrayLike,\n        spacing_zyx_um: ArrayLike,\n        fusion_type: str = \"polyDT\",\n        return_future: Optional[bool] = False,\n    ):\n        \"\"\"Save downsampled, fused fidicual image.\n\n        Parameters\n        ----------\n        fused_image : ArrayLike\n            Downsampled, fused fidicual image.\n        affine_zyx_um : ArrayLike\n            Global affine registration transform for fused image.\n        origin_zyx_um : ArrayLike\n            Global origin registration transform for fused image.\n        spacing_zyx_um : ArrayLike\n            Global spacing registration transform for fused image.\n        fusion_type : str\n            Type of fusion (polyDT or all_channels).\n        return_future : Optional[bool]\n            Return future array.\n        \"\"\"\n\n        if fusion_type == \"polyDT\":\n            filename = \"fused_polyDT_iso_zyx\"\n        else:\n            filename = \"fused_all_channels_zyx\"\n        current_local_zarr_path = str(\n            self._fused_root_path / Path(\"fused.zarr\") / Path(filename)\n        )\n        current_local_zattrs_path = str(\n            self._fused_root_path\n            / Path(\"fused.zarr\")\n            / Path(filename)\n            / Path(\".zattrs\")\n        )\n\n        attributes = {\n            \"affine_zyx_um\": affine_zyx_um.tolist(),\n            \"origin_zyx_um\": origin_zyx_um.tolist(),\n            \"spacing_zyx_um\": spacing_zyx_um.tolist(),\n        }\n        try:\n            self._save_to_zarr_array(\n                fused_image.astype(np.uint16),\n                self._get_kvstore_key(current_local_zarr_path),\n                self._zarrv2_spec.copy(),\n                return_future,\n            )\n            self._save_to_json(attributes, current_local_zattrs_path)\n        except (IOError, OSError, TimeoutError):\n            print(\"Error saving fused image.\")\n            return None\n\n    def load_local_decoded_spots(\n        self,\n        tile: Union[int, str],\n    ) -&gt; Optional[pd.DataFrame]:\n        \"\"\"Load decoded spots and features for one tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n\n        Returns\n        -------\n        tile_features : Optional[pd.DataFrame]\n            Decoded spots and features for one tile.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        current_tile_features_path = self._decoded_root_path / Path(\n            tile_id + \"_decoded_features.parquet\"\n        )\n\n        if not current_tile_features_path.exists():\n            print(\"Decoded spots not found.\")\n            return None\n        else:\n            tile_features = self._load_from_parquet(current_tile_features_path)\n            return tile_features\n\n    def save_local_decoded_spots(\n        self,\n        features_df: pd.DataFrame,\n        tile: Union[int, str],\n    ) -&gt; None:\n        \"\"\"Save decoded spots and features for one tile.\n\n        Parameters\n        ----------\n        features_df : pd.DataFrame\n            Decoded spots and features for one tile.\n        tile : Union[int, str]\n            Tile index or tile id.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        current_tile_features_path = self._decoded_root_path / Path(\n            tile_id + \"_decoded_features.parquet\"\n        )\n\n        self._save_to_parquet(features_df, current_tile_features_path)\n\n    def load_global_filtered_decoded_spots(\n        self,\n    ) -&gt; Optional[pd.DataFrame]:\n        \"\"\"Load all decoded and filtered spots.\n\n        Returns\n        -------\n        all_tiles_filtered : Optional[pd.DataFrame]\n            All decoded and filtered spots.\n        \"\"\"\n\n        current_global_filtered_decoded_dir_path = self._datastore_path / Path(\n            \"all_tiles_filtered_decoded_features\"\n        )\n        current_global_filtered_decoded_path = (\n            current_global_filtered_decoded_dir_path / Path(\"decoded_features.parquet\")\n        )\n\n        if not current_global_filtered_decoded_path.exists():\n            print(\"Global, filtered, decoded spots not found.\")\n            return None\n        else:\n            all_tiles_filtered = self._load_from_parquet(\n                current_global_filtered_decoded_path\n            )\n            return all_tiles_filtered\n\n    def save_global_filtered_decoded_spots(\n        self,\n        filtered_decoded_df: pd.DataFrame,\n    ):\n        \"\"\"Save all decoded and filtered spots.\n\n        Parameters\n        ----------\n        filtered_decoded_df : pd.DataFrame\n            All decoded and filtered spots.\n        \"\"\"\n\n        current_global_filtered_decoded_dir_path = self._datastore_path / Path(\n            \"all_tiles_filtered_decoded_features\"\n        )\n\n        if not current_global_filtered_decoded_dir_path.exists():\n            current_global_filtered_decoded_dir_path.mkdir()\n\n        current_global_filtered_decoded_path = (\n            current_global_filtered_decoded_dir_path / Path(\"decoded_features.parquet\")\n        )\n\n        self._save_to_parquet(filtered_decoded_df, current_global_filtered_decoded_path)\n\n    def load_global_cellpose_outlines(\n        self,\n    ) -&gt; Optional[dict]:\n        \"\"\"Load Cellpose max projection cell outlines.\n\n        Returns\n        -------\n        cellpose_outlines : Optional[dict]\n            Cellpose cell mask outlines.\n        \"\"\"\n\n        current_cellpose_outlines_path = (\n            self._segmentation_root_path / Path(\"cellpose\") / Path(\"cell_outlines.json\")\n        )\n\n        if not current_cellpose_outlines_path.exists():\n            print(\"Cellpose cell mask outlines not found.\")\n            return None\n        else:\n            cellpose_outlines = self._load_from_microjson(\n                current_cellpose_outlines_path\n            )\n            return cellpose_outlines\n\n    def load_global_cellpose_segmentation_image(\n        self,\n        return_future: Optional[bool] = True,\n    ) -&gt; Optional[ArrayLike]:\n        \"\"\"Load Cellpose max projection, downsampled segmentation image.\n\n        Parameters\n        ----------\n        return_future : Optional[bool]\n            Return future array.\n\n        Returns\n        -------\n        fused_image : Optional[ArrayLike]\n            Cellpose max projection, downsampled segmentation image.\n        \"\"\"\n\n        current_local_zarr_path = str(\n            self._segmentation_root_path\n            / Path(\"cellpose\")\n            / Path(\"cellpose.zarr\")\n            / Path(\"masks_polyDT_iso_zyx\")\n        )\n\n        if not current_local_zarr_path.exists():\n            print(\"Cellpose prediction on global fused image not found.\")\n            return None\n\n        try:\n            fused_image = self._load_from_zarr_array(\n                self._get_kvstore_key(current_local_zarr_path),\n                self._zarrv2_spec.copy(),\n                return_future,\n            )\n            return fused_image\n        except (IOError, OSError, ZarrError):\n            print(\"Error loading Cellpose image.\")\n            return None\n\n    def save_global_cellpose_segmentation_image(\n        self,\n        cellpose_image: ArrayLike,\n        downsampling: Sequence[float],\n        return_future: Optional[bool] = False,\n    ):\n        \"\"\"Save Cellpose max projection, downsampled segmentation image.\n\n        Parameters\n        ----------\n        cellpose_image : ArrayLike\n            Cellpose max projection, downsampled segmentation image.\n        downsampling : Sequence[float]\n            Downsample factors.\n        return_future : Optional[bool]\n            Return future array.\n        \"\"\"\n\n        current_local_zarr_path = str(\n            self._segmentation_root_path\n            / Path(\"cellpose\")\n            / Path(\"cellpose.zarr\")\n            / Path(\"masks_polyDT_iso_zyx\")\n        )\n        current_local_zattrs_path = str(\n            self._segmentation_root_path\n            / Path(\"cellpose\")\n            / Path(\"cellpose.zarr\")\n            / Path(\"masks_polyDT_iso_zyx\")\n            / Path(\".zattrs\")\n        )\n\n        attributes = {\"downsampling\": downsampling}\n\n        try:\n            self._save_to_zarr_array(\n                cellpose_image,\n                self._get_kvstore_key(current_local_zarr_path),\n                self._zarrv2_spec.copy(),\n                return_future,\n            )\n            self._save_to_json(attributes, current_local_zattrs_path)\n        except (IOError, OSError, TimeoutError):\n            print(\"Error saving Cellpose image.\")\n            return None\n\n    def save_spots_prepped_for_baysor(self, prepped_for_baysor_df: pd.DataFrame):\n        \"\"\"Save spots prepped for Baysor.\n\n        Parameters\n        ----------\n        prepped_for_baysor_df : pd.DataFrame\n            Spots prepped for Baysor.\n        \"\"\"\n\n        current_global_filtered_decoded_dir_path = self._datastore_path / Path(\n            \"all_tiles_filtered_decoded_features\"\n        )\n\n        if not current_global_filtered_decoded_dir_path.exists():\n            current_global_filtered_decoded_dir_path.mkdir()\n\n        current_global_filtered_decoded_path = (\n            current_global_filtered_decoded_dir_path / Path(\"transcripts.parquet\")\n        )\n\n        self._save_to_parquet(prepped_for_baysor_df, current_global_filtered_decoded_path)\n\n    def run_baysor(self):\n        \"\"\"Run Baysor\"\n\n        Assumes that spots are prepped for Baysor and the Baysor path and options are set.\n        Reformats ROIs into ImageJ style ROIs for later use.\n        \"\"\"\n\n        import subprocess\n\n        baysor_input_path = self._datastore_path / Path(\"all_tiles_filtered_decoded_features\") / Path(\"transcripts.parquet\")\n        baysor_output_path = self._segmentation_root_path / Path(\"baysor\")\n        baysor_output_path.mkdir(exist_ok=True)\n\n        julia_threading = r\"JULIA_NUM_THREADS=\"+str(self._julia_threads)+ \" \"\n        preview_baysor_options = r\"preview -c \" +str(self._baysor_options)\n        command = julia_threading + str(self._baysor_path) + \" \" + preview_baysor_options + \" \" +\\\n            str(baysor_input_path) + \" -o \" + str(baysor_output_path)\n\n        try:\n            result = subprocess.run(command, shell=True, check=True)\n            print(\"Baysor finished with return code:\", result.returncode)\n        except subprocess.CalledProcessError as e:\n            print(\"Baysor failed with:\", e)\n\n        # first try to run Baysor assuming that prior segmentations are present               \n        try:\n            run_baysor_options = r\"run -p -c \" +str(self._baysor_options)\n            command = julia_threading + str(self._baysor_path) + \" \" + run_baysor_options + \" \" +\\\n                str(baysor_input_path) + \" -o \" + str(baysor_output_path) + \\\n                \" --polygon-format GeometryCollectionLegacy --count-matrix-format tsv :cell_id\"\n            result = subprocess.run(command, shell=True, check=True)\n            print(\"Baysor finished with return code:\", result.returncode)\n        except subprocess.CalledProcessError:\n            # then fall back and run without prior segmentations.\n            # IMPORTANT: the .toml file has to be defined correctly for this to work!\n            try:\n                run_baysor_options = r\"run -p -c \" +str(self._baysor_options)\n                command = julia_threading + str(self._baysor_path) + \" \" + run_baysor_options + \" \" +\\\n                    str(baysor_input_path) + \" -o \" + str(baysor_output_path) + \" --count-matrix-format tsv\"\n                result = subprocess.run(command, shell=True, check=True)\n                print(\"Baysor finished with return code:\", result.returncode)\n            except subprocess.CalledProcessError as e:\n                print(\"Baysor failed with:\", e)\n\n    def reformat_baysor_3D_oultines(self):\n        \"\"\"Reformat baysor 3D json file into ImageJ ROIs.\"\"\"\n        import re\n\n        # Load the JSON file\n        baysor_output_path = self._segmentation_root_path / Path(\"baysor\")\n        baysor_segmentation = baysor_output_path / Path(r\"segmentation_polygons_3d.json\")\n        with open(baysor_segmentation, 'r') as file:\n            data = json.load(file)\n\n\n        # Dictionary to group polygons by cell ID\n        cell_polygons = defaultdict(list)\n\n        def parse_z_range(z_range):\n            cleaned_range = re.sub(r\"[^\\d.,-]\", \"\", z_range)  # Remove non-numeric, non-period, non-comma, non-dash characters\n            return map(float, cleaned_range.split(\",\"))\n\n        # Iterate through each z-plane and corresponding polygons\n        for z_range, details in data.items():\n            z_start, z_end = parse_z_range(z_range)\n\n            for geometry in details[\"geometries\"]:\n                coordinates = geometry[\"coordinates\"][0]  # Assuming the outer ring of the polygon\n                cell_id = geometry[\"cell\"]  # Get the cell ID\n\n                # Store the polygon with its z-range\n                cell_polygons[cell_id].append({\n                    \"z_start\": z_start,\n                    \"z_end\": z_end,\n                    \"coordinates\": coordinates\n                })\n\n        rois = []\n\n        # Process each cell ID to create 3D ROIs\n        for cell_id, polygons in cell_polygons.items():\n            for idx, polygon in enumerate(polygons):\n                x_coords = [point[0] for point in polygon[\"coordinates\"]]\n                y_coords = [point[1] for point in polygon[\"coordinates\"]]\n\n\n                z_start = polygon[\"z_start\"]\n                z_end = polygon[\"z_end\"]\n\n                try:\n                    # Create an ImageJRoi object for the polygon using frompoints\n                    coords = list(zip(x_coords, y_coords))  # List of (x, y) tuples\n                    roi = ImagejRoi.frompoints(coords)\n                    roi.roitype = ROI_TYPE.POLYGON  # Set the ROI type to Polygon\n                    roi.coordinates = coords  # Explicitly assign coordinates to the ROI\n                    roi.name = f\"cell_{str(cell_id)}_zstart_{str(z_start)}_zend_{str(z_end)}\"  # Ensure unique name\n                    rois.append(roi)\n                except Exception as e:\n                    print(f\"Error while creating ROI for cell ID {cell_id}: {e}\")\n\n        # Write all ROIs to a ZIP file   \n        output_file = baysor_output_path / Path(r\"3d_cell_rois.zip\")\n        roiwrite(output_file, rois,mode='w')\n\n    def load_global_baysor_filtered_spots(\n        self,\n    ) -&gt; Optional[pd.DataFrame]:\n        \"\"\"Load Baysor re-assigned decoded RNA.\n\n        Assumes Baysor has been run.\n\n        Returns\n        -------\n        baysor_filtered_genes : Optional[pd.DataFrame]\n            Baysor re-assigned decoded RNA.\n        \"\"\"\n\n        current_baysor_spots_path = (\n            self._segmentation_root_path\n            / Path(\"baysor\")\n            / Path(\"segmentation.csv\")\n        )\n\n        if not current_baysor_spots_path.exists():\n            print(\"Baysor filtered genes not found.\")\n            return None\n        else:\n            baysor_filtered_genes = self._load_from_csv(current_baysor_spots_path)\n            return baysor_filtered_genes\n\n    def load_global_baysor_outlines(\n        self,\n    ) -&gt; Optional[dict]:\n        \"\"\"Load Baysor cell outlines.\n\n        Assumes Baysor has been run.\n\n        Returns\n        -------\n        baysor_outlines : Optional[dict]\n            Baysor cell outlines.\n        \"\"\"\n\n        current_baysor_outlines_path = (\n            self._segmentation_root_path \n            / Path(\"baysor\") \n            / Path(r\"3d_cell_rois.zip\")\n        )\n\n        if not current_baysor_outlines_path.exists():\n            print(\"Baysor outlines not found.\")\n            return None\n        else:\n            baysor_rois = roiread(current_baysor_outlines_path)\n            return baysor_rois\n\n    @staticmethod\n    def _roi_to_shapely(roi):\n        return Polygon(roi.subpixel_coordinates[:, ::-1])\n\n    def reprocess_and_save_filtered_spots_with_baysor_outlines(self):\n        \"\"\"Reprocess filtered spots using baysor cell outlines, then save.\n\n        Loads the 3D cell outlines from Baysor, checks all points to see what \n        (if any) cell outline that the spot falls within, and then saves the\n        data back to the datastore.\n        \"\"\"\n        from rtree import index\n        import re\n\n        rois = self.load_global_baysor_outlines()\n        filtered_spots_df = self.load_global_filtered_decoded_spots()\n\n        parsed_spots_df = filtered_spots_df[\n                [\n                    \"gene_id\",\n                    \"global_z\",\n                    \"global_y\",\n                    \"global_x\",\n                    \"cell_id\",\n                    \"tile_idx\",\n                ]\n        ].copy()\n        parsed_spots_df.rename(\n            columns={\n                \"global_x\": \"x\",\n                \"global_y\": \"y\",\n                \"global_z\": \"z\",\n                \"gene_id\" : \"gene\",\n                \"cell_id\" : \"cell\",\n            },\n            inplace=True,\n        )\n        parsed_spots_df[\"transcript_id\"] = pd.util.hash_pandas_object(\n            parsed_spots_df, index=False\n        )\n\n        parsed_spots_df[\"assignment_confidence\"] = 1.0\n\n        # Create spatial index for ROIs\n        roi_index = index.Index()\n        roi_map = {}  # Map index IDs to ROIs\n\n        for idx, roi in enumerate(rois):\n            # Ensure roi.coordinates contains the polygon points\n            coords = roi.coordinates()\n\n            # Insert the polygon bounds into the spatial index\n            polygon = Polygon(coords)\n            roi_index.insert(idx, polygon.bounds)  # Use polygon bounds for indexing\n            roi_map[idx] = roi\n\n        # Function to check a single point\n        def point_in_roi(row):\n            point = Point(row[\"x\"], row[\"y\"])\n            candidate_indices = list(roi_index.intersection(point.bounds))  # Search spatial index\n            for idx in candidate_indices:\n                roi = roi_map[idx]\n                match = re.search(r\"zstart_([-\\d.]+)_zend_([-\\d.]+)\", roi.name)\n                if match:\n                    z_start = float(match.group(1))\n                    z_end = float(match.group(2))\n                    if z_start &lt;= row[\"z\"] &lt;= z_end:\n                        polygon = Polygon(roi.coordinates())\n                        if polygon.contains(point):\n                            return str(roi.name.split(\"_\")[1]) \n            return -1\n\n        # Apply optimized spatial lookup\n        parsed_spots_df[\"cell\"] = parsed_spots_df.apply(point_in_roi, axis=1)\n        parsed_spots_df = parsed_spots_df.loc[parsed_spots_df[\"cell\"] != -1]\n\n        current_global_filtered_decoded_path = (\n            self._datastore_path \n            / Path(\"all_tiles_filtered_decoded_features\")\n            / Path(\"refined_transcripts.parquet\")\n        )\n\n        self._save_to_parquet(parsed_spots_df, current_global_filtered_decoded_path)\n\n    def save_mtx(self, spots_source: str = \"baysor\"):\n        \"\"\"Save mtx file for downstream analysis. Assumes Baysor has been run.\n\n        Parameters\n        ----------\n        spots_source: str, default \"baysor\"\n            source of spots. \"baysor\" or \"resegmented\".\n        \"\"\"\n\n        from merfish3danalysis.utils.dataio import create_mtx\n\n        if spots_source == \"baysor\":\n            spots_path = self._datastore_path / Path(\"segmentation\") / Path(\"baysor\") / Path(\"segmentation.csv\")\n        elif spots_source == \"resegmented\":\n            spots_path = (\n                self._datastore_path \n                / Path(\"all_tiles_filtered_decoded_features\")\n                / Path(\"refined_transcripts.parquet\")\n            )\n\n        mtx_output_path = self._datastore_path / Path(\"mtx_output\")\n\n        create_mtx(\n            spots_path=spots_path,\n            output_dir_path=mtx_output_path,\n        )\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.baysor_options","title":"<code>baysor_options</code>  <code>property</code> <code>writable</code>","text":"<p>Baysor options</p> <p>Returns:</p> Name Type Description <code>baysor_options</code> <code>Union[Path, str]</code> <p>Baysor options.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.baysor_path","title":"<code>baysor_path</code>  <code>property</code> <code>writable</code>","text":"<p>Baysor path</p> <p>Returns:</p> Name Type Description <code>baysor_path</code> <code>Union[Path, str]</code> <p>Baysor path.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.binning","title":"<code>binning</code>  <code>property</code> <code>writable</code>","text":"<p>Camera binning.</p> <p>Returns:</p> Name Type Description <code>binning</code> <code>int</code> <p>Camera binning.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.bit_ids","title":"<code>bit_ids</code>  <code>property</code>","text":"<p>Bit IDs.</p> <p>Returns:</p> Name Type Description <code>bit_ids</code> <code>Collection[str]</code> <p>Bit IDs.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.camera_model","title":"<code>camera_model</code>  <code>property</code> <code>writable</code>","text":"<p>Camera model.</p> <p>Returns:</p> Name Type Description <code>camera_model</code> <code>Optional[str]</code> <p>Camera model.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.channel_psfs","title":"<code>channel_psfs</code>  <code>property</code> <code>writable</code>","text":"<p>Channel point spread functions (PSF).</p> Return <p>channel_psfs : ArrayLike     Channel point spread functions (PSF).</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.channel_shading_maps","title":"<code>channel_shading_maps</code>  <code>property</code> <code>writable</code>","text":"<p>Channel shaiding images.</p> <p>Returns:</p> Name Type Description <code>channel_shading_maps</code> <code>ArrayLike</code> <p>Channel shading images.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.channels_in_data","title":"<code>channels_in_data</code>  <code>property</code> <code>writable</code>","text":"<p>Channel indices.</p> <p>Returns:</p> Name Type Description <code>channels_in_data</code> <code>Collection[int]</code> <p>Channel indices.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.codebook","title":"<code>codebook</code>  <code>property</code> <code>writable</code>","text":"<p>Codebook.</p> <p>Returns:</p> Name Type Description <code>codebook</code> <code>DataFrame</code> <p>Codebook.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.datastore_state","title":"<code>datastore_state</code>  <code>property</code> <code>writable</code>","text":"<p>Datastore state.</p> <p>Returns:</p> Name Type Description <code>datastore_state</code> <code>Optional[dict]</code> <p>Datastore state.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.e_per_ADU","title":"<code>e_per_ADU</code>  <code>property</code> <code>writable</code>","text":"<p>Electrons per camera ADU.</p> <p>Returns:</p> Name Type Description <code>e_per_ADU</code> <code>float</code> <p>Electrons per camera ADU.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.experiment_order","title":"<code>experiment_order</code>  <code>property</code> <code>writable</code>","text":"<p>Round and bit order.</p> <p>Returns:</p> Name Type Description <code>experiment_order</code> <code>DataFrame</code> <p>Round and bit order.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.global_background_vector","title":"<code>global_background_vector</code>  <code>property</code> <code>writable</code>","text":"<p>Global background vector.</p> <p>Returns:</p> Name Type Description <code>global_background_vector</code> <code>ArrayLike</code> <p>Global background vector.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.global_normalization_vector","title":"<code>global_normalization_vector</code>  <code>property</code> <code>writable</code>","text":"<p>Global normalization vector.</p> <p>Returns:</p> Name Type Description <code>global_normalization_vector</code> <code>ArrayLike</code> <p>Global normalization vector.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.iterative_background_vector","title":"<code>iterative_background_vector</code>  <code>property</code> <code>writable</code>","text":"<p>Iterative background vector.</p> <p>Returns:</p> Name Type Description <code>iterative_background_vector</code> <code>ArrayLike</code> <p>Iterative background vector.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.iterative_normalization_vector","title":"<code>iterative_normalization_vector</code>  <code>property</code> <code>writable</code>","text":"<p>Iterative normalization vector.</p> <p>Returns:</p> Name Type Description <code>iterative_normalization_vector</code> <code>ArrayLike</code> <p>Iterative normalization vector.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.julia_threads","title":"<code>julia_threads</code>  <code>property</code> <code>writable</code>","text":"<p>Julia thread number</p> <p>Returns:</p> Name Type Description <code>julia_threads</code> <code>int</code> <p>Julia thread number.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.microscope_type","title":"<code>microscope_type</code>  <code>property</code> <code>writable</code>","text":"<p>Microscope type.</p> <p>Returns:</p> Name Type Description <code>microscope_type</code> <code>Optional[str]</code> <p>Microscope type.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.na","title":"<code>na</code>  <code>property</code> <code>writable</code>","text":"<p>Detection objective numerical aperture (NA).</p> <p>Returns:</p> Name Type Description <code>na</code> <code>float</code> <p>Detection objective numerical aperture (NA).</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.noise_map","title":"<code>noise_map</code>  <code>property</code> <code>writable</code>","text":"<p>Camera noise image.</p> <p>Returns:</p> Name Type Description <code>noise_map</code> <code>ArrayLike</code> <p>Camera noise image.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.num_bits","title":"<code>num_bits</code>  <code>property</code> <code>writable</code>","text":"<p>Number of bits.</p> <p>Returns:</p> Name Type Description <code>num_bits</code> <code>int</code> <p>Number of bits.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.num_rounds","title":"<code>num_rounds</code>  <code>property</code> <code>writable</code>","text":"<p>Number of rounds.</p> <p>Returns:</p> Name Type Description <code>num_rounds</code> <code>int</code> <p>Number of rounds.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.num_tiles","title":"<code>num_tiles</code>  <code>property</code> <code>writable</code>","text":"<p>Number of tiles.</p> <p>Returns:</p> Name Type Description <code>num_tiles</code> <code>int</code> <p>Number of tiles.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.ri","title":"<code>ri</code>  <code>property</code> <code>writable</code>","text":"<p>Detection objective refractive index (RI).</p> <p>Returns:</p> Name Type Description <code>ri</code> <code>float</code> <p>Detection objective refractive index (RI).</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.round_ids","title":"<code>round_ids</code>  <code>property</code>","text":"<p>Round IDs.</p> <p>Returns:</p> Name Type Description <code>round_ids</code> <code>Collection[str]</code> <p>Round IDs.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.tile_ids","title":"<code>tile_ids</code>  <code>property</code>","text":"<p>Tile IDs.</p> <p>Returns:</p> Name Type Description <code>tile_ids</code> <code>Collection[str]</code> <p>Tile IDs.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.tile_overlap","title":"<code>tile_overlap</code>  <code>property</code> <code>writable</code>","text":"<p>XY tile overlap.</p> <p>Returns:</p> Name Type Description <code>tile_overlap</code> <code>float</code> <p>XY tile overlap.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.voxel_size_zyx_um","title":"<code>voxel_size_zyx_um</code>  <code>property</code> <code>writable</code>","text":"<p>Voxel size, zyx order (microns).</p> <p>Returns:</p> Name Type Description <code>voxel_size_zyx_um</code> <code>ArrayLike</code> <p>Voxel size, zyx order (microns).</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore._check_for_zarr_array","title":"<code>_check_for_zarr_array(kvstore, spec)</code>  <code>staticmethod</code>","text":"<p>Check if zarr array exists using Tensortore.</p> <p>Parameters:</p> Name Type Description Default <code>kvstore</code> <code>Union[Path, str]</code> <p>Datastore location.</p> required <code>spec</code> <code>dict</code> <p>Zarr specification.</p> required Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>@staticmethod\ndef _check_for_zarr_array(kvstore: Union[Path, str], spec: dict):\n    \"\"\"Check if zarr array exists using Tensortore.\n\n    Parameters\n    ----------\n    kvstore : Union[Path, str]\n        Datastore location.\n    spec : dict\n        Zarr specification.\n    \"\"\"\n\n    current_zarr = ts.open(\n        {\n            **spec,\n            \"kvstore\": kvstore,\n        }\n    ).result()\n\n    del current_zarr\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore._get_kvstore_key","title":"<code>_get_kvstore_key(path)</code>  <code>staticmethod</code>","text":"<p>Convert datastore location to tensorstore kvstore key.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[Path, str]</code> <p>Datastore location.</p> required <p>Returns:</p> Name Type Description <code>kvstore_key</code> <code>dict</code> <p>Tensorstore kvstore key.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>@staticmethod\ndef _get_kvstore_key(path: Union[Path, str]) -&gt; dict:\n    \"\"\"Convert datastore location to tensorstore kvstore key.\n\n    Parameters\n    ----------\n    path : Union[Path, str]\n        Datastore location.\n\n    Returns\n    -------\n    kvstore_key : dict\n        Tensorstore kvstore key.\n    \"\"\"\n\n    path_str = str(path)\n    if path_str.startswith(\"s3://\") or \"s3.amazonaws.com\" in path_str:\n        return {\"driver\": \"s3\", \"path\": path_str}\n    elif path_str.startswith(\"gs://\") or \"storage.googleapis.com\" in path_str:\n        return {\"driver\": \"gcs\", \"path\": path_str}\n    elif path_str.startswith(\"azure://\") or \"blob.core.windows.net\" in path_str:\n        return {\"driver\": \"azure\", \"path\": path_str}\n    elif path_str.startswith(\"http://\") or path_str.startswith(\"https://\"):\n        raise ValueError(\"Unsupported cloud storage provider in URL\")\n    else:\n        return {\"driver\": \"file\", \"path\": path_str}\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore._init_datastore","title":"<code>_init_datastore()</code>","text":"<p>Initialize datastore.</p> <p>Create directory structure and initialize datastore state.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def _init_datastore(self):\n    \"\"\"Initialize datastore.\n\n    Create directory structure and initialize datastore state.\n    \"\"\"\n\n    self._datastore_path.mkdir(parents=True)\n    self._calibrations_zarr_path = self._datastore_path / Path(r\"calibrations.zarr\")\n    self._calibrations_zarr_path.mkdir()\n    calibrations_zattrs_path = self._calibrations_zarr_path / Path(r\".zattrs\")\n    empty_zattrs = {}\n    self._save_to_json(empty_zattrs, calibrations_zattrs_path)\n    self._polyDT_root_path = self._datastore_path / Path(r\"polyDT\")\n    self._polyDT_root_path.mkdir()\n    self._readouts_root_path = self._datastore_path / Path(r\"readouts\")\n    self._readouts_root_path.mkdir()\n    self._ufish_localizations_root_path = self._datastore_path / Path(\n        r\"ufish_localizations\"\n    )\n    self._ufish_localizations_root_path.mkdir()\n    self._decoded_root_path = self._datastore_path / Path(r\"decoded\")\n    self._decoded_root_path.mkdir()\n    self._fused_root_path = self._datastore_path / Path(r\"fused\")\n    self._fused_root_path.mkdir()\n    self._segmentation_root_path = self._datastore_path / Path(r\"segmentation\")\n    self._segmentation_root_path.mkdir()\n    self._mtx_output_root_path = self._datastore_path / Path(r\"mtx_output\")\n    self._mtx_output_root_path.mkdir()\n    self._baysor_path = r\"\"\n    self._baysor_options = r\"\"\n    self._julia_threads = 0\n\n    # initialize datastore state\n    self._datastore_state_json_path = self._datastore_path / Path(\n        r\"datastore_state.json\"\n    )\n    self._datastore_state = {\n        \"Version\": 0.3,\n        \"Initialized\": True,\n        \"Calibrations\": False,\n        \"Corrected\": False,\n        \"LocalRegistered\": False,\n        \"GlobalRegistered\": False,\n        \"Fused\": False,\n        \"SegmentedCells\": False,\n        \"DecodedSpots\": False,\n        \"FilteredSpots\": False,\n        \"RefinedSpots\": False,\n        \"mtxOutput\": False,\n        \"BaysorPath\": str(self._baysor_path),\n        \"BaysorOptions\": str(self._baysor_options),\n        \"JuliaThreads\": str(self._julia_threads)\n    }\n\n    self._save_to_json(self._datastore_state, self._datastore_state_json_path)\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore._load_from_json","title":"<code>_load_from_json(dictionary_path)</code>  <code>staticmethod</code>","text":"<p>Load json as dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_path</code> <code>Union[Path, str]</code> <p>Path to json file.</p> required <p>Returns:</p> Name Type Description <code>dictionary</code> <code>dict</code> <p>Dictionary from json file.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>@staticmethod\ndef _load_from_json(dictionary_path: Union[Path, str]) -&gt; dict:\n    \"\"\"Load json as dictionary.\n\n    Parameters\n    ----------\n    dictionary_path : Union[Path, str]\n        Path to json file.\n\n    Returns\n    -------\n    dictionary : dict\n        Dictionary from json file.\n    \"\"\"\n\n    try:\n        with open(dictionary_path, \"r\") as f:\n            dictionary = json.load(f)\n    except (FileNotFoundError, json.JSONDecodeError):\n        dictionary = {}\n    return dictionary\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore._load_from_microjson","title":"<code>_load_from_microjson(dictionary_path)</code>  <code>staticmethod</code>","text":"<p>Load cell outlines outlines microjson as dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_path</code> <code>Union[Path, str]</code> <p>Path to microjson file.</p> required <p>Returns:</p> Name Type Description <code>outlines</code> <code>dict</code> <p>Cell outlines dictionary.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>@staticmethod\ndef _load_from_microjson(dictionary_path: Union[Path, str]) -&gt; dict:\n    \"\"\"Load cell outlines outlines microjson as dictionary.\n\n    Parameters\n    ----------\n    dictionary_path : Union[Path, str]\n        Path to microjson file.\n\n    Returns\n    -------\n    outlines : dict\n        Cell outlines dictionary.\n    \"\"\"\n\n    try:\n        with open(dictionary_path, \"r\") as f:\n            data = json.load(f)\n            outlines = {}\n            for feature in data[\"features\"]:\n                cell_id = feature[\"properties\"][\"cell_id\"]\n                coordinates = feature[\"geometry\"][\"coordinates\"][0]\n                outlines[cell_id] = np.array(coordinates)\n    except (FileNotFoundError, json.JSONDecodeError, KeyError, TypeError, ValueError):\n        outlines = {}\n    return outlines\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore._load_from_parquet","title":"<code>_load_from_parquet(parquet_path)</code>  <code>staticmethod</code>","text":"<p>Load dataframe from parquet.</p> <p>Parameters:</p> Name Type Description Default <code>parquet_path</code> <code>Union[Path, str]</code> <p>Path to parquet file.</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Dataframe from parquet file.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>@staticmethod\ndef _load_from_parquet(parquet_path: Union[Path, str]) -&gt; pd.DataFrame:\n    \"\"\"Load dataframe from parquet.\n\n    Parameters\n    ----------\n    parquet_path : Union[Path, str]\n        Path to parquet file.\n\n    Returns\n    -------\n    df : pd.DataFrame\n        Dataframe from parquet file.\n    \"\"\"\n\n    return pd.read_parquet(parquet_path)\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore._load_from_zarr_array","title":"<code>_load_from_zarr_array(kvstore, spec, return_future=True)</code>  <code>staticmethod</code>","text":"<p>Return tensorstore array from zarr</p> <p>Defaults to returning future result.</p> <p>Parameters:</p> Name Type Description Default <code>kvstore</code> <code>dict</code> <p>Tensorstore kvstore specification.</p> required <code>spec</code> <code>dict</code> <p>Tensorstore zarr specification.</p> required <code>return_future</code> <code>bool</code> <p>Return future (True) or immediately read (False).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>array</code> <code>ArrayLike</code> <p>Delayed (future) or immediate array.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>@staticmethod\ndef _load_from_zarr_array(\n    kvstore: dict, spec: dict, return_future=True\n) -&gt; ArrayLike:\n    \"\"\"Return tensorstore array from zarr\n\n    Defaults to returning future result.\n\n    Parameters\n    ----------\n    kvstore : dict\n        Tensorstore kvstore specification.\n    spec : dict\n        Tensorstore zarr specification.\n    return_future : bool\n        Return future (True) or immediately read (False).\n\n    Returns\n    -------\n    array : ArrayLike\n        Delayed (future) or immediate array.\n    \"\"\"\n\n    current_zarr = ts.open(\n        {\n            **spec,\n            \"kvstore\": kvstore,\n        }\n    ).result()\n\n    read_future = current_zarr.read()\n\n    if return_future:\n        return read_future\n    else:\n        return read_future.result()\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore._parse_datastore","title":"<code>_parse_datastore()</code>","text":"<p>Parse datastore to discover available components.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def _parse_datastore(self):\n    \"\"\"Parse datastore to discover available components.\"\"\"\n\n    # directory structure as defined by qi2lab spec\n    self._calibrations_zarr_path = self._datastore_path / Path(r\"calibrations.zarr\")\n    self._polyDT_root_path = self._datastore_path / Path(r\"polyDT\")\n    self._readouts_root_path = self._datastore_path / Path(r\"readouts\")\n    self._ufish_localizations_root_path = self._datastore_path / Path(\n        r\"ufish_localizations\"\n    )\n    self._decoded_root_path = self._datastore_path / Path(r\"decoded\")\n    self._fused_root_path = self._datastore_path / Path(r\"fused\")\n    self._segmentation_root_path = self._datastore_path / Path(r\"segmentation\")\n    self._mtx_output_root_path = self._datastore_path / Path(r\"mtx_output\")\n    self._datastore_state_json_path = self._datastore_path / Path(\n        r\"datastore_state.json\"\n    )\n\n    # read in .json in root directory that indicates what steps have been run\n    with open(self._datastore_state_json_path, \"r\") as json_file:\n        self._datastore_state = json.load(json_file)\n\n    # validate calibrations.zarr\n    if self._datastore_state[\"Calibrations\"]:\n        if not (self._calibrations_zarr_path.exists()):\n            print(\"Calibration data error.\")\n        try:\n            zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n            attributes = self._load_from_json(zattrs_path)\n        except (FileNotFoundError, json.JSONDecodeError):\n            print(\"Calibration attributes not found\")\n\n        keys_to_check = [\n            \"num_rounds\",\n            \"num_tiles\",\n            \"channels_in_data\",\n            \"tile_overlap\",\n            \"binning\",\n            \"e_per_ADU\",\n            \"na\",\n            \"ri\",\n            \"exp_order\",\n            \"codebook\",\n            \"num_bits\"\n        ]\n        if self._datastore_state[\"Version\"] == 0.3:\n            keys_to_check.append(\"microscope_type\")\n            keys_to_check.append(\"camera_model\")\n            keys_to_check.append(\"voxel_size_zyx_um\")\n        for key in keys_to_check:\n            if key not in attributes.keys():\n                raise KeyError(\"Calibration attributes incomplete\")\n            else:\n                setattr(self, \"_\" + key, attributes[key])\n\n        current_local_zarr_path = str(\n            self._calibrations_zarr_path / Path(\"psf_data\")\n        )\n\n        try:\n            self._psfs = (\n                self._load_from_zarr_array(\n                    kvstore=self._get_kvstore_key(current_local_zarr_path),\n                    spec=self._zarrv2_spec.copy(),\n                )\n            ).result()\n        except (IOError, OSError, ZarrError):\n            print(\"Calibration psfs missing.\")\n\n        del current_local_zarr_path\n\n        # current_local_zarr_path = str(\n        #     self._calibrations_zarr_path / Path(\"noise_map\")\n        # )\n\n        # try:\n        #     self._noise_map = (\n        #         self._load_from_zarr_array(\n        #             kvstore=self._get_kvstore_key(current_local_zarr_path),\n        #             spec=self._zarrv2_spec,\n        #         )\n        #     ).result()\n        # except Exception:\n        #     print(\"Calibration noise map missing.\")\n\n    # validate polyDT and readout bits data\n    if self._datastore_state[\"Corrected\"]:\n        if not (self._polyDT_root_path.exists()):\n            raise FileNotFoundError(\"PolyDT directory not initialized\")\n        else:\n            polyDT_tile_ids = sorted(\n                [\n                    entry.name\n                    for entry in self._polyDT_root_path.iterdir()\n                    if entry.is_dir()\n                ],\n                key=lambda x: int(x.split(\"tile\")[1].split(\".zarr\")[0]),\n            )\n            current_tile_dir_path = self._polyDT_root_path / Path(\n                polyDT_tile_ids[0]\n            )\n            self._round_ids = sorted(\n                [\n                    entry.name.split(\".\")[0]\n                    for entry in current_tile_dir_path.iterdir()\n                    if entry.is_dir()\n                ],\n                key=lambda x: int(x.split(\"round\")[1].split(\".zarr\")[0]),\n            )\n        if not (self._readouts_root_path.exists()):\n            raise FileNotFoundError(\"Readout directory not initialized\")\n        else:\n            readout_tile_ids = sorted(\n                [\n                    entry.name\n                    for entry in self._readouts_root_path.iterdir()\n                    if entry.is_dir()\n                ],\n                key=lambda x: int(x.split(\"tile\")[1].split(\".zarr\")[0]),\n            )\n            current_tile_dir_path = self._readouts_root_path / Path(\n                readout_tile_ids[0]\n            )\n            self._bit_ids = sorted(\n                [\n                    entry.name.split(\".\")[0]\n                    for entry in current_tile_dir_path.iterdir()\n                    if entry.is_dir()\n                ],\n                key=lambda x: int(x.split(\"bit\")[1].split(\".zarr\")[0]),\n            )\n        assert (\n            polyDT_tile_ids == readout_tile_ids\n        ), \"polyDT and readout tile ids do not match. Conversion error.\"\n        self._tile_ids = polyDT_tile_ids.copy()\n        del polyDT_tile_ids, readout_tile_ids\n\n        for tile_id, round_id in product(self._tile_ids, self._round_ids):\n            try:\n                zattrs_path = str(\n                    self._polyDT_root_path\n                    / Path(tile_id)\n                    / Path(round_id + \".zarr\")\n                    / Path(\".zattrs\")\n                )\n                attributes = self._load_from_json(zattrs_path)\n            except (FileNotFoundError, json.JSONDecodeError):\n                print(\"polyDT tile attributes not found\")\n\n            keys_to_check = [\n                \"stage_zyx_um\",\n                \"excitation_um\",\n                \"emission_um\",\n                \"bit_linker\",\n                # \"exposure_ms\",\n                \"psf_idx\",\n            ]\n\n            for key in keys_to_check:\n                if key not in attributes.keys():\n                    print(tile_id, round_id, key)\n                    raise KeyError(\"Corrected polyDT attributes incomplete\")\n\n            current_local_zarr_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(round_id + \".zarr\")\n                / Path(\"corrected_data\")\n            )\n\n            try:\n                self._check_for_zarr_array(\n                    self._get_kvstore_key(current_local_zarr_path),\n                    self._zarrv2_spec.copy(),\n                )\n            except (IOError, OSError, ZarrError):\n                print(tile_id, round_id)\n                print(\"Corrected polyDT data missing.\")\n\n        for tile_id, bit_id in product(self._tile_ids, self._bit_ids):\n            try:\n                zattrs_path = str(\n                    self._readouts_root_path\n                    / Path(tile_id)\n                    / Path(bit_id + \".zarr\")\n                    / Path(\".zattrs\")\n                )\n                attributes = self._load_from_json(zattrs_path)\n            except (FileNotFoundError, json.JSONDecodeError):\n                print(\"Readout tile attributes not found\")\n\n            keys_to_check = [\n                \"excitation_um\",\n                \"emission_um\",\n                \"round_linker\",\n                # \"exposure_ms\",\n                \"psf_idx\",\n            ]\n            for key in keys_to_check:\n                if key not in attributes.keys():\n                    raise KeyError(\"Corrected readout attributes incomplete\")\n\n            current_local_zarr_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(bit_id + \".zarr\")\n                / Path(\"corrected_data\")\n            )\n\n            try:\n                self._check_for_zarr_array(\n                    self._get_kvstore_key(current_local_zarr_path),\n                    self._zarrv2_spec.copy(),\n                )\n            except (IOError, OSError, ZarrError):\n                print(tile_id, bit_id)\n                print(\"Corrected readout data missing.\")\n\n    # check and validate local registered data\n    if self._datastore_state[\"LocalRegistered\"]:\n        for tile_id, round_id in product(self._tile_ids, self._round_ids):\n            if round_id is not self._round_ids[0]:\n                try:\n                    zattrs_path = str(\n                        self._polyDT_root_path\n                        / Path(tile_id)\n                        / Path(round_id + \".zarr\")\n                        / Path(\".zattrs\")\n                    )\n                    with open(zattrs_path, \"r\") as f:\n                        attributes = json.load(f)\n                except (FileNotFoundError, json.JSONDecodeError):\n                    print(\"polyDT tile attributes not found\")\n\n                keys_to_check = [\"rigid_xform_xyz_px\"]\n\n                for key in keys_to_check:\n                    if key not in attributes.keys():\n                        raise KeyError(f\"{round_id,tile_id} Rigid registration missing\")\n\n                current_local_zarr_path = str(\n                    self._polyDT_root_path\n                    / Path(tile_id)\n                    / Path(round_id + \".zarr\")\n                    / Path(\"opticalflow_xform_px\")\n                )\n\n                try:\n                    self._check_for_zarr_array(\n                        self._get_kvstore_key(current_local_zarr_path),\n                        self._zarrv2_spec.copy(),\n                    )\n                except (IOError, OSError, ZarrError):\n                    #print(tile_id, round_id)\n                    #print(\"Optical flow registration data missing.\")\n                    pass\n\n            current_local_zarr_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(round_id + \".zarr\")\n                / Path(\"registered_decon_data\")\n            )\n            if round_id is self._round_ids[0]:\n                try:\n                    self._check_for_zarr_array(\n                        self._get_kvstore_key(current_local_zarr_path),\n                        self._zarrv2_spec.copy(),\n                    )\n                except (IOError, OSError, ZarrError):\n                    print(tile_id, round_id)\n                    print(\"Registered polyDT data missing.\")\n\n        for tile_id, bit_id in product(self._tile_ids, self._bit_ids):\n            current_local_zarr_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(bit_id + \".zarr\")\n                / Path(\"registered_decon_data\")\n            )\n\n            try:\n                self._check_for_zarr_array(\n                    self._get_kvstore_key(current_local_zarr_path),\n                    self._zarrv2_spec.copy(),\n                )\n            except (IOError, OSError, ZarrError):\n                print(tile_id, round_id)\n                print(\"Registered readout data missing.\")\n\n            current_local_zarr_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(bit_id + \".zarr\")\n                / Path(\"registered_ufish_data\")\n            )\n\n            try:\n                self._check_for_zarr_array(\n                    self._get_kvstore_key(current_local_zarr_path),\n                    self._zarrv2_spec.copy(),\n                )\n            except (IOError, OSError, ZarrError):\n                print(tile_id, round_id)\n                print(\"Registered ufish prediction missing.\")\n\n        for tile_id, bit_id in product(self._tile_ids, self._bit_ids):\n            current_ufish_path = (\n                self._ufish_localizations_root_path\n                / Path(tile_id)\n                / Path(bit_id + \".parquet\")\n            )\n            if not (current_ufish_path.exists()):\n                raise FileNotFoundError(\n                    tile_id + \" \" + bit_id + \" ufish localization missing\"\n                )\n\n    # check and validate global registered data\n    if self._datastore_state[\"GlobalRegistered\"]:\n        for tile_id in self._tile_ids:\n            try:\n                zattrs_path = str(\n                    self._polyDT_root_path\n                    / Path(tile_id)\n                    / Path(self._round_ids[0] + \".zarr\")\n                    / Path(\".zattrs\")\n                )\n                with open(zattrs_path, \"r\") as f:\n                    attributes = json.load(f)\n            except (FileNotFoundError, json.JSONDecodeError):\n                print(\"polyDT tile attributes not found\")\n\n            keys_to_check = [\"affine_zyx_um\", \"origin_zyx_um\", \"spacing_zyx_um\"]\n\n            for key in keys_to_check:\n                if key not in attributes.keys():\n                    raise KeyError(\"Global registration missing\")\n\n    # check and validate fused\n    if self._datastore_state[\"Fused\"]:\n        try:\n            zattrs_path = str(\n                self._fused_root_path\n                / Path(\"fused.zarr\")\n                / Path(\"fused_polyDT_iso_zyx\")\n                / Path(\".zattrs\")\n            )\n            with open(zattrs_path, \"r\") as f:\n                attributes = json.load(f)\n        except (FileNotFoundError, json.JSONDecodeError):\n            print(\"Fused image attributes not found\")\n\n        keys_to_check = [\"affine_zyx_um\", \"origin_zyx_um\", \"spacing_zyx_um\"]\n\n        for key in keys_to_check:\n            if key not in attributes.keys():\n                raise KeyError(\"Fused image metadata missing\")\n\n        current_local_zarr_path = str(\n            self._fused_root_path\n            / Path(\"fused.zarr\")\n            / Path(\"fused_polyDT_iso_zyx\")\n        )\n\n        try:\n            self._check_for_zarr_array(\n                self._get_kvstore_key(current_local_zarr_path),\n                self._zarrv2_spec.copy(),\n            )\n        except (IOError, OSError, ZarrError):\n            print(\"Fused data missing.\")\n\n    # check and validate cellpose segmentation\n    if self._datastore_state[\"SegmentedCells\"]:\n        current_local_zarr_path = str(\n            self._segmentation_root_path\n            / Path(\"cellpose\")\n            / Path(\"cellpose.zarr\")\n            / Path(\"masks_polyDT_iso_zyx\")\n        )\n\n        try:\n            self._check_for_zarr_array(\n                self._get_kvstore_key(current_local_zarr_path),\n                self._zarrv2_spec.copy(),\n            )\n        except (IOError, OSError, ZarrError):\n            print(\"Cellpose data missing.\")\n\n        cell_outlines_path = (\n            self._segmentation_root_path\n            / Path(\"cellpose\")\n            / Path(\"imagej_rois\")\n            / Path(\"global_coords_rois.zip\")\n        )\n        if not (cell_outlines_path.exists()):\n            raise FileNotFoundError(\"Cellpose cell outlines missing.\")\n\n    # check and validate decoded spots\n    if self._datastore_state[\"DecodedSpots\"]:\n        for tile_id in self._tile_ids:\n            decoded_path = self._decoded_root_path / Path(\n                tile_id + \"_decoded_features.parquet\"\n            )\n\n            if not (decoded_path.exists()):\n                raise FileNotFoundError(tile_id + \" decoded spots missing.\")\n\n    # check and validate filtered decoded spots\n    if self._datastore_state[\"FilteredSpots\"]:\n        filtered_path = self._decoded_root_path / Path(\n            \"all_tiles_filtered_decoded_features.parquet\"\n        )\n\n        if not (filtered_path.exists()):\n            raise FileNotFoundError(\"filtered decoded spots missing.\")\n\n    if self._datastore_state[\"RefinedSpots\"]:\n        baysor_spots_path = (\n            self._segmentation_root_path\n            / Path(\"baysor\")\n            / Path(\"segmentation.csv\")\n        )\n\n        if not (baysor_spots_path.exists()):\n            raise FileNotFoundError(\"Baysor filtered decoded spots missing.\")\n\n    # check and validate mtx\n    if self._datastore_state[\"mtxOutput\"]:\n        mtx_barcodes_path = self._mtx_output_root_path / Path(\"barcodes.tsv.gz\")\n        mtx_features_path = self._mtx_output_root_path / Path(\"features.tsv.gz\")\n        mtx_matrix_path = self._mtx_output_root_path / Path(\"matrix.tsv.gz\")\n\n        if (\n            not (mtx_barcodes_path.exists())\n            or not (mtx_features_path.exists())\n            or not (mtx_matrix_path.exists())\n        ):\n            raise FileNotFoundError(\"mtx output missing.\")\n\n    try:\n        self._baysor_path = Path(str(self._datastore_state[\"BaysorPath\"]))\n        self._baysor_options = Path(str(self._datastore_state[\"BaysorOptions\"]))\n        self._julia_threads = int(self._datastore_state[\"JuliaThreads\"])\n    except KeyError:\n        self._baysor_path = r\"\"\n        self._baysor_options = r\"\"\n        self._julia_threads = 1\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore._save_to_json","title":"<code>_save_to_json(dictionary, dictionary_path)</code>  <code>staticmethod</code>","text":"<p>Save dictionary to json.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary</code> <code>dict</code> <p>The data to be saved.</p> required <code>dictionary_path</code> <code>Union[Path, str]</code> <p>The path to the JSON file where the data will be saved.</p> required Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>@staticmethod\ndef _save_to_json(dictionary: dict, dictionary_path: Union[Path, str]):\n    \"\"\"Save dictionary to json.\n\n    Parameters\n    ----------\n    dictionary : dict\n        The data to be saved.\n    dictionary_path : Union[Path,str]\n        The path to the JSON file where the data will be saved.\n    \"\"\"\n\n    with open(dictionary_path, \"w\") as file:\n        json.dump(dictionary, file, indent=4)\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore._save_to_parquet","title":"<code>_save_to_parquet(df, parquet_path)</code>  <code>staticmethod</code>","text":"<p>Save dataframe to parquet.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe to save.</p> required <code>parquet_path</code> <code>Union[Path, str]</code> <p>Path to parquet file.</p> required Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>@staticmethod\ndef _save_to_parquet(df: pd.DataFrame, parquet_path: Union[Path, str]):\n    \"\"\"Save dataframe to parquet.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Dataframe to save.\n    parquet_path : Union[Path, str]\n        Path to parquet file.\n    \"\"\"\n\n    # df.to_parquet(\n    #     parquet_path,\n    #     engine=\"pyarrow\",\n    #     version=\"1.0\",\n    #     write_statistics=False\n    # )\n\n    df.to_parquet(\n        parquet_path,\n        engine=\"fastparquet\", \n        index=False\n    )\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore._save_to_zarr_array","title":"<code>_save_to_zarr_array(array, kvstore, spec, return_future=False)</code>  <code>staticmethod</code>","text":"<p>Save array to zarr using tensorstore.</p> <p>Defaults to returning future result.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ArrayLike</code> <p>Array to save.</p> required <code>kvstore</code> <code>dict</code> <p>Tensorstore kvstore specification.</p> required <code>spec</code> <code>dict</code> <p>Tensorstore zarr specification.</p> required <code>return_future</code> <code>Optional[bool]</code> <p>Return future (True) or immediately write (False).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>write_future</code> <code>Optional[ArrayLike]</code> <p>Delayed (future) if return_future is True.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>@staticmethod\ndef _save_to_zarr_array(\n    array: ArrayLike,\n    kvstore: dict,\n    spec: dict,\n    return_future: Optional[bool] = False,\n) -&gt; Optional[ArrayLike]:\n    \"\"\"Save array to zarr using tensorstore.\n\n    Defaults to returning future result.\n\n    Parameters\n    ----------\n    array : ArrayLike\n        Array to save.\n    kvstore : dict\n        Tensorstore kvstore specification.\n    spec : dict\n        Tensorstore zarr specification.\n    return_future : Optional[bool]\n        Return future (True) or immediately write (False).\n\n    Returns\n    -------\n    write_future : Optional[ArrayLike]\n        Delayed (future) if return_future is True.\n    \"\"\"\n\n    # check datatype\n    if str(array.dtype) == \"uint8\":\n        array_dtype = \"&lt;u1\"\n    elif str(array.dtype) == \"uint16\":\n        array_dtype = \"&lt;u2\"\n    elif str(array.dtype) == \"float16\":\n        array_dtype = \"&lt;f2\"\n    elif str(array.dtype) == \"float32\":\n        array_dtype = \"&lt;f4\"\n    else:\n        print(\"Unsupported data type: \" + str(array.dtype))\n        return None\n\n    # check array dimension\n    spec[\"metadata\"][\"shape\"] = array.shape\n    if len(array.shape) == 2:\n        spec[\"metadata\"][\"chunks\"] = [array.shape[0], array.shape[1]]\n    elif len(array.shape) == 3:\n        spec[\"metadata\"][\"chunks\"] = [1, array.shape[1], array.shape[2]]\n    elif len(array.shape) == 4:\n        spec[\"metadata\"][\"chunks\"] = [1, 1, array.shape[1], array.shape[2]]\n    spec[\"metadata\"][\"dtype\"] = array_dtype\n\n    try:\n        current_zarr = ts.open(\n            {\n                **spec,\n                \"kvstore\": kvstore,\n            }\n        ).result()\n\n        write_future = current_zarr.write(array)\n\n        if return_future:\n            return write_future\n        else:\n            write_future.result()\n            return None\n    except (IOError, OSError, TimeoutError):\n        print(\"Error writing zarr array.\")\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.initialize_tile","title":"<code>initialize_tile(tile)</code>","text":"<p>Initialize directory structure for a tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def initialize_tile(\n    self,\n    tile: Union[int, str],\n):\n    \"\"\"Initialize directory structure for a tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n    \"\"\"\n\n    if getattr(self, \"_experiment_order\", None) is None:\n        print(\"Assign experimental order before creating tiles.\")\n        return None\n\n    if getattr(self, \"_num_tiles\", None) is None:\n        print(\"Assign number of tiles before creating tiles.\")\n        return None\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tile id.\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    try:\n        polyDT_tile_path = self._polyDT_root_path / Path(tile_id)\n        polyDT_tile_path.mkdir()\n        for round_idx, round_id in enumerate(self._round_ids):\n            polyDT_round_path = polyDT_tile_path / Path(round_id + \".zarr\")\n            polyDT_round_path.mkdir()\n            polydt_round_attrs_path = polyDT_round_path / Path(\".zattrs\")\n            round_attrs = {\n                \"bit_linker\": self._experiment_order.to_numpy()[round_idx, 1:]\n                .astype(int)\n                .tolist(),\n            }\n            self._save_to_json(round_attrs, polydt_round_attrs_path)\n    except FileExistsError:\n        print(\"Error creating polyDT tile. Does it exist already?\")\n\n    try:\n        readout_tile_path = self._readouts_root_path / Path(tile_id)\n        readout_tile_path.mkdir()\n        for bit_idx, bit_id in enumerate(self._bit_ids):\n            readout_bit_path = readout_tile_path / Path(bit_id + \".zarr\")\n            readout_bit_path.mkdir()\n            readout_bit_attrs_path = readout_bit_path / Path(\".zattrs\")\n            fiducial_channel = str(self._channels_in_data[0])\n            readout_one_channel = str(self._channels_in_data[1])\n\n            if len(self._channels_in_data) == 3:\n                readout_two_channel = str(self._channels_in_data[2])\n                condition_one = self._experiment_order[readout_one_channel] == (\n                    bit_idx + 1\n                )\n                condition_two = self._experiment_order[readout_two_channel] == (\n                    bit_idx + 1\n                )\n                combined_condition = condition_one | condition_two\n\n            else:\n                combined_condition = self._experiment_order[\n                    readout_one_channel\n                ] == (bit_idx + 1)\n            matching_rows = self._experiment_order.loc[combined_condition]\n\n            bit_attrs = {\n                \"round_linker\": int(matching_rows[fiducial_channel].values[0])\n            }\n            self._save_to_json(bit_attrs, readout_bit_attrs_path)\n    except FileExistsError:\n        print(\"Error creating readout tile. Does it exist already?\")\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_codebook_parsed","title":"<code>load_codebook_parsed()</code>","text":"<p>Load and split codebook into gene_ids and codebook matrix.</p> <p>Returns:</p> Name Type Description <code>gene_ids</code> <code>Collection[str]</code> <p>Gene IDs.</p> <code>codebook_matrix</code> <code>ArrayLike</code> <p>Codebook matrix.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_codebook_parsed(\n    self,\n) -&gt; Optional[tuple[Collection[str], ArrayLike]]:\n    \"\"\"Load and split codebook into gene_ids and codebook matrix.\n\n    Returns\n    -------\n    gene_ids : Collection[str]\n        Gene IDs.\n    codebook_matrix : ArrayLike\n        Codebook matrix.\n    \"\"\"\n\n    try:\n        data = getattr(self, \"_codebook\", None)\n\n        if data is None:\n            return None\n        num_columns = len(data[0]) if data else 0\n        columns = [\"gene_id\"] + [f\"bit{i:02d}\" for i in range(1, num_columns)]\n        codebook_df = pd.DataFrame(data, columns=columns)\n\n        gene_ids = codebook_df.iloc[:, 0].tolist()\n        codebook_matrix = codebook_df.iloc[:, 1:].to_numpy().astype(int)\n        del data, codebook_df\n        return gene_ids, codebook_matrix\n    except (KeyError, ValueError, TypeError):\n        print(\"Error parsing codebook.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_coord_of_xform_px","title":"<code>load_coord_of_xform_px(tile, round, return_future=True)</code>","text":"<p>Local fidicual optical flow matrix for one round and tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Optional[Union[int, str]]</code> <p>Tile index or tile id.</p> required <code>round</code> <code>Optional[Union[int, str]]</code> <p>Round index or round id.</p> required <code>return_future</code> <code>Optional[bool]</code> <p>Return future array.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>of_xform_px</code> <code>Optional[ArrayLike]</code> <p>Local fidicual optical flow matrix for one round and tile.</p> <code>downsampling</code> <code>Optional[ArrayLike]</code> <p>Downsampling factor.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_coord_of_xform_px(\n    self,\n    tile: Optional[Union[int, str]],\n    round: Optional[Union[int, str]],\n    return_future: Optional[bool] = True,\n) -&gt; Optional[tuple[ArrayLike, ArrayLike]]:\n    \"\"\"Local fidicual optical flow matrix for one round and tile.\n\n    Parameters\n    ----------\n    tile : Optional[Union[int, str]]\n        Tile index or tile id.\n    round : Optional[Union[int, str]]\n        Round index or round id.\n    return_future : Optional[bool]\n        Return future array.\n\n    Returns\n    -------\n    of_xform_px : Optional[ArrayLike]\n        Local fidicual optical flow matrix for one round and tile.\n    downsampling : Optional[ArrayLike]\n        Downsampling factor.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(round, int):\n        if round &lt; 0:\n            print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n            return None\n        else:\n            round_id = self._round_ids[round]\n    elif isinstance(round, str):\n        if round not in self._round_ids:\n            print(\"Set valid round id\")\n            return None\n        else:\n            round_id = round\n    else:\n        print(\"'round' must be integer index or string identifier\")\n        return None\n\n    current_local_zarr_path = str(\n        self._polyDT_root_path\n        / Path(tile_id)\n        / Path(round_id + \".zarr\")\n        / Path(\"opticalflow_xform_px\")\n    )\n    zattrs_path = str(\n        self._polyDT_root_path\n        / Path(tile_id)\n        / Path(round_id + \".zarr\")\n        / Path(\".zattrs\")\n    )\n\n    if not Path(current_local_zarr_path).exists():\n        print(\"Optical flow transform mapping back to first round not found.\")\n        return None\n\n    try:\n        compressor = {\n            \"id\": \"blosc\",\n            \"cname\": \"zstd\",\n            \"clevel\": 5,\n            \"shuffle\": 2,\n        }\n        spec_of = {\n            \"driver\": \"zarr\",\n            \"kvstore\": None,\n            \"metadata\": {\"compressor\": compressor},\n            \"open\": True,\n            \"assume_metadata\": False,\n            \"create\": True,\n            \"delete_existing\": False,\n        }\n        spec_of[\"metadata\"][\"dtype\"] = \"&lt;f4\"\n        of_xform_px = self._load_from_zarr_array(\n            self._get_kvstore_key(current_local_zarr_path),\n            spec_of.copy(),\n            return_future,\n        )\n        attributes = self._load_from_json(zattrs_path)\n        block_size = np.asarray(\n            attributes[\"block_size\"], dtype=np.float32\n        )\n        block_stride = np.asarray(\n            attributes[\"block_stride\"], dtype=np.float32\n        )\n\n        return (of_xform_px, block_size, block_stride)\n    except (IOError, OSError, ZarrError) as e:\n        print(e)\n        print(\"Error loading optical flow transform.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_global_baysor_filtered_spots","title":"<code>load_global_baysor_filtered_spots()</code>","text":"<p>Load Baysor re-assigned decoded RNA.</p> <p>Assumes Baysor has been run.</p> <p>Returns:</p> Name Type Description <code>baysor_filtered_genes</code> <code>Optional[DataFrame]</code> <p>Baysor re-assigned decoded RNA.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_global_baysor_filtered_spots(\n    self,\n) -&gt; Optional[pd.DataFrame]:\n    \"\"\"Load Baysor re-assigned decoded RNA.\n\n    Assumes Baysor has been run.\n\n    Returns\n    -------\n    baysor_filtered_genes : Optional[pd.DataFrame]\n        Baysor re-assigned decoded RNA.\n    \"\"\"\n\n    current_baysor_spots_path = (\n        self._segmentation_root_path\n        / Path(\"baysor\")\n        / Path(\"segmentation.csv\")\n    )\n\n    if not current_baysor_spots_path.exists():\n        print(\"Baysor filtered genes not found.\")\n        return None\n    else:\n        baysor_filtered_genes = self._load_from_csv(current_baysor_spots_path)\n        return baysor_filtered_genes\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_global_baysor_outlines","title":"<code>load_global_baysor_outlines()</code>","text":"<p>Load Baysor cell outlines.</p> <p>Assumes Baysor has been run.</p> <p>Returns:</p> Name Type Description <code>baysor_outlines</code> <code>Optional[dict]</code> <p>Baysor cell outlines.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_global_baysor_outlines(\n    self,\n) -&gt; Optional[dict]:\n    \"\"\"Load Baysor cell outlines.\n\n    Assumes Baysor has been run.\n\n    Returns\n    -------\n    baysor_outlines : Optional[dict]\n        Baysor cell outlines.\n    \"\"\"\n\n    current_baysor_outlines_path = (\n        self._segmentation_root_path \n        / Path(\"baysor\") \n        / Path(r\"3d_cell_rois.zip\")\n    )\n\n    if not current_baysor_outlines_path.exists():\n        print(\"Baysor outlines not found.\")\n        return None\n    else:\n        baysor_rois = roiread(current_baysor_outlines_path)\n        return baysor_rois\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_global_cellpose_outlines","title":"<code>load_global_cellpose_outlines()</code>","text":"<p>Load Cellpose max projection cell outlines.</p> <p>Returns:</p> Name Type Description <code>cellpose_outlines</code> <code>Optional[dict]</code> <p>Cellpose cell mask outlines.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_global_cellpose_outlines(\n    self,\n) -&gt; Optional[dict]:\n    \"\"\"Load Cellpose max projection cell outlines.\n\n    Returns\n    -------\n    cellpose_outlines : Optional[dict]\n        Cellpose cell mask outlines.\n    \"\"\"\n\n    current_cellpose_outlines_path = (\n        self._segmentation_root_path / Path(\"cellpose\") / Path(\"cell_outlines.json\")\n    )\n\n    if not current_cellpose_outlines_path.exists():\n        print(\"Cellpose cell mask outlines not found.\")\n        return None\n    else:\n        cellpose_outlines = self._load_from_microjson(\n            current_cellpose_outlines_path\n        )\n        return cellpose_outlines\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_global_cellpose_segmentation_image","title":"<code>load_global_cellpose_segmentation_image(return_future=True)</code>","text":"<p>Load Cellpose max projection, downsampled segmentation image.</p> <p>Parameters:</p> Name Type Description Default <code>return_future</code> <code>Optional[bool]</code> <p>Return future array.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>fused_image</code> <code>Optional[ArrayLike]</code> <p>Cellpose max projection, downsampled segmentation image.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_global_cellpose_segmentation_image(\n    self,\n    return_future: Optional[bool] = True,\n) -&gt; Optional[ArrayLike]:\n    \"\"\"Load Cellpose max projection, downsampled segmentation image.\n\n    Parameters\n    ----------\n    return_future : Optional[bool]\n        Return future array.\n\n    Returns\n    -------\n    fused_image : Optional[ArrayLike]\n        Cellpose max projection, downsampled segmentation image.\n    \"\"\"\n\n    current_local_zarr_path = str(\n        self._segmentation_root_path\n        / Path(\"cellpose\")\n        / Path(\"cellpose.zarr\")\n        / Path(\"masks_polyDT_iso_zyx\")\n    )\n\n    if not current_local_zarr_path.exists():\n        print(\"Cellpose prediction on global fused image not found.\")\n        return None\n\n    try:\n        fused_image = self._load_from_zarr_array(\n            self._get_kvstore_key(current_local_zarr_path),\n            self._zarrv2_spec.copy(),\n            return_future,\n        )\n        return fused_image\n    except (IOError, OSError, ZarrError):\n        print(\"Error loading Cellpose image.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_global_coord_xforms_um","title":"<code>load_global_coord_xforms_um(tile)</code>","text":"<p>Load global registration transform for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <p>Returns:</p> Name Type Description <code>affine_zyx_um</code> <code>Optional[ArrayLike]</code> <p>Global affine registration transform for one tile.</p> <code>origin_zyx_um</code> <code>Optional[ArrayLike]</code> <p>Global origin registration transform for one tile.</p> <code>spacing_zyx_um</code> <code>Optional[ArrayLike]</code> <p>Global spacing registration transform for one tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_global_coord_xforms_um(\n    self,\n    tile: Union[int, str],\n) -&gt; Optional[tuple[ArrayLike, ArrayLike, ArrayLike]]:\n    \"\"\"Load global registration transform for one tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n\n    Returns\n    -------\n    affine_zyx_um : Optional[ArrayLike]\n        Global affine registration transform for one tile.\n    origin_zyx_um : Optional[ArrayLike]\n        Global origin registration transform for one tile.\n    spacing_zyx_um : Optional[ArrayLike]\n        Global spacing registration transform for one tile.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None, None, None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None, None, None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    try:\n        zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(self._round_ids[0] + \".zarr\")\n            / Path(\".zattrs\")\n        )\n        attributes = self._load_from_json(zattrs_path)\n        affine_zyx_um = np.asarray(attributes[\"affine_zyx_um\"], dtype=np.float32)\n        origin_zyx_um = np.asarray(attributes[\"origin_zyx_um\"], dtype=np.float32)\n        spacing_zyx_um = np.asarray(attributes[\"spacing_zyx_um\"], dtype=np.float32)\n        return (affine_zyx_um, origin_zyx_um, spacing_zyx_um)\n    except (FileNotFoundError, json.JSONDecodeError):\n        print(tile_id, self._round_ids[0])\n        print(\"Global coordinate transforms not found\")\n        return None, None, None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_global_fidicual_image","title":"<code>load_global_fidicual_image(return_future=True)</code>","text":"<p>Load downsampled, fused fidicual image.</p> <p>Parameters:</p> Name Type Description Default <code>return_future</code> <code>Optional[bool]</code> <p>Return future array.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>fused_image</code> <code>Optional[ArrayLike]</code> <p>Downsampled, fused fidicual image.</p> <code>affine_zyx_um</code> <code>Optional[ArrayLike]</code> <p>Global affine registration transform for fused image.</p> <code>origin_zyx_um</code> <code>Optional[ArrayLike]</code> <p>Global origin registration transform for fused image.</p> <code>spacing_zyx_um</code> <code>Optional[ArrayLike]</code> <p>Global spacing registration transform for fused image.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_global_fidicual_image(\n    self,\n    return_future: Optional[bool] = True,\n) -&gt; Optional[tuple[ArrayLike, ArrayLike, ArrayLike, ArrayLike]]:\n    \"\"\"Load downsampled, fused fidicual image.\n\n    Parameters\n    ----------\n    return_future : Optional[bool]\n        Return future array.\n\n    Returns\n    -------\n    fused_image : Optional[ArrayLike]\n        Downsampled, fused fidicual image.\n    affine_zyx_um : Optional[ArrayLike]\n        Global affine registration transform for fused image.\n    origin_zyx_um : Optional[ArrayLike]\n        Global origin registration transform for fused image.\n    spacing_zyx_um : Optional[ArrayLike]\n        Global spacing registration transform for fused image.\n    \"\"\"\n\n    current_local_zarr_path = str(\n        self._fused_root_path / Path(\"fused.zarr\") / Path(\"fused_polyDT_iso_zyx\")\n    )\n\n    if not Path(current_local_zarr_path).exists():\n        print(\"Globally registered, fused image not found.\")\n        return None\n\n    zattrs_path = str(current_local_zarr_path / Path(\".zattrs\"))\n\n    try:\n        fused_image = self._load_from_zarr_array(\n            self._get_kvstore_key(current_local_zarr_path),\n            self._zarrv2_spec.copy(),\n            return_future,\n        )\n        attributes = self._load_from_json(zattrs_path)\n        affine_zyx_um = np.asarray(attributes[\"affine_zyx_um\"], dtype=np.float32)\n        origin_zyx_um = np.asarray(attributes[\"origin_zyx_um\"], dtype=np.float32)\n        spacing_zyx_um = np.asarray(attributes[\"spacing_zyx_um\"], dtype=np.float32)\n        return fused_image, affine_zyx_um, origin_zyx_um, spacing_zyx_um\n    except (IOError, OSError, ZarrError):\n        print(\"Error loading globally registered, fused image.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_global_filtered_decoded_spots","title":"<code>load_global_filtered_decoded_spots()</code>","text":"<p>Load all decoded and filtered spots.</p> <p>Returns:</p> Name Type Description <code>all_tiles_filtered</code> <code>Optional[DataFrame]</code> <p>All decoded and filtered spots.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_global_filtered_decoded_spots(\n    self,\n) -&gt; Optional[pd.DataFrame]:\n    \"\"\"Load all decoded and filtered spots.\n\n    Returns\n    -------\n    all_tiles_filtered : Optional[pd.DataFrame]\n        All decoded and filtered spots.\n    \"\"\"\n\n    current_global_filtered_decoded_dir_path = self._datastore_path / Path(\n        \"all_tiles_filtered_decoded_features\"\n    )\n    current_global_filtered_decoded_path = (\n        current_global_filtered_decoded_dir_path / Path(\"decoded_features.parquet\")\n    )\n\n    if not current_global_filtered_decoded_path.exists():\n        print(\"Global, filtered, decoded spots not found.\")\n        return None\n    else:\n        all_tiles_filtered = self._load_from_parquet(\n            current_global_filtered_decoded_path\n        )\n        return all_tiles_filtered\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_local_bit_linker","title":"<code>load_local_bit_linker(tile, round)</code>","text":"<p>Load readout bits linked to fidicual round for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>round</code> <code>Union[int, str]</code> <p>Round index or round id.</p> required <p>Returns:</p> Name Type Description <code>bit_linker</code> <code>Optional[Sequence[int]]</code> <p>Readout bits linked to fidicual round for one tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_local_bit_linker(\n    self,\n    tile: Union[int, str],\n    round: Union[int, str],\n) -&gt; Optional[Sequence[int]]:\n    \"\"\"Load readout bits linked to fidicual round for one tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n    round : Union[int, str]\n        Round index or round id.\n\n    Returns\n    -------\n    bit_linker : Optional[Sequence[int]]\n        Readout bits linked to fidicual round for one tile.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id.\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(round, int):\n        if round &lt; 0:\n            print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n            return None\n        else:\n            round_id = self._round_ids[round]\n    elif isinstance(round, str):\n        if round not in self._round_ids:\n            print(\"Set valid round id.\")\n            return None\n        else:\n            round_id = round\n    else:\n        print(\"'round' must be integer index or string identifier\")\n        return None\n\n    try:\n        zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(round_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n        attributes = self._load_from_json(zattrs_path)\n        return attributes[\"bits\"][1:]\n    except (FileNotFoundError, json.JSONDecodeError):\n        print(tile_id, round_id)\n        print(\"Bit linker attribute not found.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_local_corrected_image","title":"<code>load_local_corrected_image(tile, round=None, bit=None, return_future=True)</code>","text":"<p>Load gain and offset corrected image for fiducial OR readout bit for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>round</code> <code>Optional[Union[int, str]]</code> <p>Round index or round id.</p> <code>None</code> <code>bit</code> <code>Optional[Union[int, str]]</code> <p>Bit index or bit id.</p> <code>None</code> <code>return_future</code> <code>Optional[bool]</code> <p>Return future array.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>corrected_image</code> <code>Optional[ArrayLike]</code> <p>Gain and offset corrected image for fiducial OR readout bit for one tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_local_corrected_image(\n    self,\n    tile: Union[int, str],\n    round: Optional[Union[int, str]] = None,\n    bit: Optional[Union[int, str]] = None,\n    return_future: Optional[bool] = True,\n) -&gt; Optional[ArrayLike]:\n    \"\"\"Load gain and offset corrected image for fiducial OR readout bit for one tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n    round : Optional[Union[int, str]]\n        Round index or round id.\n    bit : Optional[Union[int, str]]\n        Bit index or bit id.\n    return_future : Optional[bool]\n        Return future array.\n\n    Returns\n    -------\n    corrected_image : Optional[ArrayLike]\n        Gain and offset corrected image for fiducial OR readout bit for one tile.\n    \"\"\"\n\n    if (round is None and bit is None) or (round is not None and bit is not None):\n        print(\"Provide either 'round' or 'bit', but not both\")\n        return None\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if bit is not None:\n        if isinstance(bit, int):\n            if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                return None\n            else:\n                local_id = self._bit_ids[bit]\n        elif isinstance(bit, str):\n            if bit not in self._bit_ids:\n                print(\"Set valid bit id\")\n                return None\n            else:\n                local_id = bit\n        else:\n            print(\"'bit' must be integer index or string identifier\")\n            return None\n        current_local_zarr_path = str(\n            self._readouts_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\"corrected_data\")\n        )\n    else:\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                local_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id\")\n                return None\n            else:\n                local_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n        current_local_zarr_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\"corrected_data\")\n        )\n\n    if not Path(current_local_zarr_path).exists():\n        print(\"Corrected image not found.\")\n        return None\n\n    try:\n        spec = self._zarrv2_spec.copy()\n        spec[\"metadata\"][\"dtype\"] = \"&lt;u2\"\n        corrected_image = self._load_from_zarr_array(\n            self._get_kvstore_key(current_local_zarr_path),\n            spec,\n            return_future,\n        )\n        return corrected_image\n    except (IOError, OSError, ZarrError):\n        print(\"Error loading corrected image.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_local_decoded_spots","title":"<code>load_local_decoded_spots(tile)</code>","text":"<p>Load decoded spots and features for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <p>Returns:</p> Name Type Description <code>tile_features</code> <code>Optional[DataFrame]</code> <p>Decoded spots and features for one tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_local_decoded_spots(\n    self,\n    tile: Union[int, str],\n) -&gt; Optional[pd.DataFrame]:\n    \"\"\"Load decoded spots and features for one tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n\n    Returns\n    -------\n    tile_features : Optional[pd.DataFrame]\n        Decoded spots and features for one tile.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    current_tile_features_path = self._decoded_root_path / Path(\n        tile_id + \"_decoded_features.parquet\"\n    )\n\n    if not current_tile_features_path.exists():\n        print(\"Decoded spots not found.\")\n        return None\n    else:\n        tile_features = self._load_from_parquet(current_tile_features_path)\n        return tile_features\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_local_registered_image","title":"<code>load_local_registered_image(tile, round=None, bit=None, return_future=True)</code>","text":"<p>Local registered, deconvolved image for fidiculial OR readout bit for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>round</code> <code>Optional[Union[int, str]]</code> <p>Round index or round id.</p> <code>None</code> <code>bit</code> <code>Optional[Union[int, str]]</code> <p>Bit index or bit id.</p> <code>None</code> <code>return_future</code> <code>Optional[bool]</code> <p>Return future array.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>registered_decon_image</code> <code>Optional[ArrayLike]</code> <p>Registered, deconvolved image for fidiculial OR readout bit for one tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_local_registered_image(\n    self,\n    tile: Union[int, str],\n    round: Optional[Union[int, str]] = None,\n    bit: Optional[Union[int, str]] = None,\n    return_future: Optional[bool] = True,\n) -&gt; Optional[ArrayLike]:\n    \"\"\"Local registered, deconvolved image for fidiculial OR readout bit for one tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n    round : Optional[Union[int, str]]\n        Round index or round id.\n    bit : Optional[Union[int, str]]\n        Bit index or bit id.\n    return_future : Optional[bool]\n        Return future array.\n\n    Returns\n    -------\n    registered_decon_image : Optional[ArrayLike]\n        Registered, deconvolved image for fidiculial OR readout bit for one tile.\n    \"\"\"\n\n    if (round is None and bit is None) or (round is not None and bit is not None):\n        print(\"Provide either 'round' or 'bit', but not both\")\n        return None\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if bit is not None:\n        if isinstance(bit, int):\n            if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                return None\n            else:\n                local_id = self._bit_ids[bit]\n        elif isinstance(bit, str):\n            if bit not in self._bit_ids:\n                print(\"Set valid bit id\")\n                return None\n            else:\n                local_id = bit\n        else:\n            print(\"'bit' must be integer index or string identifier\")\n            return None\n        current_local_zarr_path = str(\n            self._readouts_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\"registered_decon_data\")\n        )\n    else:\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                local_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id\")\n                return None\n            else:\n                local_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n        current_local_zarr_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\"registered_decon_data\")\n        )\n\n    if not Path(current_local_zarr_path).exists():\n        #print(\"Registered deconvolved image not found.\")\n        return None\n\n    try:\n        spec = self._zarrv2_spec.copy()\n        spec[\"metadata\"][\"dtype\"] = \"&lt;u2\"\n        registered_decon_image = self._load_from_zarr_array(\n            self._get_kvstore_key(current_local_zarr_path),\n            spec,\n            return_future,\n        )\n        return registered_decon_image\n    except (IOError, OSError, ZarrError) as e:\n        print(e)\n        print(\"Error loading registered deconvolved image.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_local_rigid_xform_xyz_px","title":"<code>load_local_rigid_xform_xyz_px(tile, round)</code>","text":"<p>Load calculated rigid registration transform for one round and tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>round</code> <code>Union[int, str]</code> <p>Round index or round id.</p> required <p>Returns:</p> Name Type Description <code>rigid_xform_xyz_px</code> <code>Optional[ArrayLike]</code> <p>Local rigid registration transform for one round and tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_local_rigid_xform_xyz_px(\n    self,\n    tile: Union[int, str],\n    round: Union[int, str],\n) -&gt; Optional[ArrayLike]:\n    \"\"\"Load calculated rigid registration transform for one round and tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n    round : Union[int, str]\n        Round index or round id.\n\n    Returns\n    -------\n    rigid_xform_xyz_px : Optional[ArrayLike]\n        Local rigid registration transform for one round and tile.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(round, int):\n        if round &lt; 0:\n            print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n            return None\n        else:\n            round_id = self._round_ids[round]\n    elif isinstance(round, str):\n        if round not in self._round_ids:\n            print(\"Set valid round id\")\n            return None\n        else:\n            round_id = round\n    else:\n        print(\"'round' must be integer index or string identifier\")\n        return None\n    try:\n        zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(round_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n        attributes = self._load_from_json(zattrs_path)\n        rigid_xform_xyz_px = np.asarray(\n            attributes[\"rigid_xform_xyz_px\"], dtype=np.float32\n        )\n        return rigid_xform_xyz_px\n    except (FileNotFoundError, json.JSONDecodeError):\n        print(tile_id, round_id)\n        print(\"Rigid transform mapping back to first round not found.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_local_round_linker","title":"<code>load_local_round_linker(tile, bit)</code>","text":"<p>Load fidicual round linked to readout bit for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>bit</code> <code>Union[int, str]</code> <p>Bit index or bit id.</p> required <p>Returns:</p> Name Type Description <code>round_linker</code> <code>Optional[Sequence[int]]</code> <p>Fidicual round linked to readout bit for one tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_local_round_linker(\n    self,\n    tile: Union[int, str],\n    bit: Union[int, str],\n) -&gt; Optional[Sequence[int]]:\n    \"\"\"Load fidicual round linked to readout bit for one tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n    bit : Union[int, str]\n        Bit index or bit id.\n\n    Returns\n    -------\n    round_linker : Optional[Sequence[int]]\n        Fidicual round linked to readout bit for one tile.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id.\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(bit, int):\n        if bit &lt; 0 or bit &gt; len(self._bit_ids):\n            print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n            return None\n        else:\n            bit_id = self._bit_ids[bit]\n    elif isinstance(bit, str):\n        if bit not in self._bit_ids:\n            print(\"Set valid bit id.\")\n            return None\n        else:\n            bit_id = bit\n    else:\n        print(\"'bit' must be integer index or string identifier\")\n        return None\n\n    try:\n        zattrs_path = str(\n            self._readouts_root_path\n            / Path(tile_id)\n            / Path(bit_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n        attributes = self._load_from_json(zattrs_path)\n        return int(attributes[\"round_linker\"])\n    except FileNotFoundError:\n        print(tile_id, bit_id)\n        print(\"Round linker attribute not found.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_local_stage_position_zyx_um","title":"<code>load_local_stage_position_zyx_um(tile, round)</code>","text":"<p>Load tile stage position for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>round</code> <code>Union[int, str]</code> <p>Round index or round id.</p> required <p>Returns:</p> Name Type Description <code>stage_zyx_um</code> <code>Optional[ArrayLike]</code> <p>Tile stage position for one tile.</p> <code>affine_zyx_um</code> <code>Optional[ArrayLike]</code> <p>Affine transformation between stage and camera</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_local_stage_position_zyx_um(\n    self,\n    tile: Union[int, str],\n    round: Union[int, str],\n) -&gt; Optional[ArrayLike]:\n    \"\"\"Load tile stage position for one tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n    round : Union[int, str]\n        Round index or round id.\n\n    Returns\n    -------\n    stage_zyx_um : Optional[ArrayLike]\n        Tile stage position for one tile.\n    affine_zyx_um: Optional[ArrayLike]\n        Affine transformation between stage and camera\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id.\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(round, int):\n        if round &lt; 0:\n            print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n            return None\n        else:\n            round_id = self._round_ids[round]\n    elif isinstance(round, str):\n        if round not in self._round_ids:\n            print(\"Set valid round id.\")\n            return None\n        else:\n            round_id = round\n    else:\n        print(\"'round' must be integer index or string identifier\")\n        return None\n\n    try:\n        zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(round_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n        attributes = self._load_from_json(zattrs_path)\n        return np.asarray(attributes[\"stage_zyx_um\"], dtype=np.float32),\\\n            np.asarray(attributes[\"affine_zyx_px\"], dtype=np.float32)\n    except FileNotFoundError:\n        print(tile_id, round_id)\n        print(\"Stage position attribute not found.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_local_ufish_image","title":"<code>load_local_ufish_image(tile, bit, return_future=True)</code>","text":"<p>Load readout bit U-FISH prediction image for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>bit</code> <code>Union[int, str]</code> <p>Bit index or bit id.</p> required <code>return_future</code> <code>Optional[bool]</code> <code>True</code> <p>Returns:</p> Name Type Description <code>registered_ufish_image</code> <code>Optional[ArrayLike]</code> <p>U-FISH prediction image for one tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_local_ufish_image(\n    self,\n    tile: Union[int, str],\n    bit: Union[int, str],\n    return_future: Optional[bool] = True,\n) -&gt; Optional[ArrayLike]:\n    \"\"\"Load readout bit U-FISH prediction image for one tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n    bit : Union[int, str]\n        Bit index or bit id.\n    return_future : Optional[bool]\n\n    Returns\n    -------\n    registered_ufish_image : Optional[ArrayLike]\n        U-FISH prediction image for one tile.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(bit, int):\n        if bit &lt; 0 or bit &gt; len(self._bit_ids):\n            print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n            return None\n        else:\n            bit_id = self._bit_ids[bit]\n    elif isinstance(bit, str):\n        if bit not in self._bit_ids:\n            print(\"Set valid bit id\")\n            return None\n        else:\n            bit_id = bit\n    else:\n        print(\"'bit' must be integer index or string identifier\")\n        return None\n\n    current_local_zarr_path = str(\n        self._readouts_root_path\n        / Path(tile_id)\n        / Path(bit_id + \".zarr\")\n        / Path(\"registered_ufish_data\")\n    )\n\n    if not Path(current_local_zarr_path).exists():\n        print(\"U-FISH prediction image not found.\")\n        return None\n\n    try:\n        spec = self._zarrv2_spec.copy()\n        spec[\"metadata\"][\"dtype\"] = \"&lt;f4\"\n        registered_ufish_image = self._load_from_zarr_array(\n            self._get_kvstore_key(current_local_zarr_path),\n            spec,\n            return_future,\n        )\n        return registered_ufish_image\n    except (IOError, OSError, ZarrError) as e:\n        print(e)\n        print(\"Error loading U-FISH image.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_local_ufish_spots","title":"<code>load_local_ufish_spots(tile, bit)</code>","text":"<p>Load U-FISH spot localizations and features for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>bit</code> <code>Union[int, str]</code> <p>Bit index or bit id.</p> required <p>Returns:</p> Name Type Description <code>ufish_localizations</code> <code>Optional[DataFrame]</code> <p>U-FISH localizations and features for one tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_local_ufish_spots(\n    self,\n    tile: Union[int, str],\n    bit: Union[int, str],\n) -&gt; Optional[pd.DataFrame]:\n    \"\"\"Load U-FISH spot localizations and features for one tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n    bit : Union[int, str]\n        Bit index or bit id.\n\n    Returns\n    -------\n    ufish_localizations : Optional[pd.DataFrame]\n        U-FISH localizations and features for one tile.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(bit, int):\n        if bit &lt; 0 or bit &gt; len(self._bit_ids):\n            print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n            return None\n        else:\n            bit_id = self._bit_ids[bit]\n    elif isinstance(bit, str):\n        if bit not in self._bit_ids:\n            print(\"Set valid bit id\")\n            return None\n        else:\n            bit_id = bit\n    else:\n        print(\"'bit' must be integer index or string identifier\")\n        return None\n\n    current_ufish_localizations_path = (\n        self._ufish_localizations_root_path\n        / Path(tile_id)\n        / Path(bit_id + \".parquet\")\n    )\n\n    if not current_ufish_localizations_path.exists():\n        print(\"U-FISH localizations not found.\")\n        return None\n    else:\n        ufish_localizations = self._load_from_parquet(\n            current_ufish_localizations_path\n        )\n        return ufish_localizations\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_local_wavelengths_um","title":"<code>load_local_wavelengths_um(tile, round=None, bit=None)</code>","text":"<p>Load wavelengths for fidicual OR readout bit for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>round</code> <code>Optional[Union[int, str]]</code> <p>Round index or round id.</p> <code>None</code> <code>bit</code> <code>Optional[Union[int, str]]</code> <p>Bit index or bit id.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>wavelengths_um</code> <code>Optional[tuple[float, float]]</code> <p>Wavelengths for fidicual OR readout bit for one tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_local_wavelengths_um(\n    self,\n    tile: Union[int, str],\n    round: Optional[Union[int, str]] = None,\n    bit: Optional[Union[int, str]] = None,\n) -&gt; Optional[tuple[float, float]]:\n    \"\"\"Load wavelengths for fidicual OR readout bit for one tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n    round : Optional[Union[int, str]]   \n        Round index or round id.\n    bit : Optional[Union[int, str]]\n        Bit index or bit id.\n\n    Returns\n    -------\n    wavelengths_um : Optional[tuple[float, float]]\n        Wavelengths for fidicual OR readout bit for one tile.\n    \"\"\"\n\n    if (round is None and bit is None) or (round is not None and bit is not None):\n        print(\"Provide either 'round' or 'bit', but not both\")\n        return None\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if bit is not None:\n        if isinstance(bit, int):\n            if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                return None\n            else:\n                local_id = self._bit_ids[bit]\n        elif isinstance(bit, str):\n            if bit not in self._bit_ids:\n                print(\"Set valid bit id\")\n                return None\n            else:\n                local_id = bit\n        else:\n            print(\"'bit' must be integer index or string identifier\")\n            return None\n        zattrs_path = str(\n            self._readouts_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n    else:\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                local_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id\")\n                return None\n            else:\n                local_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n        zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n\n    try:\n        attributes = self._load_from_json(zattrs_path)\n        ex_wavelength_um = attributes[\"excitation_um\"]\n        em_wavelength_um = attributes[\"emission_um\"]\n        return (ex_wavelength_um, em_wavelength_um)\n    except KeyError:\n        print(\"Wavelength attributes not found.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.reformat_baysor_3D_oultines","title":"<code>reformat_baysor_3D_oultines()</code>","text":"<p>Reformat baysor 3D json file into ImageJ ROIs.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def reformat_baysor_3D_oultines(self):\n    \"\"\"Reformat baysor 3D json file into ImageJ ROIs.\"\"\"\n    import re\n\n    # Load the JSON file\n    baysor_output_path = self._segmentation_root_path / Path(\"baysor\")\n    baysor_segmentation = baysor_output_path / Path(r\"segmentation_polygons_3d.json\")\n    with open(baysor_segmentation, 'r') as file:\n        data = json.load(file)\n\n\n    # Dictionary to group polygons by cell ID\n    cell_polygons = defaultdict(list)\n\n    def parse_z_range(z_range):\n        cleaned_range = re.sub(r\"[^\\d.,-]\", \"\", z_range)  # Remove non-numeric, non-period, non-comma, non-dash characters\n        return map(float, cleaned_range.split(\",\"))\n\n    # Iterate through each z-plane and corresponding polygons\n    for z_range, details in data.items():\n        z_start, z_end = parse_z_range(z_range)\n\n        for geometry in details[\"geometries\"]:\n            coordinates = geometry[\"coordinates\"][0]  # Assuming the outer ring of the polygon\n            cell_id = geometry[\"cell\"]  # Get the cell ID\n\n            # Store the polygon with its z-range\n            cell_polygons[cell_id].append({\n                \"z_start\": z_start,\n                \"z_end\": z_end,\n                \"coordinates\": coordinates\n            })\n\n    rois = []\n\n    # Process each cell ID to create 3D ROIs\n    for cell_id, polygons in cell_polygons.items():\n        for idx, polygon in enumerate(polygons):\n            x_coords = [point[0] for point in polygon[\"coordinates\"]]\n            y_coords = [point[1] for point in polygon[\"coordinates\"]]\n\n\n            z_start = polygon[\"z_start\"]\n            z_end = polygon[\"z_end\"]\n\n            try:\n                # Create an ImageJRoi object for the polygon using frompoints\n                coords = list(zip(x_coords, y_coords))  # List of (x, y) tuples\n                roi = ImagejRoi.frompoints(coords)\n                roi.roitype = ROI_TYPE.POLYGON  # Set the ROI type to Polygon\n                roi.coordinates = coords  # Explicitly assign coordinates to the ROI\n                roi.name = f\"cell_{str(cell_id)}_zstart_{str(z_start)}_zend_{str(z_end)}\"  # Ensure unique name\n                rois.append(roi)\n            except Exception as e:\n                print(f\"Error while creating ROI for cell ID {cell_id}: {e}\")\n\n    # Write all ROIs to a ZIP file   \n    output_file = baysor_output_path / Path(r\"3d_cell_rois.zip\")\n    roiwrite(output_file, rois,mode='w')\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.reprocess_and_save_filtered_spots_with_baysor_outlines","title":"<code>reprocess_and_save_filtered_spots_with_baysor_outlines()</code>","text":"<p>Reprocess filtered spots using baysor cell outlines, then save.</p> <p>Loads the 3D cell outlines from Baysor, checks all points to see what  (if any) cell outline that the spot falls within, and then saves the data back to the datastore.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def reprocess_and_save_filtered_spots_with_baysor_outlines(self):\n    \"\"\"Reprocess filtered spots using baysor cell outlines, then save.\n\n    Loads the 3D cell outlines from Baysor, checks all points to see what \n    (if any) cell outline that the spot falls within, and then saves the\n    data back to the datastore.\n    \"\"\"\n    from rtree import index\n    import re\n\n    rois = self.load_global_baysor_outlines()\n    filtered_spots_df = self.load_global_filtered_decoded_spots()\n\n    parsed_spots_df = filtered_spots_df[\n            [\n                \"gene_id\",\n                \"global_z\",\n                \"global_y\",\n                \"global_x\",\n                \"cell_id\",\n                \"tile_idx\",\n            ]\n    ].copy()\n    parsed_spots_df.rename(\n        columns={\n            \"global_x\": \"x\",\n            \"global_y\": \"y\",\n            \"global_z\": \"z\",\n            \"gene_id\" : \"gene\",\n            \"cell_id\" : \"cell\",\n        },\n        inplace=True,\n    )\n    parsed_spots_df[\"transcript_id\"] = pd.util.hash_pandas_object(\n        parsed_spots_df, index=False\n    )\n\n    parsed_spots_df[\"assignment_confidence\"] = 1.0\n\n    # Create spatial index for ROIs\n    roi_index = index.Index()\n    roi_map = {}  # Map index IDs to ROIs\n\n    for idx, roi in enumerate(rois):\n        # Ensure roi.coordinates contains the polygon points\n        coords = roi.coordinates()\n\n        # Insert the polygon bounds into the spatial index\n        polygon = Polygon(coords)\n        roi_index.insert(idx, polygon.bounds)  # Use polygon bounds for indexing\n        roi_map[idx] = roi\n\n    # Function to check a single point\n    def point_in_roi(row):\n        point = Point(row[\"x\"], row[\"y\"])\n        candidate_indices = list(roi_index.intersection(point.bounds))  # Search spatial index\n        for idx in candidate_indices:\n            roi = roi_map[idx]\n            match = re.search(r\"zstart_([-\\d.]+)_zend_([-\\d.]+)\", roi.name)\n            if match:\n                z_start = float(match.group(1))\n                z_end = float(match.group(2))\n                if z_start &lt;= row[\"z\"] &lt;= z_end:\n                    polygon = Polygon(roi.coordinates())\n                    if polygon.contains(point):\n                        return str(roi.name.split(\"_\")[1]) \n        return -1\n\n    # Apply optimized spatial lookup\n    parsed_spots_df[\"cell\"] = parsed_spots_df.apply(point_in_roi, axis=1)\n    parsed_spots_df = parsed_spots_df.loc[parsed_spots_df[\"cell\"] != -1]\n\n    current_global_filtered_decoded_path = (\n        self._datastore_path \n        / Path(\"all_tiles_filtered_decoded_features\")\n        / Path(\"refined_transcripts.parquet\")\n    )\n\n    self._save_to_parquet(parsed_spots_df, current_global_filtered_decoded_path)\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.run_baysor","title":"<code>run_baysor()</code>","text":"<p>Run Baysor\"</p> <p>Assumes that spots are prepped for Baysor and the Baysor path and options are set. Reformats ROIs into ImageJ style ROIs for later use.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def run_baysor(self):\n    \"\"\"Run Baysor\"\n\n    Assumes that spots are prepped for Baysor and the Baysor path and options are set.\n    Reformats ROIs into ImageJ style ROIs for later use.\n    \"\"\"\n\n    import subprocess\n\n    baysor_input_path = self._datastore_path / Path(\"all_tiles_filtered_decoded_features\") / Path(\"transcripts.parquet\")\n    baysor_output_path = self._segmentation_root_path / Path(\"baysor\")\n    baysor_output_path.mkdir(exist_ok=True)\n\n    julia_threading = r\"JULIA_NUM_THREADS=\"+str(self._julia_threads)+ \" \"\n    preview_baysor_options = r\"preview -c \" +str(self._baysor_options)\n    command = julia_threading + str(self._baysor_path) + \" \" + preview_baysor_options + \" \" +\\\n        str(baysor_input_path) + \" -o \" + str(baysor_output_path)\n\n    try:\n        result = subprocess.run(command, shell=True, check=True)\n        print(\"Baysor finished with return code:\", result.returncode)\n    except subprocess.CalledProcessError as e:\n        print(\"Baysor failed with:\", e)\n\n    # first try to run Baysor assuming that prior segmentations are present               \n    try:\n        run_baysor_options = r\"run -p -c \" +str(self._baysor_options)\n        command = julia_threading + str(self._baysor_path) + \" \" + run_baysor_options + \" \" +\\\n            str(baysor_input_path) + \" -o \" + str(baysor_output_path) + \\\n            \" --polygon-format GeometryCollectionLegacy --count-matrix-format tsv :cell_id\"\n        result = subprocess.run(command, shell=True, check=True)\n        print(\"Baysor finished with return code:\", result.returncode)\n    except subprocess.CalledProcessError:\n        # then fall back and run without prior segmentations.\n        # IMPORTANT: the .toml file has to be defined correctly for this to work!\n        try:\n            run_baysor_options = r\"run -p -c \" +str(self._baysor_options)\n            command = julia_threading + str(self._baysor_path) + \" \" + run_baysor_options + \" \" +\\\n                str(baysor_input_path) + \" -o \" + str(baysor_output_path) + \" --count-matrix-format tsv\"\n            result = subprocess.run(command, shell=True, check=True)\n            print(\"Baysor finished with return code:\", result.returncode)\n        except subprocess.CalledProcessError as e:\n            print(\"Baysor failed with:\", e)\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_coord_of_xform_px","title":"<code>save_coord_of_xform_px(of_xform_px, tile, block_size, block_stride, round, return_future=False)</code>","text":"<p>Save fidicual optical flow matrix for one round and tile.</p> <p>Parameters:</p> Name Type Description Default <code>of_xform_px</code> <code>ArrayLike</code> <p>Local fidicual optical flow matrix for one round and tile.</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>block_size</code> <code>Sequence[float]</code> <p>Block size for pixel warp</p> required <code>block_stride</code> <code>Sequence[float]</code> <p>Block stride for pixel warp</p> required <code>round</code> <code>Union[int, str]</code> <p>Round index or round id.</p> required <code>return_future</code> <code>Optional[bool]</code> <p>Return future array.</p> <code>False</code> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_coord_of_xform_px(\n    self,\n    of_xform_px: ArrayLike,\n    tile: Union[int, str],\n    block_size: Sequence[float],\n    block_stride: Sequence[float],\n    round: Union[int, str],\n    return_future: Optional[bool] = False,\n):\n    \"\"\"Save fidicual optical flow matrix for one round and tile.\n\n    Parameters\n    ----------\n    of_xform_px : ArrayLike\n        Local fidicual optical flow matrix for one round and tile.\n    tile : Union[int, str]\n        Tile index or tile id.\n    block_size : Sequence[float]\n        Block size for pixel warp\n    block_stride: Sequence[float]\n        Block stride for pixel warp\n    round : Union[int, str] \n        Round index or round id.\n    return_future : Optional[bool]\n        Return future array.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(round, int):\n        if round &lt; 0:\n            print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n            return None\n        else:\n            local_id = self._round_ids[round]\n    elif isinstance(round, str):\n        if round not in self._round_ids:\n            print(\"Set valid round id\")\n            return None\n        else:\n            local_id = round\n    else:\n        print(\"'round' must be integer index or string identifier\")\n        return None\n    current_local_zarr_path = str(\n        self._polyDT_root_path\n        / Path(tile_id)\n        / Path(local_id + \".zarr\")\n        / Path(\"opticalflow_xform_px\")\n    )\n    current_local_zattrs_path = str(\n        self._polyDT_root_path\n        / Path(tile_id)\n        / Path(local_id + \".zarr\")\n        / Path(\".zattrs\")\n    )\n\n    try:\n        compressor = {\n            \"id\": \"blosc\",\n            \"cname\": \"zstd\",\n            \"clevel\": 5,\n            \"shuffle\": 2,\n        }\n        spec_of = {\n            \"driver\": \"zarr\",\n            \"kvstore\": None,\n            \"metadata\": {\"compressor\": compressor},\n            \"open\": True,\n            \"assume_metadata\": False,\n            \"create\": True,\n            \"delete_existing\": False,\n        }\n        self._save_to_zarr_array(\n            of_xform_px,\n            self._get_kvstore_key(current_local_zarr_path),\n            spec_of.copy(),\n            return_future,\n        )\n        attributes = self._load_from_json(current_local_zattrs_path)\n        attributes[\"block_size\"] = block_size.tolist()\n        attributes[\"block_stride\"] = block_stride.tolist()\n        self._save_to_json(attributes, current_local_zattrs_path)\n    except (IOError, OSError, TimeoutError):\n        print(\"Error saving optical flow transform.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_global_cellpose_segmentation_image","title":"<code>save_global_cellpose_segmentation_image(cellpose_image, downsampling, return_future=False)</code>","text":"<p>Save Cellpose max projection, downsampled segmentation image.</p> <p>Parameters:</p> Name Type Description Default <code>cellpose_image</code> <code>ArrayLike</code> <p>Cellpose max projection, downsampled segmentation image.</p> required <code>downsampling</code> <code>Sequence[float]</code> <p>Downsample factors.</p> required <code>return_future</code> <code>Optional[bool]</code> <p>Return future array.</p> <code>False</code> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_global_cellpose_segmentation_image(\n    self,\n    cellpose_image: ArrayLike,\n    downsampling: Sequence[float],\n    return_future: Optional[bool] = False,\n):\n    \"\"\"Save Cellpose max projection, downsampled segmentation image.\n\n    Parameters\n    ----------\n    cellpose_image : ArrayLike\n        Cellpose max projection, downsampled segmentation image.\n    downsampling : Sequence[float]\n        Downsample factors.\n    return_future : Optional[bool]\n        Return future array.\n    \"\"\"\n\n    current_local_zarr_path = str(\n        self._segmentation_root_path\n        / Path(\"cellpose\")\n        / Path(\"cellpose.zarr\")\n        / Path(\"masks_polyDT_iso_zyx\")\n    )\n    current_local_zattrs_path = str(\n        self._segmentation_root_path\n        / Path(\"cellpose\")\n        / Path(\"cellpose.zarr\")\n        / Path(\"masks_polyDT_iso_zyx\")\n        / Path(\".zattrs\")\n    )\n\n    attributes = {\"downsampling\": downsampling}\n\n    try:\n        self._save_to_zarr_array(\n            cellpose_image,\n            self._get_kvstore_key(current_local_zarr_path),\n            self._zarrv2_spec.copy(),\n            return_future,\n        )\n        self._save_to_json(attributes, current_local_zattrs_path)\n    except (IOError, OSError, TimeoutError):\n        print(\"Error saving Cellpose image.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_global_coord_xforms_um","title":"<code>save_global_coord_xforms_um(affine_zyx_um, origin_zyx_um, spacing_zyx_um, tile)</code>","text":"<p>Save global registration transform for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>affine_zyx_um</code> <code>ArrayLike</code> <p>Global affine registration transform for one tile.</p> required <code>origin_zyx_um</code> <code>ArrayLike</code> <p>Global origin registration transform for one tile.</p> required <code>spacing_zyx_um</code> <code>ArrayLike</code> <p>Global spacing registration transform for one tile.</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_global_coord_xforms_um(\n    self,\n    affine_zyx_um: ArrayLike,\n    origin_zyx_um: ArrayLike,\n    spacing_zyx_um: ArrayLike,\n    tile: Union[int, str],\n) -&gt; None:\n    \"\"\"Save global registration transform for one tile.\n\n    Parameters\n    ----------\n    affine_zyx_um : ArrayLike\n        Global affine registration transform for one tile.\n    origin_zyx_um : ArrayLike\n        Global origin registration transform for one tile.\n    spacing_zyx_um : ArrayLike\n        Global spacing registration transform for one tile.\n    tile : Union[int, str]\n        Tile index or tile id.\n    \"\"\"\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    try:\n        zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(self._round_ids[0] + \".zarr\")\n            / Path(\".zattrs\")\n        )\n        attributes = self._load_from_json(zattrs_path)\n        attributes[\"affine_zyx_um\"] = affine_zyx_um.tolist()\n        attributes[\"origin_zyx_um\"] = origin_zyx_um.tolist()\n        attributes[\"spacing_zyx_um\"] = spacing_zyx_um.tolist()\n        self._save_to_json(attributes, zattrs_path)\n    except (FileNotFoundError, json.JSONDecodeError) as e:\n        print(e)\n        print(\"Could not save global coordinate transforms.\")\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_global_fidicual_image","title":"<code>save_global_fidicual_image(fused_image, affine_zyx_um, origin_zyx_um, spacing_zyx_um, fusion_type='polyDT', return_future=False)</code>","text":"<p>Save downsampled, fused fidicual image.</p> <p>Parameters:</p> Name Type Description Default <code>fused_image</code> <code>ArrayLike</code> <p>Downsampled, fused fidicual image.</p> required <code>affine_zyx_um</code> <code>ArrayLike</code> <p>Global affine registration transform for fused image.</p> required <code>origin_zyx_um</code> <code>ArrayLike</code> <p>Global origin registration transform for fused image.</p> required <code>spacing_zyx_um</code> <code>ArrayLike</code> <p>Global spacing registration transform for fused image.</p> required <code>fusion_type</code> <code>str</code> <p>Type of fusion (polyDT or all_channels).</p> <code>'polyDT'</code> <code>return_future</code> <code>Optional[bool]</code> <p>Return future array.</p> <code>False</code> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_global_fidicual_image(\n    self,\n    fused_image: ArrayLike,\n    affine_zyx_um: ArrayLike,\n    origin_zyx_um: ArrayLike,\n    spacing_zyx_um: ArrayLike,\n    fusion_type: str = \"polyDT\",\n    return_future: Optional[bool] = False,\n):\n    \"\"\"Save downsampled, fused fidicual image.\n\n    Parameters\n    ----------\n    fused_image : ArrayLike\n        Downsampled, fused fidicual image.\n    affine_zyx_um : ArrayLike\n        Global affine registration transform for fused image.\n    origin_zyx_um : ArrayLike\n        Global origin registration transform for fused image.\n    spacing_zyx_um : ArrayLike\n        Global spacing registration transform for fused image.\n    fusion_type : str\n        Type of fusion (polyDT or all_channels).\n    return_future : Optional[bool]\n        Return future array.\n    \"\"\"\n\n    if fusion_type == \"polyDT\":\n        filename = \"fused_polyDT_iso_zyx\"\n    else:\n        filename = \"fused_all_channels_zyx\"\n    current_local_zarr_path = str(\n        self._fused_root_path / Path(\"fused.zarr\") / Path(filename)\n    )\n    current_local_zattrs_path = str(\n        self._fused_root_path\n        / Path(\"fused.zarr\")\n        / Path(filename)\n        / Path(\".zattrs\")\n    )\n\n    attributes = {\n        \"affine_zyx_um\": affine_zyx_um.tolist(),\n        \"origin_zyx_um\": origin_zyx_um.tolist(),\n        \"spacing_zyx_um\": spacing_zyx_um.tolist(),\n    }\n    try:\n        self._save_to_zarr_array(\n            fused_image.astype(np.uint16),\n            self._get_kvstore_key(current_local_zarr_path),\n            self._zarrv2_spec.copy(),\n            return_future,\n        )\n        self._save_to_json(attributes, current_local_zattrs_path)\n    except (IOError, OSError, TimeoutError):\n        print(\"Error saving fused image.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_global_filtered_decoded_spots","title":"<code>save_global_filtered_decoded_spots(filtered_decoded_df)</code>","text":"<p>Save all decoded and filtered spots.</p> <p>Parameters:</p> Name Type Description Default <code>filtered_decoded_df</code> <code>DataFrame</code> <p>All decoded and filtered spots.</p> required Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_global_filtered_decoded_spots(\n    self,\n    filtered_decoded_df: pd.DataFrame,\n):\n    \"\"\"Save all decoded and filtered spots.\n\n    Parameters\n    ----------\n    filtered_decoded_df : pd.DataFrame\n        All decoded and filtered spots.\n    \"\"\"\n\n    current_global_filtered_decoded_dir_path = self._datastore_path / Path(\n        \"all_tiles_filtered_decoded_features\"\n    )\n\n    if not current_global_filtered_decoded_dir_path.exists():\n        current_global_filtered_decoded_dir_path.mkdir()\n\n    current_global_filtered_decoded_path = (\n        current_global_filtered_decoded_dir_path / Path(\"decoded_features.parquet\")\n    )\n\n    self._save_to_parquet(filtered_decoded_df, current_global_filtered_decoded_path)\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_local_bit_linker","title":"<code>save_local_bit_linker(bit_linker, tile, round)</code>","text":"<p>Save readout bits linked to fidicual round for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>bit_linker</code> <code>Sequence[int]</code> <p>Readout bits linked to fidicual round for one tile.</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>round</code> <code>Union[int, str]</code> <p>Round index or round id.</p> required Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_local_bit_linker(\n    self,\n    bit_linker: Sequence[int],\n    tile: Union[int, str],\n    round: Union[int, str],\n):\n    \"\"\"Save readout bits linked to fidicual round for one tile.\n\n    Parameters\n    ----------\n    bit_linker : Sequence[int]\n        Readout bits linked to fidicual round for one tile.\n    tile : Union[int, str]\n        Tile index or tile id.\n    round : Union[int, str]\n        Round index or round id.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id.\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(round, int):\n        if round &lt; 0:\n            print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n            return None\n        else:\n            round_id = self._round_ids[round]\n    elif isinstance(round, str):\n        if round not in self._round_ids:\n            print(\"Set valid round id.\")\n            return None\n        else:\n            round_id = round\n    else:\n        print(\"'round' must be integer index or string identifier\")\n        return None\n\n    try:\n        zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(round_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n        attributes = self._load_from_json(zattrs_path)\n        attributes[\"bits\"] = bit_linker\n        self._save_to_json(attributes, zattrs_path)\n    except (FileNotFoundError, json.JSONDecodeError):\n        print(tile_id, round_id)\n        print(\"Error writing bit linker attribute.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_local_corrected_image","title":"<code>save_local_corrected_image(image, tile, gain_correction=True, hotpixel_correction=True, shading_correction=False, psf_idx=0, round=None, bit=None, return_future=False)</code>","text":"<p>Save gain and offset corrected image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ArrayLike</code> <p>Local corrected image.</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>gain_correction</code> <code>bool</code> <p>Gain correction applied (True) or not (False).</p> <code>True</code> <code>hotpixel_correction</code> <code>bool</code> <p>Hotpixel correction applied (True) or not (False).</p> <code>True</code> <code>shading_correction</code> <code>bool</code> <p>Shading correction applied (True) or not (False).</p> <code>False</code> <code>psf_idx</code> <code>int</code> <p>PSF index.</p> <code>0</code> <code>round</code> <code>Optional[Union[int, str]]</code> <p>Round index or round id.</p> <code>None</code> <code>bit</code> <code>Optional[Union[int, str]]</code> <p>Bit index or bit id.</p> <code>None</code> <code>return_future</code> <code>Optional[bool]</code> <p>Return future array.</p> <code>False</code> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code> def save_local_corrected_image(\n     self,\n     image: ArrayLike,\n     tile: Union[int, str],\n     gain_correction: bool = True,\n     hotpixel_correction: bool = True,\n     shading_correction: bool = False,\n     psf_idx: int = 0,\n     round: Optional[Union[int, str]] = None,\n     bit: Optional[Union[int, str]] = None,\n     return_future: Optional[bool] = False,\n ):\n     \"\"\"Save gain and offset corrected image.\n\n     Parameters\n     ----------\n     image : ArrayLike\n         Local corrected image.\n     tile : Union[int, str]\n         Tile index or tile id.\n     gain_correction : bool\n         Gain correction applied (True) or not (False).\n     hotpixel_correction : bool\n         Hotpixel correction applied (True) or not (False).\n     shading_correction : bool\n         Shading correction applied (True) or not (False).\n     psf_idx : int\n         PSF index.\n     round : Optional[Union[int, str]]\n         Round index or round id.\n     bit : Optional[Union[int, str]]\n         Bit index or bit id.\n     return_future : Optional[bool]\n         Return future array.\n\"\"\"\n\n     if (round is None and bit is None) or (round is not None and bit is not None):\n         print(\"Provide either 'round' or 'bit', but not both\")\n         return None\n\n     if isinstance(tile, int):\n         if tile &lt; 0 or tile &gt; self._num_tiles:\n             print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n             return None\n         else:\n             tile_id = self._tile_ids[tile]\n     elif isinstance(tile, str):\n         if tile not in self._tile_ids:\n             print(\"set valid tiled id\")\n             return None\n         else:\n             tile_id = tile\n     else:\n         print(\"'tile' must be integer index or string identifier\")\n         return None\n\n     if bit is not None:\n         if isinstance(bit, int):\n             if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                 print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                 return None\n             else:\n                 local_id = self._bit_ids[bit]\n         elif isinstance(bit, str):\n             if bit not in self._bit_ids:\n                 print(\"Set valid bit id\")\n                 return None\n             else:\n                 local_id = bit\n         else:\n             print(\"'bit' must be integer index or string identifier\")\n             return None\n         current_local_zarr_path = str(\n             self._readouts_root_path\n             / Path(tile_id)\n             / Path(local_id + \".zarr\")\n             / Path(\"corrected_data\")\n         )\n         current_local_zattrs_path = str(\n             self._readouts_root_path\n             / Path(tile_id)\n             / Path(local_id + \".zarr\")\n             / Path(\".zattrs\")\n         )\n     else:\n         if isinstance(round, int):\n             if round &lt; 0:\n                 print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                 return None\n             else:\n                 local_id = self._round_ids[round]\n         elif isinstance(round, str):\n             if round not in self._round_ids:\n                 print(\"Set valid round id\")\n                 return None\n             else:\n                 local_id = round\n         else:\n             print(\"'round' must be integer index or string identifier\")\n             return None\n         current_local_zarr_path = str(\n             self._polyDT_root_path\n             / Path(tile_id)\n             / Path(local_id + \".zarr\")\n             / Path(\"corrected_data\")\n         )\n         current_local_zattrs_path = str(\n             self._polyDT_root_path\n             / Path(tile_id)\n             / Path(local_id + \".zarr\")\n             / Path(\".zattrs\")\n         )\n\n     try:\n         self._save_to_zarr_array(\n             image,\n             self._get_kvstore_key(current_local_zarr_path),\n             self._zarrv2_spec,\n             return_future,\n         )\n         attributes = self._load_from_json(current_local_zattrs_path)\n         attributes[\"gain_correction\"] = (gain_correction,)\n         attributes[\"hotpixel_correction\"] = (hotpixel_correction,)\n         attributes[\"shading_correction\"] = (shading_correction)\n         attributes[\"psf_idx\"] = psf_idx\n         self._save_to_json(attributes, current_local_zattrs_path)\n     except (IOError, OSError, TimeoutError) as e:\n         print(e)\n         print(\"Error saving corrected image.\")\n         return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_local_decoded_spots","title":"<code>save_local_decoded_spots(features_df, tile)</code>","text":"<p>Save decoded spots and features for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>features_df</code> <code>DataFrame</code> <p>Decoded spots and features for one tile.</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_local_decoded_spots(\n    self,\n    features_df: pd.DataFrame,\n    tile: Union[int, str],\n) -&gt; None:\n    \"\"\"Save decoded spots and features for one tile.\n\n    Parameters\n    ----------\n    features_df : pd.DataFrame\n        Decoded spots and features for one tile.\n    tile : Union[int, str]\n        Tile index or tile id.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    current_tile_features_path = self._decoded_root_path / Path(\n        tile_id + \"_decoded_features.parquet\"\n    )\n\n    self._save_to_parquet(features_df, current_tile_features_path)\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_local_registered_image","title":"<code>save_local_registered_image(registered_image, tile, deconvolution=True, round=None, bit=None, return_future=False)</code>","text":"<p>Save registered, deconvolved image.</p> <p>Parameters:</p> Name Type Description Default <code>registered_image</code> <code>ArrayLike</code> <p>Registered, deconvolved image.</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>deconvolution</code> <code>bool</code> <p>Deconvolution applied (True) or not (False).</p> <code>True</code> <code>round</code> <code>Optional[Union[int, str]]</code> <p>Round index or round id.</p> <code>None</code> <code>bit</code> <code>Optional[Union[int, str]]</code> <p>Bit index or bit id.</p> <code>None</code> <code>return_future</code> <code>Optional[bool]</code> <p>Return future array.</p> <code>False</code> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_local_registered_image(\n    self,\n    registered_image: ArrayLike,\n    tile: Union[int, str],\n    deconvolution: bool = True,\n    round: Optional[Union[int, str]] = None,\n    bit: Optional[Union[int, str]] = None,\n    return_future: Optional[bool] = False,\n):\n    \"\"\"Save registered, deconvolved image.\n\n    Parameters\n    ----------\n    registered_image : ArrayLike\n        Registered, deconvolved image.\n    tile : Union[int, str]\n        Tile index or tile id.\n    deconvolution : bool\n        Deconvolution applied (True) or not (False).\n    round : Optional[Union[int, str]]\n        Round index or round id.\n    bit : Optional[Union[int, str]]\n        Bit index or bit id.\n    return_future : Optional[bool]\n        Return future array.\n    \"\"\"\n\n    if (round is None and bit is None) or (round is not None and bit is not None):\n        print(\"Provide either 'round' or 'bit', but not both\")\n        return None\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if bit is not None:\n        if isinstance(bit, int):\n            if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                return None\n            else:\n                local_id = self._bit_ids[bit]\n        elif isinstance(bit, str):\n            if bit not in self._bit_ids:\n                print(\"Set valid bit id\")\n                return None\n            else:\n                local_id = bit\n        else:\n            print(\"'bit' must be integer index or string identifier\")\n            return None\n        current_local_zarr_path = str(\n            self._readouts_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\"registered_decon_data\")\n        )\n        current_local_zattrs_path = str(\n            self._readouts_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n    else:\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                local_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id\")\n                return None\n            else:\n                local_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n        current_local_zarr_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\"registered_decon_data\")\n        )\n        current_local_zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n\n    try:\n        spec = self._zarrv2_spec.copy()\n        spec[\"metadata\"][\"dtype\"] = \"&lt;u2\"\n        self._save_to_zarr_array(\n            registered_image,\n            self._get_kvstore_key(current_local_zarr_path),\n            spec,\n            return_future,\n        )\n        attributes = self._load_from_json(current_local_zattrs_path)\n        attributes[\"deconvolution\"] = deconvolution\n        self._save_to_json(attributes, current_local_zattrs_path)\n    except (IOError, OSError, TimeoutError):\n        print(\"Error saving corrected image.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_local_rigid_xform_xyz_px","title":"<code>save_local_rigid_xform_xyz_px(rigid_xform_xyz_px, tile, round)</code>","text":"<p>Save calculated rigid registration transform for one round and tile.</p> <p>Parameters:</p> Name Type Description Default <code>rigid_xform_xyz_px</code> <code>ArrayLike</code> <p>Local rigid registration transform for one round and tile.</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>round</code> <code>Union[int, str]</code> <p>Round index or round id.</p> required <p>Returns:</p> Name Type Description <code>rigid_xform_xyz_px</code> <code>Optional[ArrayLike]</code> <p>Local rigid registration transform for one round and tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_local_rigid_xform_xyz_px(\n    self,\n    rigid_xform_xyz_px: ArrayLike,\n    tile: Union[int, str],\n    round: Union[int, str],\n) -&gt; Optional[ArrayLike]:\n    \"\"\"Save calculated rigid registration transform for one round and tile.\n\n    Parameters\n    ----------\n    rigid_xform_xyz_px : ArrayLike\n        Local rigid registration transform for one round and tile.\n    tile : Union[int, str]\n        Tile index or tile id.\n    round : Union[int, str]\n        Round index or round id.\n\n    Returns\n    -------\n    rigid_xform_xyz_px : Optional[ArrayLike]\n        Local rigid registration transform for one round and tile.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(round, int):\n        if round &lt; 0:\n            print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n            return None\n        else:\n            round_id = self._round_ids[round]\n    elif isinstance(round, str):\n        if round not in self._round_ids:\n            print(\"Set valid round id\")\n            return None\n        else:\n            round_id = round\n    else:\n        print(\"'round' must be integer index or string identifier\")\n        return None\n    try:\n        zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(round_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n        attributes = self._load_from_json(zattrs_path)\n        attributes[\"rigid_xform_xyz_px\"] = rigid_xform_xyz_px.tolist()\n        self._save_to_json(attributes, zattrs_path)\n    except (FileNotFoundError, json.JSONDecodeError):\n        print(\"Error writing rigid transform attribute.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_local_round_linker","title":"<code>save_local_round_linker(round_linker, tile, bit)</code>","text":"<p>Save fidicual round linker attribute to readout bit for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>round_linker</code> <code>int</code> <p>Fidicual round linked to readout bit for one tile.</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>bit</code> <code>Union[int, str]</code> <p>Bit index or bit id.</p> required Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_local_round_linker(\n    self,\n    round_linker: int,\n    tile: Union[int, str],\n    bit: Union[int, str],\n):\n    \"\"\"Save fidicual round linker attribute to readout bit for one tile.\n\n    Parameters\n    ----------\n    round_linker : int\n        Fidicual round linked to readout bit for one tile.\n    tile : Union[int, str]\n        Tile index or tile id.\n    bit : Union[int, str]\n        Bit index or bit id.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id.\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(bit, int):\n        if bit &lt; 0 or bit &gt; len(self._bit_ids):\n            print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n            return None\n        else:\n            bit_id = self._bit_ids[bit]\n    elif isinstance(bit, str):\n        if bit not in self._bit_ids:\n            print(\"Set valid bit id.\")\n            return None\n        else:\n            bit_id = bit\n    else:\n        print(\"'bit' must be integer index or string identifier\")\n        return None\n\n    try:\n        zattrs_path = str(\n            self._readouts_root_path\n            / Path(tile_id)\n            / Path(bit_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n        attributes = self._load_from_json(zattrs_path)\n        attributes[\"round\"] = int(round_linker)\n        self._save_to_json(attributes, zattrs_path)\n    except (FileNotFoundError, json.JSONDecodeError):\n        print(tile_id, bit_id)\n        print(\"Error writing round linker attribute.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_local_stage_position_zyx_um","title":"<code>save_local_stage_position_zyx_um(stage_zyx_um, affine_zyx_px, tile, round)</code>","text":"<p>Save tile stage position for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>stage_zyx_um</code> <code>ArrayLike</code> <p>Tile stage position for one tile.</p> required <code>affine_zyx_px</code> <code>ArrayLike</code> <p>4x4 homogeneous affine matrix for stage transformation</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>round</code> <code>Union[int, str]</code> <p>Round index or round id.</p> required Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_local_stage_position_zyx_um(\n    self,\n    stage_zyx_um: ArrayLike,\n    affine_zyx_px: ArrayLike,\n    tile: Union[int, str],\n    round: Union[int, str],\n):\n    \"\"\"Save tile stage position for one tile.\n\n    Parameters\n    ----------\n    stage_zyx_um : ArrayLike\n        Tile stage position for one tile.\n    affine_zyx_px; ArrayLike\n        4x4 homogeneous affine matrix for stage transformation\n    tile : Union[int, str]\n        Tile index or tile id.\n    round : Union[int, str]\n        Round index or round id.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id.\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(round, int):\n        if round &lt; 0:\n            print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n            return None\n        else:\n            round_id = self._round_ids[round]\n    elif isinstance(round, str):\n        if round not in self._round_ids:\n            print(\"Set valid round id.\")\n            return None\n        else:\n            round_id = round\n    else:\n        print(\"'round' must be integer index or string identifier\")\n        return None\n\n    try:\n        zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(round_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n        attributes = self._load_from_json(zattrs_path)\n        attributes[\"stage_zyx_um\"] = stage_zyx_um.tolist()\n        attributes[\"affine_zyx_px\"] = affine_zyx_px.tolist()\n        self._save_to_json(attributes, zattrs_path)\n    except (FileNotFoundError, json.JSONDecodeError):\n        print(tile_id, round_id)\n        print(\"Error writing stage position attribute.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_local_ufish_image","title":"<code>save_local_ufish_image(ufish_image, tile, bit, return_future=False)</code>","text":"<p>Save U-FISH prediction image.</p> <p>Parameters:</p> Name Type Description Default <code>ufish_image</code> <code>ArrayLike</code> <p>U-FISH prediction image.</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>bit</code> <code>Union[int, str]</code> <p>Bit index or bit id.</p> required <code>return_future</code> <code>Optional[bool]</code> <p>Return future array.</p> <code>False</code> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_local_ufish_image(\n    self,\n    ufish_image: ArrayLike,\n    tile: Union[int, str],\n    bit: Union[int, str],\n    return_future: Optional[bool] = False,\n):\n    \"\"\"Save U-FISH prediction image.\n\n    Parameters\n    ----------\n    ufish_image : ArrayLike\n        U-FISH prediction image.\n    tile : Union[int, str]\n        Tile index or tile id.\n    bit : Union[int, str]\n        Bit index or bit id.\n    return_future : Optional[bool]\n        Return future array.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if bit is not None:\n        if isinstance(bit, int):\n            if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                return None\n            else:\n                local_id = self._bit_ids[bit]\n        elif isinstance(bit, str):\n            if bit not in self._bit_ids:\n                print(\"Set valid bit id\")\n                return None\n            else:\n                local_id = bit\n        else:\n            print(\"'bit' must be integer index or string identifier\")\n            return None\n        current_local_zarr_path = str(\n            self._readouts_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\"registered_ufish_data\")\n        )\n\n    try:\n        self._save_to_zarr_array(\n            ufish_image,\n            self._get_kvstore_key(current_local_zarr_path),\n            self._zarrv2_spec.copy(),\n            return_future,\n        )\n    except (IOError, OSError, ZarrError) as e:\n        print(e)\n        print(\"Error saving U-Fish image.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_local_ufish_spots","title":"<code>save_local_ufish_spots(spot_df, tile, bit)</code>","text":"<p>Save U-FISH localizations and features.</p> <p>Parameters:</p> Name Type Description Default <code>spot_df</code> <code>DataFrame</code> <p>U-FISH localizations and features.</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>bit</code> <code>Union[int, str]</code> <p>Bit index or bit id.</p> required Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_local_ufish_spots(\n    self,\n    spot_df: pd.DataFrame,\n    tile: Union[int, str],\n    bit: Union[int, str],\n):\n    \"\"\"Save U-FISH localizations and features.\n\n    Parameters\n    ----------\n    spot_df : pd.DataFrame\n        U-FISH localizations and features.\n    tile : Union[int, str]\n        Tile index or tile id.\n    bit : Union[int, str]\n        Bit index or bit id.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(bit, int):\n        if bit &lt; 0 or bit &gt; len(self._bit_ids):\n            print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n            return None\n        else:\n            bit_id = self._bit_ids[bit]\n    elif isinstance(bit, str):\n        if bit not in self._bit_ids:\n            print(\"Set valid bit id\")\n            return None\n        else:\n            bit_id = bit\n    else:\n        print(\"'bit' must be integer index or string identifier\")\n        return None\n\n    if not (self._ufish_localizations_root_path / Path(tile_id)).exists():\n        (self._ufish_localizations_root_path / Path(tile_id)).mkdir()\n\n    current_ufish_localizations_path = (\n        self._ufish_localizations_root_path\n        / Path(tile_id)\n        / Path(bit_id + \".parquet\")\n    )\n\n    try:\n        self._save_to_parquet(spot_df, current_ufish_localizations_path)\n    except (IOError, OSError) as e:\n        print(e)\n        print(\"Error saving U-FISH localizations.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_local_wavelengths_um","title":"<code>save_local_wavelengths_um(wavelengths_um, tile, round=None, bit=None)</code>","text":"<p>Save wavelengths for fidicual OR readout bit for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>wavelengths_um</code> <code>tuple[float, float]</code> <p>Wavelengths for fidicual OR readout bit for one tile.</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>round</code> <code>Optional[Union[int, str]]</code> <p>Round index or round id.</p> <code>None</code> <code>bit</code> <code>Optional[Union[int, str]]</code> <p>Bit index or bit id.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>wavelengths_um</code> <code>Optional[tuple[float, float]]</code> <p>Wavelengths for fidicual OR readout bit for one tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_local_wavelengths_um(\n    self,\n    wavelengths_um: tuple[float, float],\n    tile: Union[int, str],\n    round: Optional[Union[int, str]] = None,\n    bit: Optional[Union[int, str]] = None,\n) -&gt; Optional[tuple[float, float]]:\n    \"\"\"Save wavelengths for fidicual OR readout bit for one tile.\n\n    Parameters\n    ----------\n    wavelengths_um : tuple[float, float]\n        Wavelengths for fidicual OR readout bit for one tile.\n    tile : Union[int, str]\n        Tile index or tile id.\n    round : Optional[Union[int, str]]\n        Round index or round id.\n    bit : Optional[Union[int, str]]\n        Bit index or bit id.\n\n    Returns\n    -------\n    wavelengths_um : Optional[tuple[float, float]]\n        Wavelengths for fidicual OR readout bit for one tile.\n    \"\"\"\n\n    if (round is None and bit is None) or (round is not None and bit is not None):\n        print(\"Provide either 'round' or 'bit', but not both\")\n        return None\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if bit is not None:\n        if isinstance(bit, int):\n            if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                return None\n            else:\n                local_id = self._bit_ids[bit]\n        elif isinstance(bit, str):\n            if bit not in self._bit_ids:\n                print(\"Set valid bit id\")\n                return None\n            else:\n                local_id = bit\n        else:\n            print(\"'bit' must be integer index or string identifier\")\n            return None\n        zattrs_path = str(\n            self._readouts_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n    else:\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                local_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id\")\n                return None\n            else:\n                local_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n        zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n\n    try:\n        attributes = self._load_from_json(zattrs_path)\n        attributes[\"excitation_um\"] = float(wavelengths_um[0])\n        attributes[\"emission_um\"] = float(wavelengths_um[1])\n        self._save_to_json(attributes, zattrs_path)\n    except (FileNotFoundError, json.JSONDecodeError):\n        print(\"Error writing wavelength attributes.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_mtx","title":"<code>save_mtx(spots_source='baysor')</code>","text":"<p>Save mtx file for downstream analysis. Assumes Baysor has been run.</p> <p>Parameters:</p> Name Type Description Default <code>spots_source</code> <code>str</code> <p>source of spots. \"baysor\" or \"resegmented\".</p> <code>'baysor'</code> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_mtx(self, spots_source: str = \"baysor\"):\n    \"\"\"Save mtx file for downstream analysis. Assumes Baysor has been run.\n\n    Parameters\n    ----------\n    spots_source: str, default \"baysor\"\n        source of spots. \"baysor\" or \"resegmented\".\n    \"\"\"\n\n    from merfish3danalysis.utils.dataio import create_mtx\n\n    if spots_source == \"baysor\":\n        spots_path = self._datastore_path / Path(\"segmentation\") / Path(\"baysor\") / Path(\"segmentation.csv\")\n    elif spots_source == \"resegmented\":\n        spots_path = (\n            self._datastore_path \n            / Path(\"all_tiles_filtered_decoded_features\")\n            / Path(\"refined_transcripts.parquet\")\n        )\n\n    mtx_output_path = self._datastore_path / Path(\"mtx_output\")\n\n    create_mtx(\n        spots_path=spots_path,\n        output_dir_path=mtx_output_path,\n    )\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_spots_prepped_for_baysor","title":"<code>save_spots_prepped_for_baysor(prepped_for_baysor_df)</code>","text":"<p>Save spots prepped for Baysor.</p> <p>Parameters:</p> Name Type Description Default <code>prepped_for_baysor_df</code> <code>DataFrame</code> <p>Spots prepped for Baysor.</p> required Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_spots_prepped_for_baysor(self, prepped_for_baysor_df: pd.DataFrame):\n    \"\"\"Save spots prepped for Baysor.\n\n    Parameters\n    ----------\n    prepped_for_baysor_df : pd.DataFrame\n        Spots prepped for Baysor.\n    \"\"\"\n\n    current_global_filtered_decoded_dir_path = self._datastore_path / Path(\n        \"all_tiles_filtered_decoded_features\"\n    )\n\n    if not current_global_filtered_decoded_dir_path.exists():\n        current_global_filtered_decoded_dir_path.mkdir()\n\n    current_global_filtered_decoded_path = (\n        current_global_filtered_decoded_dir_path / Path(\"transcripts.parquet\")\n    )\n\n    self._save_to_parquet(prepped_for_baysor_df, current_global_filtered_decoded_path)\n</code></pre>"},{"location":"reference/modules/dataio/","title":"Data I/O Module","text":"<p>Data I/O functions for qi2lab 3D MERFISH.</p> <p>This module provides utilities for reading and writing data in various formats used by qi2lab 3D MERFISH datasets.</p> History: <ul> <li>2024/12: Refactored repo structure.</li> <li>2024/12: Updated docstrings.</li> <li>2024/07: Removed native NDTiff reading package; integrated tifffile/zarr.                Reduced dask dependencies.</li> </ul> <p>Functions:</p> Name Description <code>create_mtx</code> <p>Create a sparse matrix in MTX format from Baysor output.</p> <code>read_config_file</code> <p>Read config data from csv file. </p> <code>read_fluidics_program</code> <p>Read fluidics program from CSV file as pandas dataframe.</p> <code>read_metadatafile</code> <p>Read metadata from csv file. </p> <code>return_data_zarr</code> <p>Return NDTIFF data as a numpy array via tiffile.</p> <code>time_stamp</code> <p>Generate timestamp string.</p> <code>write_metadata</code> <p>Write dictionary as CSV file.</p> <code>write_sparse_mtx</code> <p>Write sparse matrix in MTX format.</p> <code>write_tsv</code> <p>Write data to TSV file.</p>"},{"location":"reference/modules/dataio/#merfish3danalysis.utils.dataio.create_mtx","title":"<code>create_mtx(spots_path, output_dir_path, confidence_cutoff=0.7)</code>","text":"<p>Create a sparse matrix in MTX format from Baysor output.</p> <p>Parameters:</p> Name Type Description Default <code>spots_path</code> <code>Union[Path, str]</code> <p>Path to spots file</p> required <code>output_dir_path</code> <code>Union[Path, str]</code> <p>Path to output directory</p> required <code>confidence_cutoff</code> <code>float</code> <p>Confidence cutoff for transcript assignment</p> <code>0.7</code> Source code in <code>src/merfish3danalysis/utils/dataio.py</code> <pre><code>def create_mtx(\n    spots_path: Union[Path,str], \n    output_dir_path: Union[Path,str], \n    confidence_cutoff: float = 0.7\n):\n    \"\"\"Create a sparse matrix in MTX format from Baysor output.\n\n    Parameters\n    ----------\n    spots_path: Union[Path,str]\n        Path to spots file\n    output_dir_path: Union[Path,str]\n        Path to output directory\n    confidence_cutoff: float\n        Confidence cutoff for transcript assignment\n    \"\"\"\n\n    # Read 5 columns from transcripts Parquet file\n    if spots_path.suffix == \".csv\":\n        transcripts_df = pd.read_csv(spots_path,\n                                    usecols=[\"gene\",\n                                            \"cell\",\n                                            \"assignment_confidence\"])\n        transcripts_df['cell'] = transcripts_df['cell'].replace('', pd.NA).dropna().str.split('-').str[1]\n    else:\n        transcripts_df = pd.read_parquet(spots_path,\n                            columns=[\"gene\",\n                                    \"cell\",\n                                    \"assignment_confidence\"])\n\n\n    transcripts_df['cell'] = pd.to_numeric(transcripts_df['cell'], errors='coerce').fillna(0).astype(int)\n\n    # Find distinct set of features.\n    features = transcripts_df[\"gene\"].dropna().unique()\n\n    # Create lookup dictionary\n    feature_to_index = dict()\n    for index, val in enumerate(features):\n        feature_to_index[str(val)] = index\n\n    # Find distinct set of cells. Discard the first entry which is 0 (non-cell)\n    cells = transcripts_df[\"cell\"].dropna().unique()\n    cells = cells[cells != 0]\n\n    # Create a cells x features data frame, initialized with 0\n    matrix = pd.DataFrame(0, index=range(len(features)), columns=cells, dtype=np.int32)\n\n    # Iterate through all transcripts\n    for index, row in transcripts_df.iterrows():\n        feature = str(row['gene'])\n        cell = row['cell']\n        conf = row['assignment_confidence']\n\n        # Ignore transcript below user-specified cutoff\n        if conf &lt; confidence_cutoff:\n            continue\n\n        # If cell is not 0 at this point, it means the transcript is associated with a cell\n        if cell != 0:\n            # Increment count in feature-cell matrix\n            matrix.at[feature_to_index[feature], cell] += 1\n\n    # Call a helper function to create Seurat and Scanpy compatible MTX output\n    write_sparse_mtx(output_dir_path, matrix, cells, features)\n</code></pre>"},{"location":"reference/modules/dataio/#merfish3danalysis.utils.dataio.read_config_file","title":"<code>read_config_file(config_path)</code>","text":"<p>Read config data from csv file. </p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>Union[Path, str]</code> <p>Location of configuration file</p> required <p>Returns:</p> Name Type Description <code>dict_from_csv</code> <code>dict</code> <p>instrument configuration metadata</p> Source code in <code>src/merfish3danalysis/utils/dataio.py</code> <pre><code>def read_config_file(config_path: Union[Path,str]) -&gt; dict:\n    \"\"\"Read config data from csv file. \n\n    Parameters\n    ----------\n    config_path: Path\n        Location of configuration file\n\n    Returns\n    -------\n    dict_from_csv: dict\n        instrument configuration metadata\n    \"\"\"\n\n    dict_from_csv = pd.read_csv(config_path, header=None, index_col=0).squeeze(\"columns\").to_dict()\n\n    return dict_from_csv\n</code></pre>"},{"location":"reference/modules/dataio/#merfish3danalysis.utils.dataio.read_fluidics_program","title":"<code>read_fluidics_program(program_path)</code>","text":"<p>Read fluidics program from CSV file as pandas dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>program_path</code> <code>Union[Path, str]</code> <p>location of fluidics program</p> required <p>Returns:</p> Name Type Description <code>df_fluidics</code> <code>Dataframe</code> <p>dataframe containing fluidics program</p> Source code in <code>src/merfish3danalysis/utils/dataio.py</code> <pre><code>def read_fluidics_program(program_path: Union[Path,str]) -&gt; pd.DataFrame:\n    \"\"\"Read fluidics program from CSV file as pandas dataframe.\n\n    Parameters\n    ----------\n    program_path: Path\n        location of fluidics program\n\n    Returns\n    -------\n    df_fluidics: Dataframe\n        dataframe containing fluidics program \n    \"\"\"\n\n    try:                \n        df_fluidics = pd.read_csv(program_path)            \n        df_fluidics = df_fluidics[[\"round\", \"source\", \"time\", \"pump\"]]\n        df_fluidics.dropna(axis=0, how='any', inplace=True)\n        df_fluidics[\"round\"] = df_fluidics[\"round\"].astype(int)\n        df_fluidics[\"pump\"] = df_fluidics[\"pump\"].astype(int)\n\n        print(\"Fluidics program loaded\")\n    except Exception as e:\n        raise Exception(\"Error in loading fluidics file:\\n\", e)\n\n    return df_fluidics\n</code></pre>"},{"location":"reference/modules/dataio/#merfish3danalysis.utils.dataio.read_metadatafile","title":"<code>read_metadatafile(fname)</code>","text":"<p>Read metadata from csv file. </p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>Union[str, Path]</code> <p>filename</p> required <p>Returns:</p> Name Type Description <code>metadata</code> <code>Dict</code> <p>metadata dictionary</p> Source code in <code>src/merfish3danalysis/utils/dataio.py</code> <pre><code>def read_metadatafile(fname: Union[str,Path]) -&gt; dict:\n    \"\"\"Read metadata from csv file. \n\n    Parameters\n    ----------\n    fname: Union[str,Path]\n        filename\n\n    Returns\n    -------\n    metadata: Dict\n        metadata dictionary\n    \"\"\"\n\n    scan_data_raw_lines = []\n\n    with open(fname, \"r\") as f:\n        for line in f:\n            scan_data_raw_lines.append(line.replace(\"\\n\", \"\"))\n\n    titles = scan_data_raw_lines[0].split(\",\")\n\n    # convert values to appropriate datatypes\n    vals = scan_data_raw_lines[1].split(\",\")\n    for ii in range(len(vals)):\n        if re.fullmatch(r\"\\d+\", vals[ii]):\n            vals[ii] = int(vals[ii])\n        elif re.fullmatch(r\"\\d*.\\d+\", vals[ii]):\n            vals[ii] = float(vals[ii])\n        elif vals[ii].lower() == \"False\".lower():\n            vals[ii] = False\n        elif vals[ii].lower() == \"True\".lower():\n            vals[ii] = True\n        else:\n            # otherwise, leave as string\n            pass\n\n    # convert to dictionary\n    metadata = {}\n    for t, v in zip(titles, vals):\n        metadata[t] = v\n\n    return metadata\n</code></pre>"},{"location":"reference/modules/dataio/#merfish3danalysis.utils.dataio.return_data_zarr","title":"<code>return_data_zarr(dataset_path, ch_idx, ch_idx_offset=0)</code>","text":"<p>Return NDTIFF data as a numpy array via tiffile.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>Union[Path, str]</code> <p>pycromanager dataset object</p> required <code>ch_idx</code> <code>int</code> <p>channel index in ZarrTiffStore file</p> required <code>ch_idx_offset</code> <code>Optional[int]</code> <p>channel index offset for unused phase channels</p> <code>0</code> <p>Returns:</p> Name Type Description <code>data</code> <code>ArrayLike</code> <p>data stack</p> Source code in <code>src/merfish3danalysis/utils/dataio.py</code> <pre><code>def return_data_zarr(dataset_path: Union[Path,str],\n                     ch_idx : int,\n                     ch_idx_offset: Optional[int] = 0) -&gt; ArrayLike:\n    \"\"\"Return NDTIFF data as a numpy array via tiffile.\n\n    Parameters\n    ----------\n    dataset_path: Dataset\n        pycromanager dataset object\n    ch_idx: int\n        channel index in ZarrTiffStore file\n    ch_idx_offset: int\n        channel index offset for unused phase channels\n\n    Returns\n    -------\n    data: ArrayLike\n        data stack\n    \"\"\"\n\n    ndtiff_zarr_store = imread(dataset_path, mode='r+', aszarr=True)\n    ndtiff_zarr = zarr.open(ndtiff_zarr_store, mode='r+')\n    first_dim = str(ndtiff_zarr.attrs['_ARRAY_DIMENSIONS'][0])\n\n    if first_dim == 'C':\n        data = np.asarray(ndtiff_zarr[ch_idx-ch_idx_offset, :],dtype=np.uint16)\n    else:\n        data = np.asarray(ndtiff_zarr[:,ch_idx-ch_idx_offset,:],dtype=np.uint16)\n    del ndtiff_zarr_store, ndtiff_zarr\n\n    return np.squeeze(data)\n</code></pre>"},{"location":"reference/modules/dataio/#merfish3danalysis.utils.dataio.time_stamp","title":"<code>time_stamp()</code>","text":"<p>Generate timestamp string.</p> <p>Returns:</p> Name Type Description <code>timestamp</code> <code>str</code> <p>timestamp formatted as string</p> Source code in <code>src/merfish3danalysis/utils/dataio.py</code> <pre><code>def time_stamp():\n    \"\"\"Generate timestamp string.\n\n    Returns\n    -------\n    timestamp: str\n        timestamp formatted as string\n    \"\"\"\n\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"reference/modules/dataio/#merfish3danalysis.utils.dataio.write_metadata","title":"<code>write_metadata(data_dict, save_path)</code>","text":"<p>Write dictionary as CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>dict</code> <p>metadata dictionary</p> required <code>save_path</code> <code>Union[str, Path]</code> <p>path for file</p> required Source code in <code>src/merfish3danalysis/utils/dataio.py</code> <pre><code>def write_metadata(\n    data_dict: dict, \n    save_path: Union[str,Path]\n):\n    \"\"\"Write dictionary as CSV file.\n\n    Parameters\n    ----------\n    data_dict: dict\n        metadata dictionary\n    save_path: Union[str,Path]\n        path for file\n    \"\"\"\n\n    pd.DataFrame([data_dict]).to_csv(save_path)\n</code></pre>"},{"location":"reference/modules/dataio/#merfish3danalysis.utils.dataio.write_sparse_mtx","title":"<code>write_sparse_mtx(output_dir_path, matrix, cells, features)</code>","text":"<p>Write sparse matrix in MTX format.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir_path</code> <code>Union[Path, str]</code> <p>Path to output directory</p> required <code>matrix</code> <code>ArrayLike</code> <p>Sparse matrix</p> required <code>cells</code> <code>Sequence[str]</code> <p>Cell names</p> required <code>features</code> <code>Sequence[str]</code> <p>Feature names</p> required Source code in <code>src/merfish3danalysis/utils/dataio.py</code> <pre><code>def write_sparse_mtx(output_dir_path : Union[Path,str], \n                     matrix: ArrayLike, \n                     cells: Sequence[str], \n                     features: Sequence[str]):\n    \"\"\"Write sparse matrix in MTX format.\n\n    Parameters\n    ----------\n    output_dir_path: Union[Path,str]\n        Path to output directory\n    matrix: ArrayLike\n        Sparse matrix\n    cells: Sequence[str]\n        Cell names\n    features: Sequence[str]\n        Feature names\n    \"\"\"\n\n    sparse_mat = sparse.coo_matrix(matrix.values)\n    sio.mmwrite(str(output_dir_path / \"matrix.mtx\"), sparse_mat)\n    write_tsv(output_dir_path / \"barcodes.tsv\", [\"cell_\" + str(cell) for cell in cells])\n    write_tsv(output_dir_path / \"features.tsv\", [[str(f), str(f), \"Blank Codeword\" if str(f).startswith(\"Blank\") else \"Gene Expression\"] for f in features])\n    subprocess.run(f\"gzip -f {str(output_dir_path)}/*\", shell=True)\n</code></pre>"},{"location":"reference/modules/dataio/#merfish3danalysis.utils.dataio.write_tsv","title":"<code>write_tsv(filename, data)</code>","text":"<p>Write data to TSV file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>Union[str, Path]</code> <p>Filename</p> required <code>data</code> <code>Sequence[Union[str, Sequence[str]]]</code> <p>Data to write</p> required Source code in <code>src/merfish3danalysis/utils/dataio.py</code> <pre><code>def write_tsv(\n    filename: Union[str, Path], \n    data: Sequence[Union[str, Sequence[str]]]\n):\n    \"\"\"Write data to TSV file.\n\n    Parameters\n    ----------\n    filename: Union[str, Path]\n        Filename\n    data: Sequence[Union[str, Sequence[str]]]\n        Data to write\n    \"\"\"\n\n    with open(filename, 'w', newline='') as tsvfile:\n        writer = csv.writer(tsvfile, delimiter='\\t', lineterminator='\\n')\n        for item in data:\n            writer.writerow([item] if isinstance(item, str) else item)\n</code></pre>"},{"location":"reference/modules/imageprocessing/","title":"Image Processing Module","text":"<p>Image processing functions for qi2lab 3D MERFISH.</p> <p>This module includes various utilities for image processing, such as downsampling, padding, and chunked GPU-based deconvolution.</p> History: <ul> <li>2024/12: Refactored repo structure.</li> <li>2024/07: Added numba-accelerated downsampling, padding helper functions,                and chunked GPU deconvolution.</li> </ul> <p>Functions:</p> Name Description <code>downsample_axis</code> <p>Numba accelerated downsampling for 3D images along a specified axis.</p> <code>downsample_image_anisotropic</code> <p>Numba accelerated anisotropic downsampling</p> <code>estimate_shading</code> <p>Estimate shading using stack of images and BaSiCPy.</p> <code>no_op</code> <p>Function to monkey patch print to suppress output.</p> <code>replace_hot_pixels</code> <p>Replace hot pixels with median values surrounding them.</p>"},{"location":"reference/modules/imageprocessing/#merfish3danalysis.utils.imageprocessing.downsample_axis","title":"<code>downsample_axis(image, level=2, axis=0)</code>","text":"<p>Numba accelerated downsampling for 3D images along a specified axis.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ArrayLike</code> <p>3D image to be downsampled.</p> required <code>level</code> <code>int</code> <p>Amount of downsampling.</p> <code>2</code> <code>axis</code> <code>int</code> <p>Axis along which to downsample (0, 1, or 2).</p> <code>0</code> <p>Returns:</p> Name Type Description <code>downsampled_image</code> <code>ArrayLike</code> <p>3D downsampled image.</p> Source code in <code>src/merfish3danalysis/utils/imageprocessing.py</code> <pre><code>@njit(parallel=True)\ndef downsample_axis(\n    image: ArrayLike, \n    level: int = 2, \n    axis: int = 0\n) -&gt; ArrayLike:\n    \"\"\"Numba accelerated downsampling for 3D images along a specified axis.\n\n    Parameters\n    ----------\n    image: ArrayLike\n        3D image to be downsampled.\n    level: int\n        Amount of downsampling.\n    axis: int\n        Axis along which to downsample (0, 1, or 2).\n\n    Returns\n    -------\n    downsampled_image: ArrayLike\n        3D downsampled image.\n\n    \"\"\"\n    if axis == 0:\n        new_length = image.shape[0] // level + (1 if image.shape[0] % level != 0 else 0)\n        downsampled_image = np.zeros(\n            (new_length, image.shape[1], image.shape[2]), dtype=image.dtype\n        )\n\n        for y in prange(image.shape[1]):\n            for x in range(image.shape[2]):\n                for z in range(new_length):\n                    sum_value = 0.0\n                    count = 0\n                    for j in range(level):\n                        original_index = z * level + j\n                        if original_index &lt; image.shape[0]:\n                            sum_value += image[original_index, y, x]\n                            count += 1\n                    if count &gt; 0:\n                        downsampled_image[z, y, x] = sum_value / count\n\n    elif axis == 1:\n        new_length = image.shape[1] // level + (1 if image.shape[1] % level != 0 else 0)\n        downsampled_image = np.zeros(\n            (image.shape[0], new_length, image.shape[2]), dtype=image.dtype\n        )\n\n        for z in prange(image.shape[0]):\n            for x in range(image.shape[2]):\n                for y in range(new_length):\n                    sum_value = 0.0\n                    count = 0\n                    for j in range(level):\n                        original_index = y * level + j\n                        if original_index &lt; image.shape[1]:\n                            sum_value += image[z, original_index, x]\n                            count += 1\n                    if count &gt; 0:\n                        downsampled_image[z, y, x] = sum_value / count\n\n    elif axis == 2:\n        new_length = image.shape[2] // level + (1 if image.shape[2] % level != 0 else 0)\n        downsampled_image = np.zeros(\n            (image.shape[0], image.shape[1], new_length), dtype=image.dtype\n        )\n\n        for z in prange(image.shape[0]):\n            for y in range(image.shape[1]):\n                for x in range(new_length):\n                    sum_value = 0.0\n                    count = 0\n                    for j in range(level):\n                        original_index = x * level + j\n                        if original_index &lt; image.shape[2]:\n                            sum_value += image[z, y, original_index]\n                            count += 1\n                    if count &gt; 0:\n                        downsampled_image[z, y, x] = sum_value / count\n\n    return downsampled_image\n</code></pre>"},{"location":"reference/modules/imageprocessing/#merfish3danalysis.utils.imageprocessing.downsample_image_anisotropic","title":"<code>downsample_image_anisotropic(image, level=(2, 6, 6))</code>","text":"<p>Numba accelerated anisotropic downsampling</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ArrayLike</code> <p>3D image to be downsampled</p> required <code>level</code> <code>tuple[int, int, int]</code> <p>anisotropic downsampling level</p> <code>(2, 6, 6)</code> <p>Returns:</p> Name Type Description <code>downsampled_image</code> <code>ArrayLike</code> <p>downsampled 3D image</p> Source code in <code>src/merfish3danalysis/utils/imageprocessing.py</code> <pre><code>def downsample_image_anisotropic(image: ArrayLike, level: tuple[int,int,int] = (2,6,6)) -&gt; ArrayLike:\n    \"\"\"Numba accelerated anisotropic downsampling\n\n    Parameters\n    ----------\n    image: ArrayLike\n        3D image to be downsampled\n    level: tuple[int,int,int], default=(2,6,6)\n        anisotropic downsampling level\n\n    Returns\n    -------\n    downsampled_image: ArrayLike\n        downsampled 3D image\n    \"\"\"\n\n    downsampled_image = downsample_axis(\n        downsample_axis(downsample_axis(image, level[0], 0), level[1], 1), level[2], 2\n    )\n\n    return downsampled_image\n</code></pre>"},{"location":"reference/modules/imageprocessing/#merfish3danalysis.utils.imageprocessing.estimate_shading","title":"<code>estimate_shading(images)</code>","text":"<p>Estimate shading using stack of images and BaSiCPy.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list[ArrayLike]</code> <p>4D image stack [p,z,y,x]</p> required <p>Returns:</p> Name Type Description <code>shading_image</code> <code>ArrayLike</code> <p>estimated shading image</p> Source code in <code>src/merfish3danalysis/utils/imageprocessing.py</code> <pre><code>def estimate_shading(\n    images: list[ArrayLike]\n) -&gt; ArrayLike:\n    \"\"\"Estimate shading using stack of images and BaSiCPy.\n\n    Parameters\n    ----------\n    images: ArrayLike\n        4D image stack [p,z,y,x]\n\n    Returns\n    -------\n    shading_image: ArrayLike\n        estimated shading image\n    \"\"\"\n\n    # GPU\n\n    import cupy as cp  # type: ignore\n    from cupyx.scipy import ndimage  # type: ignore\n    from basicpy import BaSiC # type: ignore\n\n    maxz_images = []\n    for image in images:\n        maxz_images.append(cp.squeeze(cp.max(image.result(),axis=0)))    \n\n    maxz_images = cp.asnumpy(maxz_images).astype(np.uint16)\n    gc.collect()\n    cp.cuda.Stream.null.synchronize()\n    cp.get_default_memory_pool().free_all_blocks()\n    cp.get_default_pinned_memory_pool().free_all_blocks()\n\n    original_print = builtins.print\n    builtins.print = no_op\n    basic = BaSiC(get_darkfield=False)\n    basic.autotune(maxz_images[:])\n    basic.fit(maxz_images[:])\n    builtins.print = original_print\n    shading_correction = basic.flatfield.astype(np.float32) / np.max(basic.flatfield.astype(np.float32),axis=(0,1))\n\n    del basic\n    gc.collect()\n\n    cp.cuda.Stream.null.synchronize()\n    cp.get_default_memory_pool().free_all_blocks()\n    cp.get_default_pinned_memory_pool().free_all_blocks()\n\n    return shading_correction\n</code></pre>"},{"location":"reference/modules/imageprocessing/#merfish3danalysis.utils.imageprocessing.no_op","title":"<code>no_op(*args, **kwargs)</code>","text":"<p>Function to monkey patch print to suppress output.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>positional arguments</p> <code>()</code> <code>kwargs</code> <p>keyword arguments</p> <code>{}</code> Source code in <code>src/merfish3danalysis/utils/imageprocessing.py</code> <pre><code>def no_op(*args, **kwargs):\n    \"\"\"Function to monkey patch print to suppress output.\n\n    Parameters\n    ----------\n    args: Any\n        positional arguments\n    kwargs: Any\n        keyword arguments\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/modules/imageprocessing/#merfish3danalysis.utils.imageprocessing.replace_hot_pixels","title":"<code>replace_hot_pixels(noise_map, data, threshold=375.0)</code>","text":"<p>Replace hot pixels with median values surrounding them.</p> <p>Parameters:</p> Name Type Description Default <code>noise_map</code> <code>ArrayLike</code> <p>darkfield image collected at long exposure time to get hot pixels</p> required <code>data</code> <code>ArrayLike</code> <p>ND data [broadcast_dim,z,y,x]</p> required <p>Returns:</p> Name Type Description <code>data</code> <code>ArrayLike</code> <p>hotpixel corrected data</p> Source code in <code>src/merfish3danalysis/utils/imageprocessing.py</code> <pre><code>def replace_hot_pixels(\n    noise_map: ArrayLike, \n    data: ArrayLike, \n    threshold: float = 375.0\n) -&gt; ArrayLike:\n    \"\"\"Replace hot pixels with median values surrounding them.\n\n    Parameters\n    ----------\n    noise_map: ArrayLike\n        darkfield image collected at long exposure time to get hot pixels\n    data: ArrayLike\n        ND data [broadcast_dim,z,y,x]\n\n    Returns\n    -------\n    data: ArrayLike\n        hotpixel corrected data\n    \"\"\"\n\n    # GPU\n    import cupy as cp  # type: ignore\n    from cupyx.scipy import ndimage  # type: ignore\n\n    data = cp.asarray(data, dtype=cp.float32)\n    noise_map = cp.asarray(noise_map, dtype=cp.float32)\n\n    # threshold darkfield_image to generate bad pixel matrix\n    hot_pixels = cp.squeeze(cp.asarray(noise_map))\n    hot_pixels[hot_pixels &lt;= threshold] = 0\n    hot_pixels[hot_pixels &gt; threshold] = 1\n    hot_pixels = hot_pixels.astype(cp.float32)\n    inverted_hot_pixels = cp.ones_like(hot_pixels) - hot_pixels.copy()\n\n    data = cp.asarray(data, dtype=cp.float32)\n    for z_idx in range(data.shape[0]):\n        median = ndimage.median_filter(data[z_idx, :, :], size=3)\n        data[z_idx, :] = inverted_hot_pixels * data[z_idx, :] + hot_pixels * median\n\n    data[data &lt; 0] = 0\n\n    data = cp.asnumpy(data).astype(np.uint16)\n    gc.collect()\n    cp.cuda.Stream.null.synchronize()\n    cp.get_default_memory_pool().free_all_blocks()\n    cp.get_default_pinned_memory_pool().free_all_blocks()\n\n    return data\n</code></pre>"},{"location":"reference/modules/registration/","title":"Image Registration Module","text":"<p>Image registration functions using cucim, scikit-image, and SimpleITK.</p> <p>This module contains functions for image registration leveraging tools like cucim, scikit-image, and SimpleITK, optimized for use with qi2lab 3D MERFISH data.</p> History: <ul> <li>2025/07: <ul> <li>Changed to anisotropic downsampling for registration.</li> <li>Changed to GPU-accelerated pixel warping strategy </li> </ul> </li> <li>2024/12: Refactored repo structure.</li> <li>2024/07: Prepared to remove all Dask usage and integrate functions                into the DataRegistration class as static methods.</li> <li>2024/01: Updated for qi2lab MERFISH file format v0.1.</li> <li>2023/07: Initial commit.</li> </ul> <p>Functions:</p> Name Description <code>apply_transform</code> <p>Apply simpleITK transform</p> <code>compute_rigid_transform</code> <p>Calculate initial translation transform using scikit-image</p> <code>compute_warpfield</code> <p>Compute the warpfield to warp a target image to a reference image.</p>"},{"location":"reference/modules/registration/#merfish3danalysis.utils.registration.apply_transform","title":"<code>apply_transform(image1, image2, transform)</code>","text":"<p>Apply simpleITK transform</p> <p>Parameters:</p> Name Type Description Default <code>image1</code> <code>ArrayLike</code> <p>reference image</p> required <code>image2</code> <code>ArrayLike</code> <p>moving image</p> required <code>transform</code> <code>Transform</code> <p>simpleITK transform object</p> required <p>Returns:</p> Name Type Description <code>resampled_image</code> <code>ArrayLike</code> <p>transformed moving image</p> Source code in <code>src/merfish3danalysis/utils/registration.py</code> <pre><code>def apply_transform(\n    image1: ArrayLike, \n    image2: ArrayLike,\n    transform: sitk.Transform\n) -&gt; ArrayLike:\n    \"\"\"\n    Apply simpleITK transform\n\n    Parameters\n    ----------\n    image1: ArrayLike\n        reference image\n    image2: ArrayLike\n        moving image\n    transform: sitk.Transform\n        simpleITK transform object\n\n    Returns\n    -------\n    resampled_image: ArrayLike\n        transformed moving image\n    \"\"\"\n\n    image1_sitk = sitk.GetImageFromArray(image1)\n    image2_sitk = sitk.GetImageFromArray(image2)\n\n    # Resample the moving image\n    resampler = sitk.ResampleImageFilter()\n    resampler.SetReferenceImage(image1_sitk)  # The fixed image is the reference\n    resampler.SetInterpolator(sitk.sitkLinear)\n    resampler.SetDefaultPixelValue(0)\n    resampler.SetTransform(transform)  # Use the transform from the registration\n\n    # Apply the transform to the moving image\n    resampled_image = resampler.Execute(image2_sitk)\n\n    del image1_sitk, image2_sitk\n    gc.collect()\n\n    return sitk.GetArrayFromImage(resampled_image).astype(np.float32)\n</code></pre>"},{"location":"reference/modules/registration/#merfish3danalysis.utils.registration.compute_rigid_transform","title":"<code>compute_rigid_transform(image1, image2, downsample_factors=[2, 6, 6], mask=None, projection=None, gpu_id=0)</code>","text":"<p>Calculate initial translation transform using scikit-image phase cross correlation. Create simpleITK transform using shift.</p> <p>Parameters:</p> Name Type Description Default <code>image1</code> <code>ArrayLike</code> <p>reference image</p> required <code>image2</code> <code>ArrayLike</code> <p>moving image</p> required <code>downsample_factor</code> <p>amount of zyx downsampling applied before calling registration</p> required <code>use_mask</code> <p>use provided mask</p> required <code>projection</code> <code>Optional[str]</code> <p>projection method to use</p> <code>None</code> <p>Returns:</p> Name Type Description <code>transform</code> <code>simpleITK transform</code> <p>translation transform</p> <code>shift_xyz</code> <code>Sequence[float]</code> <p>xyz shifts in pixels</p> Source code in <code>src/merfish3danalysis/utils/registration.py</code> <pre><code>def compute_rigid_transform(\n    image1: ArrayLike, \n    image2: ArrayLike,\n    downsample_factors: list[int,int,int] = [2,6,6],\n    mask: Optional[ArrayLike] = None,\n    projection: Optional[str] = None,\n    gpu_id: int = 0\n) -&gt; Tuple[sitk.TranslationTransform,Sequence[float]]:\n    \"\"\"\n    Calculate initial translation transform using scikit-image\n    phase cross correlation. Create simpleITK transform using shift.\n\n    Parameters\n    ----------\n    image1: ArrayLike\n        reference image\n    image2: ArrayLike\n        moving image\n    downsample_factor: list[int,int,int], default = [2,6,6]\n        amount of zyx downsampling applied before calling registration\n    use_mask: Optional[ArrayLike], default None\n        use provided mask \n    projection: Optional[str], default None\n        projection method to use\n\n    Returns\n    -------\n    transform: simpleITK transform\n        translation transform\n    shift_xyz: Sequence[float]\n        xyz shifts in pixels\n    \"\"\"\n    import cupy as cp # type: ignore\n\n    with cp.cuda.Device(gpu_id):\n        from cucim.skimage.registration import phase_cross_correlation # type: ignore\n        from cucim.skimage.metrics import structural_similarity # type: ignore\n\n        if projection is not None:\n            if projection == 'z':\n                image1 = np.squeeze(np.max(image1,axis=0))\n                image2 = np.squeeze(np.max(image2,axis=0))\n            elif projection == 'y':\n                image1 = np.squeeze(np.max(image1,axis=1))\n                image2 = np.squeeze(np.max(image2,axis=1))\n\n        if projection == 'search':\n\n            image1_cp = cp.asarray(image1)\n            ref_slice_idx = image1_cp.shape[0]//2\n            ref_slice = image1_cp[ref_slice_idx,:,:]\n            image2_cp = cp.asarray(image2)\n            ssim = []\n            for z_idx in range(image1.shape[0]):\n                ssim_slice = structural_similarity(ref_slice.astype(cp.float32),\n                                                image2_cp[z_idx,:].astype(cp.float32),\n                                                data_range=1.0)\n                ssim.append(cp.asnumpy(ssim_slice))\n\n            ssim = np.array(ssim)\n            print(f\"SSIM: {ssim}\")\n            found_shift = float(ref_slice_idx - np.argmax(ssim))\n            print(f\"Found shift: {found_shift}\")\n            del image1_cp, image2_cp, ssim_slice, ssim\n\n        else:\n            # Perform Fourier cross-correlation\n\n            if mask is not None:\n                shift_cp, _, _ = phase_cross_correlation(reference_image=cp.asarray(image1), \n                                                        moving_image=cp.asarray(image2),\n                                                        upsample_factor=10,\n                                                        reference_mask=mask,\n                                                        disambiguate=True)\n            else:\n                shift_cp, _, _ = phase_cross_correlation(reference_image=cp.asarray(image1), \n                                                        moving_image=cp.asarray(image2),\n                                                        upsample_factor=10,\n                                                        disambiguate=True)\n            shift = cp.asnumpy(shift_cp)\n            del shift_cp\n\n        # Convert the shift to a list of doubles\n        if projection is not None:\n            if projection == 'z':\n                shift_xyz = [shift[1]*downsample_factors[2],\n                            shift[0]*downsample_factors[1],\n                            0.]\n            elif projection == 'y':\n                shift_xyz = [shift_reversed[0],\n                            0.,\n                            shift_reversed[1]]\n            elif projection == 'search':\n                shift_xyz = [0.,0.,downsample_factors[0]*found_shift]\n        else:\n            for i in range(len(shift)):\n                if downsample_factors[i] &gt; 1:\n                    shift[i] = -1*float(shift[i] * downsample_factors[i])\n                else:\n                    shift[i] = -1*float(shift[i])\n            shift_reversed = shift[::-1]\n            shift_xyz = shift_reversed\n\n        gc.collect()\n\n        # Synchronize to make sure FFT work finished\n        cp.cuda.Stream.null.synchronize()\n\n        # Clear BOTH FFT plan caches for the *current* device\n        try:\n            cp.fft.config.get_plan_cache().clear()\n        except Exception:\n            pass\n        try:\n            import cupyx\n            cupyx.scipy.fft.clear_plan_cache()\n        except Exception:\n            pass\n\n        # Now free CuPy memory pools (after plans are cleared)\n        cp.get_default_memory_pool().free_all_blocks()\n        cp.get_default_pinned_memory_pool().free_all_blocks()\n\n        # Create an affine transform with the shift from the cross-correlation\n        try:\n            transform = sitk.TranslationTransform(3, shift_xyz)\n        except:\n            transform = None\n\n        return transform, shift_xyz\n</code></pre>"},{"location":"reference/modules/registration/#merfish3danalysis.utils.registration.compute_warpfield","title":"<code>compute_warpfield(img_ref, img_trg, gpu_id=0)</code>","text":"<p>Compute the warpfield to warp a target image to a reference image.</p> <p>Parameters:</p> Name Type Description Default <code>img_ref</code> <code>ArrayLike</code> <p>reference image</p> required <code>img_trg</code> <code>ArrayLike</code> <p>moving image</p> required <code>gpu_id</code> <code>int</code> <p>GPU ID to use for computation</p> <code>0</code> <p>Returns:</p> Name Type Description <code>warp_field</code> <code>ArrayLike</code> <p>warpfield matrix</p> Source code in <code>src/merfish3danalysis/utils/registration.py</code> <pre><code>def compute_warpfield(\n    img_ref: ArrayLike, \n    img_trg: ArrayLike,\n    gpu_id: int = 0\n) -&gt; Tuple[ArrayLike, ArrayLike, ArrayLike, ArrayLike]:\n\n    \"\"\"\n    Compute the warpfield to warp a target image to a reference image.\n\n    Parameters\n    ----------\n    img_ref: ArrayLike\n        reference image\n    img_trg: ArrayLike\n        moving image\n    gpu_id: int, default 0\n        GPU ID to use for computation\n\n    Returns\n    -------\n    warp_field: ArrayLike\n        warpfield matrix\n    \"\"\"\n    import cupy as cp\n\n    cp.cuda.Device(gpu_id).use()\n\n    from warpfield import Recipe, register_volumes\n\n    recipe = Recipe() # initialized with a translation level, followed by an affine registration level\n    recipe.pre_filter.clip_thresh = 160 # clip DC background, if present\n    recipe.pre_filter.soft_edge = [4, 32, 32]\n\n    # affine level properties\n    recipe.levels[-1].repeats = 0\n\n    if max(img_ref.shape) &gt; 2048:\n        recipe.add_level(block_size=[21, 73, 73])\n        recipe.levels[-1].block_stride = 0.75\n        recipe.levels[-1].smooth.sigmas = [1., 3.0, 3.0]\n        recipe.levels[-1].smooth.long_range_ratio = 0.1\n        recipe.levels[-1].repeats = 2\n\n        recipe.add_level(block_size=[5, 17, 17])\n        recipe.levels[-1].block_stride = 0.75\n        recipe.levels[-1].smooth.sigmas = [1.5, 5.0, 5.0]\n        recipe.levels[-1].smooth.long_range_ratio = 0.1\n        recipe.levels[-1].repeats = 2\n    else:\n        recipe.add_level(block_size=[15, 45, 45])\n        recipe.levels[-1].block_stride = 0.75\n        recipe.levels[-1].smooth.sigmas = [1., 3.0, 3.0]\n        recipe.levels[-1].smooth.long_range_ratio = 0.1\n        recipe.levels[-1].repeats = 2\n\n        recipe.add_level(block_size=[3, 9, 9])\n        recipe.levels[-1].block_stride = 0.75\n        recipe.levels[-1].smooth.sigmas = [1.5, 5.0, 5.0]\n        recipe.levels[-1].smooth.long_range_ratio = 0.1\n        recipe.levels[-1].repeats = 2\n\n    warped_image, warp_map, _ = register_volumes(\n        ref = img_ref, \n        vol = img_trg, \n        recipe = recipe,\n        verbose = False,\n        gpu_id = gpu_id,\n    )\n    warped_image = cp.asnumpy(warped_image).astype(np.float32)\n    warp_field = cp.asnumpy(warp_map.warp_field).astype(np.float32)\n    block_size = cp.asnumpy(warp_map.block_size).astype(np.float32)\n    block_stride = cp.asnumpy(warp_map.block_stride).astype(np.float32)\n\n    del warp_map\n    gc.collect()\n    cp.cuda.Stream.null.synchronize()\n    cp.get_default_memory_pool().free_all_blocks()\n    cp.get_default_pinned_memory_pool().free_all_blocks()\n\n    return (warped_image, warp_field, block_size, block_stride)\n</code></pre>"}]}