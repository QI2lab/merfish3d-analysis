{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to merfish3d-analysis Documentation","text":"<p>GPU accelerated post-processing for 2D or 3D iterative barcoded FISH data. This package currently Nvidia only and Linux only due to RAPIDS.AI package availabilty.</p> <p>WARNING: alpha software. We are sharing this early in case it is useful to other groups. Please expect breaking changes.</p>"},{"location":"#motivation","title":"Motivation","text":"<p>Iterative multiplexing experiments, such as MERFISH, typically involve 6D data. These dimensions are <code>[rounds,tile,channel,z,y,x]</code> and require significant processing across each dimension to go from raw data to quality controlled transcript 3D localizations and 3D cell outlines.</p> <p>Additionally, our laboratory, the Quantiative Imaging and Inference Laboratory (qi2lab), specializes in high-throughput 3D microscopy using custom microscopes. This includes purpose built high numerical aperture widefield and oblique plane microscopy platforms. While increased sampling provides more information on the sample, it introduces new challenges due to the increase in data density and more complicated MERFISH decoding inverse problem.</p> <p>To efficiently perform 3D MERFISH processing, we created this <code>merfish3d-analysis</code> package. The goal of the package is to aid researchers in rapidly and robustly turning gigabyte to petabyte level MERFISH data into decoded transcripts using chunked, compressed file formats and GPU-accelerated processing.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Decode both 2D and 3D iterative barcoded experiments that use a codebook. Our focus on 3D MERFISH, but this library can be extended to any iterative imaging and barcoded RNA imaging approach.</li> <li>Datastore optimized for large-scale imaging data.</li> <li>Read/Write compressed Zarr v2 using Tensorstore library for performance.</li> <li>Processing capabilities for widefield, standard light-sheet, and skewed light-sheet data.</li> <li>Rigid, affine, and deformable local tile registration.</li> <li>GPU-accelerated registration estimation combined with ITK for image warping.</li> <li>Rigid and affine global registration using multiview-stitcher</li> <li>GPU-accelerated image processing and decoding.</li> <li>Nearly all image processing functions utilize GPU acceleration through CuPy, CuCIM, CuVS, and custom CUDA kernels. All non-GPU accelerated functions are Numba accelerated.</li> <li>Larger-than-GPU-memory block computations are handled using Ryomen, a lightweight solution that avoids many issues with other distribution computing solutions.</li> <li>Iterative estimation of background and normalization vectors across codebook bits to remove subjective normalization by user that often leads to non-optimal decoding solutions.</li> <li>Integrated functionality to leverage machine learning tools such as Cellpose, Baysor, and U-FISH.</li> </ul>"},{"location":"#examples","title":"Examples","text":"<p>Multiple examples are provided with the library, including qi2lab data, Zhuang laboratory data, and synthetic data.</p>"},{"location":"#api-reference","title":"API reference","text":"<p>For more information, check out the API Reference.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>This is an early project and we welcome all contributions! The easiest way to get started is to open an issue. We are especially interested in open-source iterative multiplexing datasets that we can setup as examples for this library.</p>"},{"location":"datastore/","title":"qi2lab DataStore overview","text":""},{"location":"datastore/#philosophy","title":"Philosophy","text":"<p>To help efficiently handle the data complexity of 3D MERFISH experiments, we have created a dedicated zarr based datastore, using TensorStore for efficient reading and writing of image data and Parquet for tabular data. </p>"},{"location":"datastore/#important-considerations","title":"Important considerations","text":"<p>To create a <code>qi2labDataStore</code>, we need to know the following metadata: - the effective xy pixel size and z step - the objective numerical aperture - the immersion media refractive index - the global stage zyx position at each tile - the camera orientation with respect to the stage orientation - the direction of stage motion with respect the camera view - the bits that were collected in each round - the acquisition order in each tile (channel,z) or (z,channel) - the excitation and emission wavelengths for each channel</p> <p>Most of these are straightforward to obtain. The camera orientation and stage direction can be the trickiest. In our experience, one way to figure this out is to load a few tiles of the data in napari-stitcher and explore different orientations of the images and stage direction.</p> <p>Because there are so many different microscopes and microscope acquisition software, we rely on the user to provide the images in the correct orientaton such that a positive displacement in the global stage coordinates corresponds to a positive displacement in the image and vice-versa. In the Zhuang lab examples, we show how to determine the camera and stage orientations when the metadata is not available.</p>"},{"location":"datastore/#general-use","title":"General use","text":"<p>Here, we use a hypothetical dataset that only has one round with two bits. We assume the data is already gain, offset, and hot pixel corrected.</p> <p>Tiles and rounds are indexed from 0, keeping with python conventions. However, we always index rounds and codebook bits from 1, to avoid confusion.</p> <pre><code>from merfish3danalysis.qi2labDataStore import qi2labDataStore\nfrom pathlib import Path\n\n# define the datastore directory and create the datastore\nroot_path = Path(r\"/path/to/dataset/\")\ndatastore = qi2labDataStore(root_path / Path(\"qi2labdatastore\"))\n\n# required metadata\ndatastore.channels_in_data = [\"alexa488\",\"alexa561\",\"alexa647\"]\ndatastore.baysor_path = Path(r\"/path/to/Baysor/bin/baysor/bin/./baysor\")\ndatastore.baysor_options = Path(r\"/path/to/baysor_options.toml\")\ndatastore.julia_threads = 8\ndatastore.num_rounds = 1\ndatastore.codebook = Path(r\"/path/to/dataset/raw_data/codebook.csv\")\ndatastore.experiment_order = Path(r\"/path/to/dataset/raw_data/exp_order.csv\")\ndatastore.num_tiles = 1\ndatastore.microscope_type = \"3D\"\ndatastore.tile_overlap = 0.2\ndatastore.e_per_ADU = 0.51\ndatastore.na = 1.35\ndatastore.ri = 1.51\ndatastore.binning = 1\ndatastore.noise_map = None\ndatastore.channel_psfs = channel_psfs # either experimental or theoretical PSFs\ndatastore.voxel_size_zyx_um = [.31,.098,.098]\n\n# Update datastore state that Calibration are created\ndatastore_state = datastore.datastore_state\ndatastore_state.update({\"Calibrations\": True})\ndatastore.datastore_state = datastore_state\n\n# initialize the tile\ndatastore.initialize_tile(tile_idx)\n\n# save image data for tile = 0, round = 0, polyDT\ndatastore.save_local_corrected_image(\n    polyDT_data,\n    tile=0,\n    psf_idx=0,\n    gain_correction=True,\n    hotpixel_correction=True,\n    shading_correction=False,\n    round=0,\n)\n\n# save stage position for tile = 0, round = 0, polyDT\ndatastore.save_local_stage_position_zyx_um(\n    [1000., 200., 500.], tile=0, round=0\n)\n\n# save excitation and emission wavelengths for tile = 0, round = 0, polyDT\n# this position is used for any bits linked to this round\ndatastore.save_local_wavelengths_um(\n    (.488, .520),\n    tile=0,\n    round=0,\n)\n\n# save first readout channel for tile = 0, bit_idx = 1\ndatastore.save_local_corrected_image(\n    bit001_data,\n    tile=0,\n    psf_idx=1,\n    gain_correction=True,\n    hotpixel_correction=True,\n    shading_correction=False,\n    bit=1,\n)\n\n# save excitation and emission wavelengths for tile = 0, bit_idx = 1\ndatastore.save_local_wavelengths_um(\n    (.561, .590),\n    tile=0,\n    bit=1,\n)\n\n# save second readout channel for tile = 0, bit_idx = 2\ndatastore.save_local_corrected_image(\n    bit002_data,\n    tile=0,\n    psf_idx=2,\n    gain_correction=True,\n    hotpixel_correction=True,\n    shading_correction=False,\n    bit=2,\n)\n\n# save excitation and emission wavelengths for tile = 0, bit_idx = 2\ndatastore.save_local_wavelengths_um(\n    (.635, .670),\n    tile=0,\n    bit=2,\n)\n\n# update datastore state that corrected data is saved \ndatastore_state = datastore.datastore_state\ndatastore_state.update({\"Corrected\": True})\ndatastore.datastore_state = datastore_state\n</code></pre>"},{"location":"datastore/#datastore-structure","title":"DataStore structure","text":"<pre><code>/experiment \n\u251c\u2500\u2500 raw_data/ \n  \u2514\u2500\u2500 &lt;data&gt; (raw experimental data and metadata)\n\u251c\u2500\u2500 qi2labdatastore/ \n    \u251c\u2500\u2500 datastore.json (information on the state of the datastore)\n    \u251c\u2500\u2500 calibrations.zarr/ (calibration information)\n    \u251c\u2500\u2500 .zattrs\n      \u251c\u2500\u2500 &lt;exp_codebook&gt;\n      \u251c\u2500\u2500 &lt;exp_order&gt;\n      \u2514\u2500\u2500 &lt;exp_codebook&gt;\n    \u251c\u2500\u2500 camera_noise_map/ (camera noise map array)\n    \u2514\u2500\u2500 psf_data/ (psf arrays)\n  \u251c\u2500\u2500 polyDT/ (raw and processed data for polyDT label)\n    \u251c\u2500\u2500 tile0000/\n      \u251c\u2500\u2500 round0000.zarr/\n        \u251c\u2500\u2500 .zattrs\n          \u251c\u2500\u2500 &lt;stage_zyx_um&gt; (global stage position in zyx order; unit: microns)\n          \u251c\u2500\u2500 &lt;wavelengths_um&gt; (wavelength in (excitation,emission) order; unit: microns)\n          \u251c\u2500\u2500 &lt;voxel_size_zyx_um&gt; (voxel size in zyx order; unit: microns)\n          \u251c\u2500\u2500 &lt;bit_linker&gt; (what codebook bits are linked to this fidicual image)\n          \u251c\u2500\u2500 &lt;affine_zyx_um&gt; (4x4 affine matrix generated during global registration; unit: microns)\n          \u251c\u2500\u2500 &lt;origin_zyx_um&gt; (tile origin generated during global registration; unit: microns)\n          \u2514\u2500\u2500 &lt;spacing_zyx_um&gt; (voxel size used during global registration, this must match &lt;voxel_size_zyx_um&gt;; unit: microns)\n        \u251c\u2500\u2500 camera_data/ (gain and offset corrected data in zyx order)\n        \u251c\u2500\u2500 corrected_data/ (gain and offset corrected data in zyx order)\n        \u2514\u2500\u2500 registered_decon_data/ (deconvolved data in zyx order)\n      \u251c\u2500\u2500 round0001.zarr\n        \u251c\u2500\u2500 .zattrs\n          \u251c\u2500\u2500 &lt;stage_zyx_um&gt; (global stage position in zyx order; unit: microns)\n          \u251c\u2500\u2500 &lt;wavelengths_um&gt; (wavelength in (excitation,emission) order; unit: microns)\n          \u251c\u2500\u2500 &lt;voxel_size_zyx_um&gt; (voxel size in zyx order; unit: microns)\n          \u251c\u2500\u2500 &lt;bit_linker&gt; (what codebook bits are linked to this fidicual image)\n          \u251c\u2500\u2500 &lt;affine_zyx_um&gt; (4x4 affine matrix generated during global registration; unit: microns)\n          \u251c\u2500\u2500 &lt;origin_zyx_um&gt; (tile origin generated during global registration; unit: microns)\n          \u2514\u2500\u2500 &lt;spacing_zyx_um&gt; (voxel size used during global registration, this must match &lt;voxel_size_zyx_um&gt;; unit: microns)\n        \u251c\u2500\u2500 camera_data/ (gain and offset corrected data in zyx order)\n        \u251c\u2500\u2500 corrected_data/ (gain and offset corrected data in zyx order)\n        \u251c\u2500\u2500 of_xyz_3x_downsample/ (3x downsampled optical flow field for round 0 alginment in pixels)\n        \u251c\u2500\u2500 registered_decon_data/ (deconvolved, registered back to round 0 image data in zyx order)\n        \u251c\u2500\u2500 ... \n        \u2514\u2500\u2500 roundNNNN.zarr/\n    \u251c\u2500\u2500 tile0001/\n    \u251c\u2500\u2500 tile0002/\n    \u251c\u2500\u2500 ...\n    \u2514\u2500\u2500 tileNNNN/\n  \u251c\u2500\u2500 readouts/ (raw and processed data for MERFISH bits)\n    \u251c\u2500\u2500 tile0001/\n      \u251c\u2500\u2500 bit000.zarr/\n        \u251c\u2500\u2500 .zattrs\n          \u251c\u2500\u2500 &lt;stage_zyx_um&gt; (global stage position in zyx order; unit: microns)\n          \u251c\u2500\u2500 &lt;wavelengths_um&gt; (wavelength in (excitation,emission) order; unit: microns)\n          \u251c\u2500\u2500 &lt;voxel_size_zyx_um&gt; (voxel size in zyx order; unit: microns)\n          \u2514\u2500\u2500 &lt;round_linker&gt; (what fidicual round is linked to this bit image) \n        \u251c\u2500\u2500 camera_data/\n        \u251c\u2500\u2500 correcte_data/\n        \u251c\u2500\u2500 registered_decon_data/\n        \u2514\u2500\u2500 registered_ufish_data/\n      \u251c\u2500\u2500 bit001.zarr/\n      \u251c\u2500\u2500 ...\n      \u2514\u2500\u2500 bitNNN.zarr/\n\n</code></pre>"},{"location":"datastore/#datastore-api","title":"DataStore API","text":"<p>Nearly all parameters are accessible as class properties and all data has helper functions for reading and writing. The full API reference is available at qi2labDataStore.</p>"},{"location":"installation/","title":"Install","text":""},{"location":"installation/#python-environment-creation","title":"Python environment creation","text":"<p>Because <code>merfish3d-analysis</code> relies on a number of GPU-only functions, we strongly recommend that you create a specific python environment to ensure proper functioning.</p> <p>Here are instructions using Mamba, a very fast implementation of <code>conda</code>.</p> <p>Create a python 3.10 environment using your favorite package manager, e.g. <code>mamba create -n merfish3d python=3.10</code></p>"},{"location":"installation/#installing-merfish3d-analysis","title":"Installing merfish3d-analysis","text":"<p>Activate the environment and install the GPU dependencies. This install method assumes an Nvidia GPU capable of running CUDA &gt;= 12.0.</p> <pre><code>mamba activate merfish3d\nmamba install -c conda-forge -c nvidia -c pytorch -c rapidsai cupy cucim=24.08 pylibraft=24.08 raft-dask=24.08 cudadecon \"cuda-version&gt;=12.0,&lt;=12.5\" cudnn cutensor nccl onnx onnxruntime pytorch torchvision 'pytorch=*=*cuda*'\n</code></pre> <p>Finally, clone the repository using <code>git clone https://github.com/QI2lab/merfish3d-analysis</code> and install using <code>pip install .</code>. For interactive editing use <code>pip install -e .</code>.</p> <p>To build the documentation, install using <code>pip install .[docs]</code>. Then execute <code>mkdocs build --clean</code> and <code>mkdocs serve</code>. The documentation is available in your web browser at <code>http://127.0.0.1:8000/</code>.</p>"},{"location":"installation/#installing-baysor","title":"Installing Baysor","text":"<p>Please follow the Baysor documentation to install for Linux. Keep track of the installation directory for use with <code>merfish3d-analysis</code>.</p>"},{"location":"examples/qi2lab_human_olfactory_bulb/","title":"qi2lab human olfactory bubl example","text":""},{"location":"examples/qi2lab_human_olfactory_bulb/#overview","title":"Overview","text":"<p>The goal of this example is to retrieve a 3D MERFISH experiment generate by the qi2lab at ASU from the Brain Image Library and run merfish3d-analysis through quantifying transcripts and assigning transcripts to cells. This is a 119-gene MERFISH and 2-gene smFISH experiment on post-mortem human olfactory bulb tissue.</p>"},{"location":"examples/qi2lab_human_olfactory_bulb/#preliminaries","title":"Preliminaries","text":"<p>You need to make sure you have a working python enviornment with <code>merfish3d-analysis</code> properly installed, <code>Baysor</code> properly installed, and our human olfactory bulb 3D MERFISH dataset downloaded. The dataset is ~800 GB and you will need roughly another 800 GB of temporary space to create the <code>qi2labDataStore</code> structure we use to perform tile registration, global registration, segmentation, pixel decoding, filtering, and cell assignment.</p> <ul> <li>merfish3d-analysis</li> <li>Baysor</li> <li>Raw 3D MERFISH data</li> </ul>"},{"location":"examples/qi2lab_human_olfactory_bulb/#downloaded-data","title":"Downloaded data","text":"<p>All of the required code to process this data is in the BIL download. There should be three top-level directories in the downloaded folder, <code>raw_data</code>, <code>processing_code</code>, and <code>results</code>. The directory structure is as follows:</p> <p>/  \u251c\u2500\u2500 raw_data/  \u2502 \u251c\u2500\u2500 data_r0001_tile000_xyz/ \u2502 \u251c\u2500\u2500 data_r0001_tile000_xyz.csv | ... | ... \u2502 \u251c\u2500\u2500 data_r0009_tile041_xyz/ \u2502 \u251c\u2500\u2500 data_r0009_tile041_xyz.csv \u2502 \u251c\u2500\u2500 codebook.csv \u2502 \u251c\u2500\u2500 bit_order.csv \u2502 \u251c\u2500\u2500 hot_pixel_flir.tiff \u2502 \u2514\u2500\u2500 scan_metadata.csv \u251c\u2500\u2500 processing_code/  \u2502 \u251c\u2500\u2500 00_readme.txt  \u2502 \u2514\u2500\u2500 01_convert_to_datastore.py \u2502 \u2514\u2500\u2500 02_register_and_deconvolve.py \u2502 \u2514\u2500\u2500 03_cellpose_segmentation.py \u2502 \u2514\u2500\u2500 04_pixeldecode_and_baysor.py \u2502 \u2514\u2500\u2500 qi2lab_humanOB.toml \u2514\u2500\u2500 results/ \u2502 \u2514\u2500\u2500 transcripts.parquet \u2502 \u2514\u2500\u2500 cell_outlines.gzip \u2502 |\u2500\u2500 mtx/ | \u2502 \u2514\u2500\u2500 .gzip | \u2502 \u2514\u2500\u2500 .gzip | \u2502 \u2514\u2500\u2500 .gzip</p>"},{"location":"examples/qi2lab_human_olfactory_bulb/#processing-steps","title":"Processing steps","text":"<p>For each of the python files in the <code>processing_code</code> directory, you will need to scroll to the bottom and replace the path with the correct path. For example, in <code>01_convert_to_datastore.py</code> you'll want to change this section:</p> <pre><code>if __name__ == \"__main__\":\n    root_path = Path(r\"/path/to/download/raw_data\")\n    baysor_binary_path = Path(\n        r\"/path/to/Baysor/bin/baysor/bin/./baysor\"\n    )\n    baysor_options_path = Path(\n        r\"/path/to/download/processing_code/qi2lab_humanOB.toml\"\n    )\n    julia_threads = 20 # replace with number of threads to use.\n\n    hot_pixel_image_path = Path(r\"/path/to/download/raw_data/flir_hot_pixel_image.tif\")\n\n    convert_data(\n        root_path=root_path,\n        baysor_binary_path=baysor_binary_path,\n        baysor_options_path=baysor_options_path,\n        julia_threads=julia_threads,\n        hot_pixel_image_path=hot_pixel_image_path\n    )\n</code></pre> <p>For all of the files in <code>processing_code</code>, you'll set the <code>root_path</code> to <code>root_path = Path(r\"/path/to/download/raw_data\")</code>. The package automatically places the datastore within that directory.</p> <p>Once that is done, you can run <code>01_convert_to_datastore.py</code> and <code>02_register_and_deconolve.py</code> without any interactions. Depending on your computing hardware, you should expect 1-2 hours for <code>01_convert_to_datastore.py</code> and a couple days for <code>02_register_and_deconolve.py</code>. The second file is compute intensive, because it deconvolves 3 channel z-stacks at each tile over 9 hours, runs U-FISH for 2 MERFISH (or smFISH) channel at each tile, performs a rigid, affine, and optical flow field registration across imaging rounds, and globally registers the fidicual channel.</p> <p>Once <code>02_register_and_deconolve.py</code> is finished, you will need to create the correct cellpose settings. We have found that initially peforming cellpose segmentation on a downsampled and maximum Z projected polyDT image is sufficient to seed Baysor for segmentation refinement.</p> <p>If the standard \"cyto3\" model does not work for you data, you may need to retrain the cellpose model according the cellpose documentation and pass that to the code. Please see the API documentation for how to perform that step. Given satisfactory cellpose settings, you fill them in at the bottom of <code>03_cell_segmentation.py</code>,</p> <pre><code>if __name__ == \"__main__\":\n    root_path = Path(r\"/path/to/download/raw_data\")\n    cellpose_parameters = {\n        'normalization' : [0.5,95.0],\n        'blur_kernel_size' : 2.0,\n        'flow_threshold' : 0.4,\n        'cellprob_threshold' : 0.0,\n        'diameter': 36.57\n    }\n    run_cellpose(root_path, cellpose_parameters)\n</code></pre> <p>and then run <code>03_cell_segmentation.py</code>. This will only take a few minutes to generate the initial 2D segmentation guess.</p> <p>Next, you'll run <code>04_pixeldecode_and_baysor.py</code> to first optimize the pixel-based decoding parameters on a subset of tiles, then perform pixel-based decoding for all tiles, filter the data to limit false positives, remove overlapping spots in adajacent spatial tiles, and finally re-segment the cell boundaries in 3D using Baysor. This step should again take ~1 day, depending on your hard disk and GPU configuration.</p> <p>For this dataset, we labeled a number of olfactory receptors. We exclude these genes during Baysor segmentation, because they are not highly informative on cell boundaries. Once Baysor has finished, the filtered transcripts are parsed again to assign the excluded olfactory receptors to the new 3D cell boundaries.</p>"},{"location":"examples/qi2lab_human_olfactory_bulb/#ensuring-a-sucessful-run","title":"Ensuring a sucessful run","text":"<p>We have included the proper output of the <code>merfish3d-analysis</code> package for this dataset on the BIL servers in the <code>results</code> directory. You should be able to compare your obtained results to ours to ensure that there were no computation errors. One common issue we have run into is if the optical flow registration fails due to processor specific instructions (specifically older processors that lack AVX2). Please contact us if you have this issue, we have a workaround during installation that we can provide to you. </p>"},{"location":"examples/statphysbio_synthetic/","title":"StatPhysBio synthetic data example","text":""},{"location":"examples/statphysbio_synthetic/#overview","title":"Overview","text":"<p>The goal of this example is to retrieve a simulated 3D MERFISH experiment generated by the statphysbio laboratroy at ASU and run merfish3d-analysis through quantifying transcripts. This is a synthetic 119-gene MERFISH experiment in a small field of view (FOV).</p>"},{"location":"examples/statphysbio_synthetic/#preliminaries","title":"Preliminaries","text":"<p>You need to make sure you have a working python enviornment with <code>merfish3d-analysis</code> properly installed, <code>Baysor</code> properly installed, and the synthetic dataset downloaded. The dataset is ~100 MB and you will need roughly another 100 MB of temporary space to create the <code>qi2labDataStore</code> structure we use to perform tile registration, global registration, segmentation, pixel decoding, filtering, and cell assignment.</p> <ul> <li>merfish3d-analysis</li> <li>Baysor</li> <li>Synthetic 3D MERFISH data</li> </ul>"},{"location":"examples/statphysbio_synthetic/#downloading-the-data","title":"Downloading the data","text":"<p>All of the required code to process this data is in the Zenodo download. There should be three top-level directories in the downloaded folder, <code>raw_data</code>, <code>processing_code</code>, and <code>results</code>. The directory structure is as follows:</p> <p>/  \u251c\u2500\u2500 raw_data/  \u2502 \u251c\u2500\u2500 data_r0001_tile000_xyz/ \u2502 \u251c\u2500\u2500 data_r0001_tile000_xyz.csv | ... | ... \u2502 \u251c\u2500\u2500 data_r0009_tile041_xyz/ \u2502 \u251c\u2500\u2500 data_r0009_tile041_xyz.csv \u2502 \u251c\u2500\u2500 codebook.csv \u2502 \u251c\u2500\u2500 bit_order.csv \u2502 \u251c\u2500\u2500 hot_pixel_flir.tiff \u2502 \u251c\u2500\u2500 scan_metadata.csv \u251c\u2500\u2500 processing_code/  \u2502 \u251c\u2500\u2500 00_readme.txt  \u2502 \u2514\u2500\u2500 01_convert_to_datastore.py \u2502 \u2514\u2500\u2500 02_register_and_deconvolve.py \u2502 \u2514\u2500\u2500 03_cellpose_segmentation.py \u2502 \u2514\u2500\u2500 04_pixeldecode_and_baysor.py \u2502 \u2514\u2500\u2500 qi2lab_humanOB.toml \u2514\u2500\u2500 results/ \u2502 \u2514\u2500\u2500 transcripts.parquet \u2502 \u2514\u2500\u2500 cell_outlines.gzip \u2502 |\u2500\u2500 mtx/ | \u2502 \u2514\u2500\u2500 .gzip | \u2502 \u2514\u2500\u2500 .gzip | \u2502 \u2514\u2500\u2500 .gzip</p>"},{"location":"examples/statphysbio_synthetic/#processing-non-qi2lab-data","title":"Processing non-qi2lab data","text":"<p>Because this is a simulated experiment, it does not follow the standard metadata or file structure of a microscope. </p>"},{"location":"examples/statphysbio_synthetic/#processing-steps","title":"Processing steps","text":"<p>For each of the python files in the <code>processing_code</code> directory, you will need to scroll to the bottom and replace the path with the correct path. For example, in <code>01_convert_to_datastore.py</code> you'll want to change this section:</p> <pre><code>if __name__ == \"__main__\":\n    root_path = Path(r\"/path/to/download/raw_data\")\n    baysor_binary_path = Path(\n        r\"/path/to/Baysor/bin/baysor/bin/./baysor\"\n    )\n    baysor_options_path = Path(\n        r\"/path/to/download/processing_code/qi2lab_humanOB.toml\"\n    )\n    julia_threads = 20 # replace with number of threads to use.\n\n    hot_pixel_image_path = Path(r\"/path/to/download/raw_data/flir_hot_pixel_image.tif\")\n\n    convert_data(\n        root_path=root_path,\n        baysor_binary_path=baysor_binary_path,\n        baysor_options_path=baysor_options_path,\n        julia_threads=julia_threads,\n        hot_pixel_image_path=hot_pixel_image_path\n    )\n</code></pre> <p>For all of the files in <code>processing_code</code>, you'll set the <code>root_path</code> to <code>root_path = Path(r\"/path/to/download/raw_data\")</code>. The package automatically places the datastore within that directory.</p> <p>Once that is done, you can run <code>01_convert_to_datastore.py</code> and <code>02_register_and_deconolve.py</code> without any interactions. Depending on your computing hardware, you should expect 1-2 hours for <code>01_convert_to_datastore.py</code> and a couple days for <code>02_register_and_deconolve.py</code>. The second file is compute intensive, because it deconvolves 3 channel z-stacks at each tile over 9 hours, runs U-FISH for 2 MERFISH (or smFISH) channel at each tile, performs a rigid, affine, and optical flow field registration across imaging rounds, and globally registers the fidicual channel.</p> <p>Once <code>02_register_and_deconolve.py</code> is finished, you will need to create the correct cellpose settings. We have found that initially peforming cellpose segmentation on a downsampled and maximum Z projected polyDT image is sufficient to seed Baysor for segmentation refinement.</p> <p>Next, you'll run <code>03_pixeldecode_and_baysor.py</code> to first optimize the pixel-based decoding parameters on a subset of tiles, then perform pixel-based decoding for all tiles, and filter the data to limit false positives. This step should again take a few minutes, depending on your hard disk and GPU configuration.</p>"},{"location":"examples/statphysbio_synthetic/#ensuring-a-sucessful-run","title":"Ensuring a sucessful run","text":"<p>We have included the proper output of the <code>merfish3d-analysis</code> package for this dataset on the BIL servers in the <code>results</code> directory. You should be able to compare your obtained results to ours to ensure that there were no computation errors. One common issue we have run into is if the optical flow registration fails due to processor specific instructions (specifically older processors that lack AVX2). Please contact us if you have this issue, we have a workaround during installation that we can provide to you. </p>"},{"location":"examples/zhuang_lab_mouse_brain/","title":"Zhuang laboratory mouse brain example","text":""},{"location":"examples/zhuang_lab_mouse_brain/#overview","title":"Overview","text":"<p>The goal of this example is to run <code>merfish3d-analysis</code> on an existing 2D MERFISH dataset generated by the Zhuang laboratory at Harvard. They graciously deposisted their nearly raw data on BIL and we can adapt the fully featured functionality of our package to re-process their data. This is a 22-bit MERFISH experiment that has been pre-registered across rounds within each tile.</p>"},{"location":"examples/zhuang_lab_mouse_brain/#preliminaries","title":"Preliminaries","text":"<p>You need to make sure you have a working python environment with <code>merfish3d-analysis</code> properly installed, <code>Baysor</code> properly installed, and the Zhuang laboratory mouse brain 2D MERFISH dataset downloaded. The dataset is ~760 GB and you will need roughly another 700 GB of temporary space to create the <code>qi2labDataStore</code> structure we use to perform tile registration, global registration, segmentation, pixel decoding, filtering, and cell assignment. </p> <p>We are only going to analyze one of their samples, specifically <code>mouse1_sample1_raw</code>.</p> <ul> <li>merfish3d-analysis</li> <li>Baysor</li> <li>Raw 2D MERFISH data<ul> <li>Download the <code>additional_files</code> folder, <code>mouse1_sample1_raw</code> folder, and .</li> </ul> </li> </ul>"},{"location":"examples/zhuang_lab_mouse_brain/#downloading-the-data","title":"Downloading the data","text":"<p>All of the required code to process this data is in the examples/zhuang_lab folder on <code>merfish3d-analysis</code> github repo. After downloading the data from BIL, there should be the following data structure on the disk:</p> <pre><code>/mop\n  \u251c\u2500\u2500 mouse1_sample1_raw/\n    \u251c\u2500\u2500 aligned_images1.tif\n    \u251c\u2500\u2500 aligned_images2.tif\n    \u251c\u2500\u2500 ...\n    \u2514\u2500\u2500 aligned_images447.tif\n  \u251c\u2500\u2500 additional_files/\n    \u251c\u2500\u2500 codebook.csv\n    \u251c\u2500\u2500 data_organization_raw.csv\n    \u251c\u2500\u2500 microscope.json\n    \u251c\u2500\u2500 fov_positions/\n      \u2514\u2500\u2500 mouse1_sample1.txt\n  \u2514\u2500\u2500 zhuang_decoded_codewords/\n    \u2514\u2500\u2500 spots_mouse1sample1.csv\n</code></pre>"},{"location":"examples/zhuang_lab_mouse_brain/#processing-non-qi2lab-data","title":"Processing non-qi2lab data","text":"<p>Because this data is generated by a different custom microscope design with a unique microscope control package and is already pre-processed, we have to manually write most of the data conversion steps. Please see the DataStore page for more information on the key parameters that are requred to create a <code>qi2labDataStore</code>.</p> <p>The data in this BIL dataset consists of six small section of mouse brain, all separated on the slide. Looking at the <code>fov_positions/mouse1_sample1.txt</code> file, we can find where there are discontinuous jumps in the stage positions. Those jumps indicate the stage moved to a new brain section. </p> <p>A key issue with this data is the stage direction and camera orientation are flipped, which can be quite confusing when trying to figure out how the tiles are spatially related to each other. Do account for this and correctly register the data, we have to swap the xy axes. This can be accomplish by swap the last two axes of the tif,</p> <pre><code>flipped_image = np.swapaxes(raw_image,axes=(-2,-1))\n</code></pre> <p>Another difference in this data is that the spacing between z-planes is 1.5 microns, quite a bit larger than the Nyquist sampling of ~0.3 microns given the objective NA. It does not make sense to perform 3D deconvolution or decoding for this large of a sampling, so each z plane is deconvolved and decoded as independent from the surrounding planes. At the end, we collapse decoded transcripts that show up in adajacent Z planes to the transcript with the largest brightness.</p> <p>Finally, much of the metadata information we need (refractive index, numerical aperture, wavelengths, camera specifications, etc...) is only easily available via the publication, Spatially resolved cell atlas of the mouse primary motor cortex by MERFISH. We have noted in the conversion script where we had to look up these values or find values from equipment suppliers.</p> <p>While we have already created the data conversion code for this example, please reach out with questions if the process is not clear.</p>"},{"location":"examples/zhuang_lab_mouse_brain/#processing-steps","title":"Processing steps","text":"<p>For each of the python files in the <code>examples/zhuang_lab</code> directory, you will need to scroll to the bottom and replace the path with the correct path. For example, in <code>01_convert_to_datastore.py</code> you'll want to change this section:</p> <pre><code>if __name__ == \"__main__\":\n    root_path = Path(r\"/path/to/download/mop\")\n    baysor_binary_path = Path(\n        r\"/path/to/Baysor/bin/baysor/bin/./baysor\"\n    )\n    baysor_options_path = Path(\n        r\"/path/to/merfish3d-analysis/examples/zhuang_lab/zhuang_mouse.toml\"\n    )\n    julia_threads = 20 # replace with number of threads to use.\n\n    hot_pixel_image_path = None\n\n    convert_data(\n        root_path=root_path,\n        baysor_binary_path=baysor_binary_path,\n        baysor_options_path=baysor_options_path,\n        julia_threads=julia_threads,\n        hot_pixel_image_path=hot_pixel_image_path\n    )\n</code></pre> <p>For all of the files in <code>processing_code</code>, you'll set the <code>root_path</code> to <code>root_path = Path(r\"/path/to/download/mop\")</code>. The package automatically places the datastore within that directory, <code>/path/to/download/mop/qi2labdatastore</code>.</p> <p>Once that is done, you can run <code>01_convert_to_datastore.py</code> and <code>02_register_and_deconolve.py</code> without any interactions. Depending on your computing hardware, you should expect ~12 hours for <code>01_convert_to_datastore.py</code> and less than a week for <code>02_register_and_deconolve.py</code>. The second file is extremely compute intensive, because performs 2D deconvolution each z-plane of the 3 channel z-stacks at each tile, registers the polyDT channel back to the first round, runs U-FISH for the MERFISH bits in each tile, and finally performs global registration for all polyDT tiles in the first round. On a 12-core/24-thread AMD CPU workstation with 128 GB RAM and an Nvidia RTX 4080 24 GB GPU this step takes about 4 days to complete for all 441 tiles of shape <code>[40,6,2048,2408]</code> in the dataset.</p> <p>Once <code>02_register_and_deconolve.py</code> is finished, you will need to create the correct cellpose settings. We have found that initially peforming cellpose segmentation on a downsampled and maximum Z projected polyDT image is sufficient to seed Baysor for segmentation refinement.</p> <p>If the standard \"cyto3\" model does not work for you data, you may need to retrain the cellpose model and pass that to the code. Please see the API documentation for how to perform that step. Given satisfactory cellpose settings, you fill them in at the bottom of <code>03_cell_segmentation.py</code>,</p> <pre><code>if __name__ == \"__main__\":\n    root_path = Path(r\"/path/to/download/mop\")\n    cellpose_parameters = {\n        'normalization' : [0.5,95.0],\n        'blur_kernel_size' : 2.0,\n        'flow_threshold' : 0.4,\n        'cellprob_threshold' : 0.0,\n        'diameter': 36.57\n    }\n    run_cellpose(root_path, cellpose_parameters)\n</code></pre> <p>and then run <code>03_cell_segmentation.py</code>. This will only take a few minutes to generate the initial 2D segmentation guess. We rewrite the cell outlines using the ImageJ ROI file structure in global coordinates.</p> <p>Next, you'll run <code>04_pixeldecode_and_baysor.py</code> to first optimize the pixel-based decoding parameters on a subset of tiles, then perform pixel-based decoding for all tiles, filter the data to limit false positives, remove overlapping spots in adajacent spatial tiles, and finally re-segment the cell boundaries in 3D using Baysor. This step should take ~1 day, depending on your hard disk and GPU configuration.</p>"},{"location":"examples/zhuang_lab_mouse_brain/#ensuring-a-sucessful-run","title":"Ensuring a sucessful run","text":"<p>We have included the proper output of the <code>merfish3d-analysis</code> package for this dataset on the BIL servers in the <code>results</code> directory. You should be able to compare your obtained results to ours to ensure that there were no computation errors. One common issue we have run into is if the optical flow registration fails due to processor specific instructions (specifically older processors that lack AVX2). Please contact us if you have this issue, we have a workaround during installation that we can provide to you. </p>"},{"location":"reference/","title":"API Reference","text":"<p>Welcome to the API Reference for MERFISH3D Analysis.</p> <p>Select a module or class below for detailed documentation:</p>"},{"location":"reference/#classes","title":"Classes","text":"<ul> <li>DataRegistration</li> <li>PixelDecoder</li> <li>qi2labDataStore</li> </ul>"},{"location":"reference/#modules","title":"Modules","text":"<ul> <li>Data IO Module</li> <li>Image Processing Module</li> <li>OPM Tools Module</li> <li>Registration Module</li> </ul>"},{"location":"reference/classes/DataRegistration/","title":"DataRegistration Class","text":"<p>Register qi2lab 3D MERFISH data using cross-correlation and optical flow.</p> <p>This module enables the registration of MERFISH datasets by utilizing cross-correlation and optical flow techniques.</p> History: <ul> <li>2024/12: Refactor repo structure.</li> <li>2024/08:<ul> <li>Switched to qi2labdatastore for data access.</li> <li>Implemented numba-accelerated downsampling.</li> <li>Cleaned numpy usage for ryomen tiling.</li> </ul> </li> <li>2024/07: Integrated pycudadecon and removed Dask usage.</li> <li>2024/04: Updated for U-FISH, removed SPOTS3D.</li> <li>2024/01: Adjusted for qi2lab MERFISH file format v0.1.</li> <li>2023/09: Initial commit.</li> </ul>"},{"location":"reference/classes/DataRegistration/#merfish3danalysis.DataRegistration.DataRegistration","title":"<code>DataRegistration</code>","text":"<p>Register 2D or 3D MERFISH data across rounds.</p> <p>Parameters:</p> Name Type Description Default <code>datastore</code> <code>qi2labDataStore</code> <p>Initialized qi2labDataStore object</p> required <code>overwrite_registered</code> <code>bool</code> <p>Overwrite existing registered data and registrations</p> <code>False</code> <code>perform_optical_flow</code> <code>bool</code> <p>Perform optical flow registration</p> <code>False</code> <code>save_all_polyDT_registered</code> <code>bool</code> <p>Save fidicual polyDT rounds &gt; 1. These are not used for analysis.</p> <code>True</code> <code>decon_iters</code> <code>Optional[int]</code> <p>Deconvolution iterations</p> <code>10</code> <code>decon_background</code> <code>Optional[float]</code> <p>Background to substract during deconvolution</p> <code>50.0</code> Source code in <code>src/merfish3danalysis/DataRegistration.py</code> <pre><code>class DataRegistration:\n    \"\"\"Register 2D or 3D MERFISH data across rounds.\n\n    Parameters\n    ----------\n    datastore : qi2labDataStore\n        Initialized qi2labDataStore object\n    overwrite_registered: bool, default False\n        Overwrite existing registered data and registrations\n    perform_optical_flow: bool, default False\n        Perform optical flow registration\n    save_all_polyDT_registered: bool, default True\n        Save fidicual polyDT rounds &gt; 1. These are not used for analysis. \n    decon_iters : Optional[int], default 10\n        Deconvolution iterations\n    decon_background: Optional[float], default 50.0\n        Background to substract during deconvolution\n    \"\"\"\n\n    def __init__(\n        self,\n        datastore: qi2labDataStore,\n        overwrite_registered: bool = False,\n        perform_optical_flow: bool = False,\n        save_all_polyDT_registered: bool = True,\n        decon_iters: Optional[int] = 10,\n        decon_background: Optional[float] = 50.0,\n    ):\n\n        self._datastore = datastore\n        self._tile_ids = self._datastore.tile_ids\n        self._round_ids = self._datastore.round_ids\n        self._bit_ids = self._datastore.bit_ids\n        self._psfs = self._datastore.channel_psfs\n\n        self._perform_optical_flow = perform_optical_flow\n        self._data_raw = None\n        self._has_registered_data = None\n        self._overwrite_registered = overwrite_registered\n        self.save_all_polyDT_registered = save_all_polyDT_registered\n        self._decon_iters = decon_iters\n        self._decon_background = decon_background\n        self._original_print = builtins.print\n\n    # -----------------------------------\n    # property access for class variables\n    # -----------------------------------\n    @property\n    def datastore(self):\n        \"\"\"Return the qi2labDataStore object.\n\n        Returns\n        -------\n        qi2labDataStore\n            qi2labDataStore object\n        \"\"\"\n\n        if self._dataset_path is not None:\n            return self._datastore\n        else:\n            print(\"Datastore not defined.\")\n            return None\n\n    @datastore.setter\n    def dataset_path(self, value: qi2labDataStore):\n        \"\"\"Set the qi2labDataStore object.\n\n        Parameters\n        ----------\n        value : qi2labDataStore\n            qi2labDataStore object\n        \"\"\"\n\n        del self._datastore\n        self._datastore = value\n\n    @property\n    def tile_id(self):\n        \"\"\"Get the current tile id.\n\n        Returns\n        -------\n        tile_id: Union[int,str]\n            Tile id\n        \"\"\"\n\n        if self._tile_id is not None:\n            tile_id = self._tile_id\n            return tile_id\n        else:\n            print(\"Tile coordinate not defined.\")\n            return None\n\n    @tile_id.setter\n    def tile_id(self, value: Union[int,str]):\n        \"\"\"Set the tile id.\n\n        Parameters\n        ----------\n        value : Union[int,str]\n            Tile id\n        \"\"\"\n\n        if isinstance(value, int):\n            if value &lt; 0 or value &gt; self._datastore.num_tiles:\n                print(\"Set value index &gt;=0 and &lt;=\" + str(self._datastore.num_tiles))\n                return None\n            else:\n                self._tile_id = self._datastore.tile_ids[value]\n        elif isinstance(value, str):\n            if value not in self._datastore.tile_ids:\n                print(\"set valid tile id\")\n                return None\n            else:\n                self._tile_id = value\n\n    @property\n    def perform_optical_flow(self):\n        \"\"\"Get the perform_optical_flow flag.\n\n        Returns\n        -------\n        perform_optical_flow: bool\n            Perform optical flow registration\n        \"\"\"\n\n        return self._perform_optical_flow\n\n    @perform_optical_flow.setter\n    def perform_optical_flow(self, value: bool):\n        \"\"\"Set the perform_optical_flow flag.\n\n        Parameters\n        ----------\n        value : bool\n            Perform optical flow registration\n        \"\"\"\n\n        self._perform_optical_flow = value\n\n    @property\n    def overwrite_registered(self):\n        \"\"\"Get the overwrite_registered flag.\n\n        Returns\n        -------\n        overwrite_registered: bool\n            Overwrite existing registered data and registrations\n        \"\"\"\n\n        return self._overwrite_registered\n\n    @overwrite_registered.setter\n    def overwrite_registered(self, value: bool):\n        \"\"\"Set the overwrite_registered flag.\n\n        Parameters\n        ----------\n        value : bool\n            Overwrite existing registered data and registrations\n        \"\"\"\n\n        self._overwrite_registered = value\n\n    def register_all_tiles(self):\n        \"\"\"Helper function to register all tiles.\"\"\"\n        for tile_id in tqdm(self._datastore.tile_ids,desc=\"tiles\"):\n            self.tile_id=tile_id\n            self._load_raw_data()\n            self._generate_registrations()\n            self._apply_registration_to_bits()\n\n    def register_one_tile(self, tile_id: Union[int,str]):\n        \"\"\"Helper function to register one tile.\n\n        Parameters\n        ----------\n        tile_id : Union[int,str]\n            Tile id\n        \"\"\"\n\n        self.tile_id = tile_id\n        self._load_raw_data()\n        self._generate_registrations()\n        self._apply_registration_to_bits()\n\n    def _load_raw_data(self):\n        \"\"\"Load raw data across rounds for one tile.\"\"\"\n\n        self._data_raw = []\n        stage_positions = []\n\n        for round_id in self._round_ids:\n            self._data_raw.append(\n                self._datastore.load_local_corrected_image(\n                tile=self._tile_id,\n                round=round_id,\n                )\n            )\n\n            stage_positions.append(\n                self._datastore.load_local_stage_position_zyx_um(\n                    tile=self._tile_id,\n                    round=round_id\n                )\n            )\n\n        self._stage_positions = np.stack(stage_positions, axis=0)\n        del stage_positions\n        gc.collect()\n\n    def _generate_registrations(self):\n        \"\"\"Generate registered, deconvolved fiducial data and save to datastore.\"\"\"\n\n        test =  self._datastore.load_local_registered_image(\n            tile=self._tile_id,\n            round=self._round_ids[0]\n        )\n\n        if test is None:\n            has_reg_decon_data = False\n        else:\n            has_reg_decon_data = True\n\n        if not (has_reg_decon_data) or self._overwrite_registered:\n\n            ref_image_decon = chunked_cudadecon(\n                image=np.asarray(self._data_raw[0].result(),dtype=np.uint16),\n                psf=self._psfs[0, :],\n                image_voxel_zyx_um=self._datastore.voxel_size_zyx_um,\n                psf_voxel_zyx_um=self._datastore.voxel_size_zyx_um,\n                wavelength_um=self._datastore.load_local_wavelengths_um(\n                    tile=self._tile_id,\n                    round=self._round_ids[0])[1],\n                na=self._datastore.na,\n                ri=self._datastore.ri,\n                n_iters=self._decon_iters,\n                background=self._decon_background,\n            )\n\n            self._datastore.save_local_registered_image(\n                ref_image_decon,\n                tile=self._tile_id,\n                deconvolution=True,\n                round=self._round_ids[0]\n            )\n\n        for r_idx, round_id in enumerate(tqdm(self._round_ids[1:],desc=\"rounds\")):\n\n            test =  self._datastore.load_local_registered_image(\n                tile=self._tile_id,\n                round=round_id\n            )\n            if test is None:\n                has_reg_decon_data = False\n            else:\n                has_reg_decon_data = True\n\n            if not (has_reg_decon_data) or self._overwrite_registered:\n                try:\n                    temp = ref_image_decon[0:1,0:1,0:1].astype(np.float32)\n                    del temp\n                    gc.collect()\n                except FileNotFoundError :\n                    ref_image_decon = self._datastore.load_local_registered_image(\n                        tile=self._tile_id,\n                        round=self._round_ids[0],\n                        return_future=False\n                    )\n\n\n                mov_image_decon = chunked_cudadecon(\n                    image=np.asarray(\n                        self._data_raw[r_idx].result(),dtype=np.uint16\n                    ),\n                    psf=self._psfs[0, :],\n                    image_voxel_zyx_um=self._datastore.voxel_size_zyx_um,\n                    psf_voxel_zyx_um=self._datastore.voxel_size_zyx_um,\n                    wavelength_um=float(self._datastore.load_local_wavelengths_um(\n                        tile=self._tile_id,\n                        round=self._round_ids[0])[1]\n                    ),\n                    na=self._datastore.na,\n                    ri=self._datastore.ri,\n                    n_iters=self._decon_iters,\n                    background=self._decon_background,\n                )\n\n                downsample_factor = 2\n                if downsample_factor &gt; 1:\n                    ref_image_decon_ds = downsample_image_isotropic(\n                        ref_image_decon, downsample_factor\n                    )\n                    mov_image_decon_ds = downsample_image_isotropic(\n                        mov_image_decon, downsample_factor\n                    )\n                else:\n                    ref_image_decon_ds = ref_image_decon.copy()\n                    mov_image_decon_ds = mov_image_decon.copy()\n\n                _, initial_xy_shift = compute_rigid_transform(\n                    ref_image_decon_ds,\n                    mov_image_decon_ds,\n                    use_mask=True,\n                    downsample_factor=downsample_factor,\n                    projection=\"z\",\n                )\n\n                intial_xy_transform = sitk.TranslationTransform(3, initial_xy_shift)\n\n                mov_image_decon = apply_transform(\n                    ref_image_decon, mov_image_decon, intial_xy_transform\n                )\n\n                downsample_factor = 2\n                if downsample_factor &gt; 1:\n                    ref_image_decon_ds = downsample_image_isotropic(\n                        ref_image_decon, downsample_factor\n                    )\n                    mov_image_decon_ds = downsample_image_isotropic(\n                        mov_image_decon, downsample_factor\n                    )\n                else:\n                    ref_image_decon_ds = ref_image_decon.copy()\n                    mov_image_decon_ds = mov_image_decon.copy()\n\n                _, intial_z_shift = compute_rigid_transform(\n                    ref_image_decon_ds,\n                    mov_image_decon_ds,\n                    use_mask=False,\n                    downsample_factor=downsample_factor,\n                    projection=\"search\",\n                )\n\n                intial_z_transform = sitk.TranslationTransform(3, intial_z_shift)\n\n                mov_image_decon = apply_transform(\n                    ref_image_decon, mov_image_decon, intial_z_transform\n                )\n\n                downsample_factor = 4\n                if downsample_factor &gt; 1:\n                    ref_image_decon_ds = downsample_image_isotropic(\n                        ref_image_decon, downsample_factor\n                    )\n                    mov_image_decon_ds = downsample_image_isotropic(\n                        mov_image_decon, downsample_factor\n                    )\n                else:\n                    ref_image_decon_ds = ref_image_decon.copy()\n                    mov_image_decon_ds = mov_image_decon.copy()\n\n                _, xyz_shift_4x = compute_rigid_transform(\n                    ref_image_decon_ds,\n                    mov_image_decon_ds,\n                    use_mask=True,\n                    downsample_factor=downsample_factor,\n                    projection=None,\n                )\n\n                final_xyz_shift = (\n                    np.asarray(initial_xy_shift)\n                    + np.asarray(intial_z_shift)\n                    + np.asarray(xyz_shift_4x)\n                )\n                # final_xyz_shift = np.asarray(xyz_shift_4x)\n                self._datastore.save_local_rigid_xform_xyz_px(\n                    rigid_xform_xyz_px=final_xyz_shift,\n                    tile=self._tile_id,\n                    round=round_id\n                )\n\n                xyz_transform_4x = sitk.TranslationTransform(3, xyz_shift_4x)\n                mov_image_decon = apply_transform(\n                    ref_image_decon, mov_image_decon, xyz_transform_4x\n                )\n\n                if self._perform_optical_flow:\n                    downsample_factor = 3\n                    if downsample_factor &gt; 1:\n                        ref_image_decon_ds = downsample_image_isotropic(\n                            ref_image_decon, downsample_factor\n                        )\n                        mov_image_decon_ds = downsample_image_isotropic(\n                            mov_image_decon, downsample_factor\n                        )\n\n                    of_xform_px = compute_optical_flow(\n                        ref_image_decon_ds, \n                        mov_image_decon_ds\n                    )\n\n                    self._datastore.save_coord_of_xform_px(\n                        of_xform_px=of_xform_px,\n                        tile=self._tile_id,\n                        downsampling=[\n                            float(downsample_factor),\n                            float(downsample_factor),\n                            float(downsample_factor)],\n                        round=round_id\n                    )\n\n                    of_xform_sitk = sitk.GetImageFromArray(\n                        of_xform_px.transpose(1, 2, 3, 0).astype(np.float64),\n                        isVector=True,\n                    )\n                    interpolator = sitk.sitkLinear\n                    identity_transform = sitk.Transform(3, sitk.sitkIdentity)\n                    optical_flow_sitk = sitk.Resample(\n                        of_xform_sitk,\n                        sitk.GetImageFromArray(mov_image_decon),\n                        identity_transform,\n                        interpolator,\n                        0,\n                        of_xform_sitk.GetPixelID(),\n                    )\n                    displacement_field = sitk.DisplacementFieldTransform(\n                        optical_flow_sitk\n                    )\n                    del optical_flow_sitk, of_xform_px\n                    gc.collect()\n\n                    # apply optical flow\n                    mov_image_sitk = sitk.Resample(\n                        sitk.GetImageFromArray(mov_image_decon), \n                        displacement_field\n                    )\n\n                    data_registered = sitk.GetArrayFromImage(\n                        mov_image_sitk\n                    ).astype(np.float32)\n                    data_registered[data_registered &lt; 0.0] = 0\n                    data_registered = data_registered.astype(np.uint16)\n\n                    del mov_image_sitk, displacement_field\n                    gc.collect()\n                else:\n                    mov_image_decon[mov_image_decon &lt; 0.0] = 0\n                    data_registered = mov_image_decon.astype(np.uint16)\n\n                if self.save_all_polyDT_registered:\n                    self._datastore.save_local_registered_image(\n                        registered_image=data_registered.astype(np.uint16),\n                        tile=self._tile_id,\n                        deconvolution=True,\n                        round=round_id\n                    )\n\n                del data_registered\n                gc.collect()\n\n    def _apply_registration_to_bits(self):\n        \"\"\"Generate ufish + deconvolved, registered readout data and save to datastore.\"\"\"\n\n        for bit_idx, bit_id in enumerate(tqdm(self._bit_ids,desc='bits')):\n\n            r_idx = self._datastore.load_local_round_linker(\n                tile=self._tile_id,\n                bit=bit_id\n            )\n            r_idx = r_idx - 1\n            ex_wavelength_um, em_wavelength_um = self._datastore.load_local_wavelengths_um(\n                tile=self._tile_id,\n                bit=bit_id\n            )\n\n            # TO DO: hacky fix. Need to come up with a better way.\n            if ex_wavelength_um &lt; 600:\n                psf_idx = 1\n            else:\n                psf_idx = 2\n\n            test = self._datastore.load_local_registered_image(\n                tile=self._tile_id,\n                bit=bit_id\n            )\n\n            if test is None:\n                reg_decon_data_on_disk = False\n            else:\n                reg_decon_data_on_disk = True\n\n\n            if (not (reg_decon_data_on_disk) or self._overwrite_registered):\n\n                corrected_image = self._datastore.load_local_corrected_image(\n                    tile=self._tile_id,\n                    bit=bit_id,\n                    return_future=False,\n                )\n\n                decon_image = chunked_cudadecon(\n                    image=corrected_image,\n                    psf=self._psfs[psf_idx, :],\n                    image_voxel_zyx_um=self._datastore.voxel_size_zyx_um,\n                    psf_voxel_zyx_um=self._datastore.voxel_size_zyx_um,\n                    wavelength_um=em_wavelength_um,\n                    na=self._datastore.na,\n                    ri=self._datastore.ri,\n                    n_iters=self._decon_iters,\n                    background=self._decon_background,\n                )\n\n\n                if r_idx &gt; 0:\n                    rigid_xform_xyz_um = self._datastore.load_local_rigid_xform_xyz_px(\n                        tile=self._tile_id,\n                        round=self._round_ids[r_idx],\n                    )\n                    shift_xyz = [float(i) for i in rigid_xform_xyz_um]\n                    xyz_transform = sitk.TranslationTransform(3, shift_xyz)\n\n                    if self._perform_optical_flow:\n\n                        of_xform_px, _ = self._datastore.load_coord_of_xform_px(\n                            tile=self._tile_id,\n                            round=self._round_ids[r_idx],\n                            return_future=False\n                        )\n\n                        of_xform_sitk = sitk.GetImageFromArray(\n                            of_xform_px.transpose(1, 2, 3, 0).astype(np.float64),\n                            isVector=True,\n                        )\n\n                        interpolator = sitk.sitkLinear\n                        identity_transform = sitk.Transform(3, sitk.sitkIdentity)\n\n                        optical_flow_sitk = sitk.Resample(\n                            of_xform_sitk,\n                            sitk.GetImageFromArray(decon_image),\n                            identity_transform,\n                            interpolator,\n                            0,\n                            of_xform_sitk.GetPixelID(),\n                        )\n                        displacement_field = sitk.DisplacementFieldTransform(\n                            optical_flow_sitk\n                        )\n                        del optical_flow_sitk, of_xform_px\n                        gc.collect()\n\n                    decon_image_rigid = apply_transform(\n                        decon_image, \n                        decon_image, \n                        xyz_transform\n                    )\n                    del decon_image\n\n                    if self._perform_optical_flow:\n                        decon_bit_image_sitk = sitk.Resample(\n                            sitk.GetImageFromArray(decon_image_rigid), \n                            displacement_field\n                        )\n                        del displacement_field\n\n                        data_decon_registered = sitk.GetArrayFromImage(\n                            decon_bit_image_sitk\n                        ).astype(np.float32)\n                        del decon_bit_image_sitk\n                    else:\n                        data_decon_registered = decon_image_rigid.copy()\n                        del decon_image_rigid\n                    gc.collect()\n\n                else:\n                    data_decon_registered = decon_image.copy()\n                    del decon_image\n                    gc.collect()\n\n                data_decon_registered[data_decon_registered&lt;0.]=0.0\n\n                builtins.print = _no_op\n                ufish = UFish(device=\"cuda\")\n                ufish.load_weights_from_internet()\n\n                ufish_localization, ufish_data = ufish.predict(\n                    data_decon_registered, axes=\"zyx\", blend_3d=False, batch_size=8\n                )\n                builtins.print = self._original_print\n\n                ufish_localization = ufish_localization.rename(columns={\"axis-0\": \"z\"})\n                ufish_localization = ufish_localization.rename(columns={\"axis-1\": \"y\"})\n                ufish_localization = ufish_localization.rename(columns={\"axis-2\": \"x\"})\n\n                del ufish\n                gc.collect()\n\n                torch.cuda.empty_cache()\n                cp.get_default_memory_pool().free_all_blocks()\n                gc.collect()\n\n                roi_z, roi_y, roi_x = 7, 5, 5\n\n                def sum_pixels_in_roi(row, image, roi_dims):\n                    z, y, x = row[\"z\"], row[\"y\"], row[\"x\"]\n                    roi_z, roi_y, roi_x = roi_dims\n                    z_min, y_min, x_min = (\n                        max(0, z - roi_z // 2),\n                        max(0, y - roi_y // 2),\n                        max(0, x - roi_x // 2),\n                    )\n                    z_max, y_max, x_max = (\n                        min(image.shape[0], z_min + roi_z),\n                        min(image.shape[1], y_min + roi_y),\n                        min(image.shape[2], x_min + roi_x),\n                    )\n                    roi = image[\n                        int(z_min) : int(z_max),\n                        int(y_min) : int(y_max),\n                        int(x_min) : int(x_max),\n                    ]\n                    return np.sum(roi)\n\n                ufish_localization[\"sum_prob_pixels\"] = ufish_localization.apply(\n                    sum_pixels_in_roi,\n                    axis=1,\n                    image=ufish_data,\n                    roi_dims=(roi_z, roi_y, roi_x),\n                )\n                ufish_localization[\"sum_decon_pixels\"] = ufish_localization.apply(\n                    sum_pixels_in_roi,\n                    axis=1,\n                    image=data_decon_registered,\n                    roi_dims=(roi_z, roi_y, roi_x),\n                )\n\n                ufish_localization[\"tile_idx\"] = self._tile_ids.index(self._tile_id)\n                ufish_localization[\"bit_idx\"] = bit_idx + 1\n                ufish_localization[\"tile_z_px\"] = ufish_localization[\"z\"]\n                ufish_localization[\"tile_y_px\"] = ufish_localization[\"y\"]\n                ufish_localization[\"tile_x_px\"] = ufish_localization[\"x\"]\n\n                self._datastore.save_local_registered_image(\n                    data_decon_registered.astype(np.uint16),\n                    tile=self._tile_id,\n                    deconvolution=True,\n                    bit=bit_id\n                )\n                self._datastore.save_local_ufish_image(\n                    ufish_data,\n                    tile=self._tile_id,\n                    bit=bit_id\n                )\n                self._datastore.save_local_ufish_spots(\n                    ufish_localization,\n                    tile=self._tile_id,\n                    bit=bit_id\n                )\n\n                del (\n                    data_decon_registered,\n                    ufish_data,\n                    ufish_localization,\n                )\n                gc.collect()\n</code></pre>"},{"location":"reference/classes/DataRegistration/#merfish3danalysis.DataRegistration.DataRegistration.datastore","title":"<code>datastore</code>  <code>property</code>","text":"<p>Return the qi2labDataStore object.</p> <p>Returns:</p> Type Description <code>qi2labDataStore</code> <p>qi2labDataStore object</p>"},{"location":"reference/classes/DataRegistration/#merfish3danalysis.DataRegistration.DataRegistration.overwrite_registered","title":"<code>overwrite_registered</code>  <code>property</code> <code>writable</code>","text":"<p>Get the overwrite_registered flag.</p> <p>Returns:</p> Name Type Description <code>overwrite_registered</code> <code>bool</code> <p>Overwrite existing registered data and registrations</p>"},{"location":"reference/classes/DataRegistration/#merfish3danalysis.DataRegistration.DataRegistration.perform_optical_flow","title":"<code>perform_optical_flow</code>  <code>property</code> <code>writable</code>","text":"<p>Get the perform_optical_flow flag.</p> <p>Returns:</p> Name Type Description <code>perform_optical_flow</code> <code>bool</code> <p>Perform optical flow registration</p>"},{"location":"reference/classes/DataRegistration/#merfish3danalysis.DataRegistration.DataRegistration.tile_id","title":"<code>tile_id</code>  <code>property</code> <code>writable</code>","text":"<p>Get the current tile id.</p> <p>Returns:</p> Name Type Description <code>tile_id</code> <code>Union[int, str]</code> <p>Tile id</p>"},{"location":"reference/classes/DataRegistration/#merfish3danalysis.DataRegistration.DataRegistration.dataset_path","title":"<code>dataset_path(value)</code>","text":"<p>Set the qi2labDataStore object.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>qi2labDataStore</code> <p>qi2labDataStore object</p> required Source code in <code>src/merfish3danalysis/DataRegistration.py</code> <pre><code>@datastore.setter\ndef dataset_path(self, value: qi2labDataStore):\n    \"\"\"Set the qi2labDataStore object.\n\n    Parameters\n    ----------\n    value : qi2labDataStore\n        qi2labDataStore object\n    \"\"\"\n\n    del self._datastore\n    self._datastore = value\n</code></pre>"},{"location":"reference/classes/DataRegistration/#merfish3danalysis.DataRegistration.DataRegistration.register_all_tiles","title":"<code>register_all_tiles()</code>","text":"<p>Helper function to register all tiles.</p> Source code in <code>src/merfish3danalysis/DataRegistration.py</code> <pre><code>def register_all_tiles(self):\n    \"\"\"Helper function to register all tiles.\"\"\"\n    for tile_id in tqdm(self._datastore.tile_ids,desc=\"tiles\"):\n        self.tile_id=tile_id\n        self._load_raw_data()\n        self._generate_registrations()\n        self._apply_registration_to_bits()\n</code></pre>"},{"location":"reference/classes/DataRegistration/#merfish3danalysis.DataRegistration.DataRegistration.register_one_tile","title":"<code>register_one_tile(tile_id)</code>","text":"<p>Helper function to register one tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile_id</code> <code>Union[int, str]</code> <p>Tile id</p> required Source code in <code>src/merfish3danalysis/DataRegistration.py</code> <pre><code>def register_one_tile(self, tile_id: Union[int,str]):\n    \"\"\"Helper function to register one tile.\n\n    Parameters\n    ----------\n    tile_id : Union[int,str]\n        Tile id\n    \"\"\"\n\n    self.tile_id = tile_id\n    self._load_raw_data()\n    self._generate_registrations()\n    self._apply_registration_to_bits()\n</code></pre>"},{"location":"reference/classes/PixelDecoder/","title":"PixelDecoder Class","text":"<p>Perform pixel-based decoding for qi2lab widefield MERFISH data using GPU acceleration.</p> <p>This module leverages GPU acceleration to decode pixel-based widefield MERFISH datasets efficiently.</p> History: <ul> <li>2024/12: Refactor repo structure.</li> <li>2024/03: Reworked GPU logic to reduce out-of-memory crashes.</li> <li>2024/01: Updated for qi2lab MERFISH file format v1.0.</li> </ul>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder","title":"<code>PixelDecoder</code>","text":"<p>Retrieve and process one tile from qi2lab 3D widefield zarr structure. Normalize codebook and data, perform plane-by-plane pixel decoding, extract barcode features, and save to disk.</p> <p>Parameters:</p> Name Type Description Default <code>datastore</code> <code>qi2labDataStore</code> <p>qi2labDataStore object</p> required <code>merfish_bits</code> <code>int</code> <p>number of merfish bits. Assumes that in codebook, MERFISH rounds are [0,merfish_bits].</p> <code>16</code> <code>verbose</code> <code>int</code> <p>control verbosity. 0 - no output, 1 - tqdm bars, 2 - diagnostic outputs</p> <code>1</code> <code>use_mask</code> <code>Optional[bool]</code> <p>use mask stored in polyDT directory</p> <code>False</code> <code>z_range</code> <code>Optional[Sequence[int]]</code> <p>z range to analyze. In integer indices from [0,N] where N is number of z planes.</p> <code>None</code> <code>include_blanks</code> <code>Optional[bool]</code> <p>Include Blank codewords in decoding process.</p> <code>True</code> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>class PixelDecoder:\n    \"\"\"\n    Retrieve and process one tile from qi2lab 3D widefield zarr structure.\n    Normalize codebook and data, perform plane-by-plane pixel decoding,\n    extract barcode features, and save to disk.\n\n    Parameters\n    ----------\n    datastore: qi2labDataStore\n        qi2labDataStore object\n    merfish_bits: int, default 16\n        number of merfish bits. Assumes that in codebook, MERFISH rounds are [0,merfish_bits].\n    verbose: int, default 1\n        control verbosity. 0 - no output, 1 - tqdm bars, 2 - diagnostic outputs\n    use_mask: Optiona[bool], default False\n        use mask stored in polyDT directory\n    z_range: Optional[Sequence[int]], default None\n        z range to analyze. In integer indices from [0,N] where N is number of\n        z planes.\n    include_blanks: Optional[bool], default True\n        Include Blank codewords in decoding process.\n    \"\"\"\n\n    def __init__(\n        self,\n        datastore: qi2labDataStore,\n        merfish_bits: int = 16,\n        verbose: int = 1,\n        use_mask: Optional[bool] = False,\n        z_range: Optional[Sequence[int]] = None,\n        include_blanks: Optional[bool] = True,\n    ):\n        self._datastore = datastore\n        self._verbose = verbose\n        self._barcodes_filtered = False\n        self._include_blanks = include_blanks\n\n        self._n_merfish_bits = merfish_bits\n\n        if self._datastore.microscope_type == \"2D\":\n            self._is_3D = False\n        else:\n            self._is_3D = True\n        if z_range is None:\n            self._z_crop = False\n        else:\n            self._z_crop = True\n            self._z_range = [z_range[0], z_range[1]]\n\n        self._load_codebook()\n        self._decoding_matrix_no_errors = self._normalize_codebook(include_errors=False)\n        self._decoding_matrix = self._decoding_matrix_no_errors.copy()\n        self._barcode_count = self._decoding_matrix.shape[0]\n        self._bit_count = self._decoding_matrix.shape[1]\n\n        if use_mask:\n            self._load_mask()  # TO DO: implement\n        else:\n            self._mask_image = None\n\n        self._codebook_style = 1\n        self._optimize_normalization_weights = False\n        self._global_normalization_loaded = False\n        self._iterative_normalization_loaded = False\n        self._distance_threshold = 0.5172  # default for HW4D4 code. TO DO: calculate based on self._num_on-bits\n        self._magnitude_threshold = 0.9  # default for HW4D4 code\n\n    def _load_codebook(self):\n        \"\"\"Load and parse codebook into gene_id and codeword matrix.\"\"\"\n\n        self._df_codebook = self._datastore.codebook.copy()\n        self._df_codebook.fillna(0, inplace=True)\n\n        self._blank_count = (\n            self._df_codebook[\"gene_id\"].str.lower().str.startswith(\"blank\").sum()\n        )\n\n        if not (self._include_blanks):\n            self._df_codebook.drop(\n                self._df_codebook[self._df_codebook[0].str.startswith(\"Blank\")].index,\n                inplace=True,\n            )\n\n        self._codebook_matrix = self._df_codebook.iloc[:, 1:].to_numpy().astype(int)\n        self._gene_ids = self._df_codebook.iloc[:, 0].tolist()\n\n    def _normalize_codebook(self, include_errors: bool = False):\n        \"\"\"Normalize each codeword by L2 norm.\n\n        Parameters\n        ----------\n        include_errors : bool, default False\n            Include single-bit errors as unique barcodes in the decoding matrix.\"\"\"\n\n        self._barcode_set = cp.asarray(\n            self._codebook_matrix[:, 0 : self._n_merfish_bits]\n        )\n        magnitudes = cp.linalg.norm(self._barcode_set, axis=1, keepdims=True)\n        magnitudes[magnitudes == 0] = 1  # ensure with smFISH rounds have magnitude 1\n\n        if not include_errors:\n            # Normalize directly using broadcasting\n            normalized_barcodes = self._barcode_set / magnitudes\n            return cp.asnumpy(normalized_barcodes)\n        else:\n            # Pre-compute the normalized barcodes\n            normalized_barcodes = self._barcode_set / magnitudes\n\n            # Initialize an empty list to hold all barcodes with single errors\n            barcodes_with_single_errors = [normalized_barcodes]\n\n            # Generate single-bit errors\n            for bit_index in range(self._barcode_set.shape[1]):\n                flipped_barcodes = self._barcode_set.copy()\n                flipped_barcodes[:, bit_index] = 1 - flipped_barcodes[:, bit_index]\n                flipped_magnitudes = cp.sqrt(cp.sum(flipped_barcodes**2, axis=1))\n                flipped_magnitudes = cp.where(\n                    flipped_magnitudes == 0, 1, flipped_magnitudes\n                )\n                normalized_flipped = flipped_barcodes / flipped_magnitudes\n                barcodes_with_single_errors.append(normalized_flipped)\n\n            # Stack all barcodes (original normalized + with single errors)\n            all_barcodes = cp.vstack(barcodes_with_single_errors)\n            return cp.asnumpy(all_barcodes)\n\n    def _load_global_normalization_vectors(self):\n        \"\"\"Load or calculate global normalization and background vectors.\"\"\"\n        normalization_vector = self._datastore.global_normalization_vector\n        background_vector = self._datastore.global_background_vector\n        if normalization_vector is not None and background_vector is not None:\n            self._global_normalization_vector = cp.asarray(normalization_vector)\n            self._global_background_vector = cp.asarray(background_vector)\n            self._global_normalization_loaded = True\n        else:\n            self._global_normalization_vectors()\n\n    def _global_normalization_vectors(\n        self,\n        low_percentile_cut: float = 10.0,\n        high_percentile_cut: float = 90.0,\n        hot_pixel_threshold: int = 50000,\n    ):\n        \"\"\"Calculate global normalization and background vectors.\n\n        Parameters\n        ----------\n        low_percentile_cut : float, default 10.0\n            Lower percentile cut for background estimation.\n        high_percentile_cut : float, default 90.0\n            Upper percentile cut for normalization estimation.\n        hot_pixel_threshold : int, default 50000\n            Threshold for hot pixel removal.\n        \"\"\"\n\n        if len(self._datastore.tile_ids) &gt; 5:\n            random_tiles = sample(self._datastore.tile_ids, 5)\n        else:\n            random_tiles = self._datastore.tile_ids\n\n        normalization_vector = cp.ones(len(self._datastore.bit_ids), dtype=cp.float32)\n        background_vector = cp.zeros(len(self._datastore.bit_ids), dtype=cp.float32)\n\n        if self._verbose &gt;= 1:\n            print(\"calculate normalizations\")\n            iterable_bits = enumerate(\n                tqdm(self._datastore.bit_ids, desc=\"bit\", leave=False)\n            )\n        else:\n            iterable_bits = enumerate(self._datastore.bit_ids)\n\n        for bit_idx, bit_id in iterable_bits:\n            all_images = []\n\n            if self._verbose &gt;= 1:\n                iterable_tiles = tqdm(random_tiles, desc=\"loading tiles\", leave=False)\n            else:\n                iterable_tiles = random_tiles\n\n            for tile_id in iterable_tiles:\n                decon_image = self._datastore.load_local_registered_image(\n                    tile=tile_id, bit=bit_id, return_future=False\n                )\n                ufish_image = self._datastore.load_local_ufish_image(\n                    tile=tile_id, bit=bit_id, return_future=False\n                )\n\n                current_image = cp.where(\n                    cp.asarray(ufish_image, dtype=cp.float32) &gt; 0.1,\n                    cp.asarray(decon_image, dtype=cp.float32),\n                    0.0,\n                )\n                current_image[current_image &gt; hot_pixel_threshold] = cp.median(\n                    current_image[current_image.shape[0] // 2, :, :]\n                ).astype(cp.float32)\n                if self._z_crop:\n                    all_images.append(\n                        cp.asnumpy(\n                            current_image[self._z_range[0] : self._z_range[1], :]\n                        ).astype(np.float32)\n                    )\n                else:\n                    all_images.append(cp.asnumpy(current_image).astype(np.float32))\n                del current_image\n                cp.get_default_memory_pool().free_all_blocks()\n                gc.collect()\n\n            all_images = np.array(all_images)\n\n            if self._verbose &gt;= 1:\n                iterable_tiles = enumerate(\n                    tqdm(random_tiles, desc=\"background est.\", leave=False)\n                )\n            else:\n                iterable_tiles = random_tiles\n\n            low_pixels = []\n            for tile_idx, tile_id in iterable_tiles:\n                current_image = cp.asarray(all_images[tile_idx, :], dtype=cp.float32)\n                low_cutoff = cp.percentile(current_image, low_percentile_cut)\n                low_pixels.append(\n                    current_image[current_image &lt; low_cutoff]\n                    .flatten()\n                    .astype(cp.float32)\n                )\n                del current_image\n                cp.get_default_memory_pool().free_all_blocks()\n                gc.collect()\n\n            low_pixels = cp.concatenate(low_pixels, axis=0)\n            if low_pixels.shape[0] &gt; 0:\n                background_vector[bit_idx] = cp.median(low_pixels)\n            else:\n                background_vector[bit_idx] = 0\n\n            del low_pixels\n            cp.get_default_memory_pool().free_all_blocks()\n            gc.collect()\n\n            if self._verbose &gt;= 1:\n                iterable_tiles = enumerate(\n                    tqdm(random_tiles, desc=\"normalization est.\", leave=False)\n                )\n            else:\n                iterable_tiles = random_tiles\n\n            high_pixels = []\n            for tile_idx, tile_id in iterable_tiles:\n                current_image = (\n                    cp.asarray(all_images[tile_idx, :], dtype=cp.float32)\n                    - background_vector[bit_idx]\n                )\n                current_image[current_image &lt; 0] = 0\n                high_cutoff = cp.percentile(current_image, high_percentile_cut)\n                high_pixels.append(\n                    current_image[current_image &gt; high_cutoff]\n                    .flatten()\n                    .astype(cp.float32)\n                )\n\n                del current_image\n                cp.get_default_memory_pool().free_all_blocks()\n                gc.collect()\n\n            high_pixels = cp.concatenate(high_pixels, axis=0)\n            if high_pixels.shape[0] &gt; 0:\n                normalization_vector[bit_idx] = cp.median(high_pixels)\n            else:\n                normalization_vector[bit_idx] = 1\n\n            del high_pixels\n            cp.get_default_memory_pool().free_all_blocks()\n            gc.collect()\n\n        self._datastore.global_normalization_vector = (\n            cp.asnumpy(normalization_vector).astype(np.float32).tolist()\n        )\n        self._datastore.global_background_vector = (\n            cp.asnumpy(background_vector).astype(np.float32).tolist()\n        )\n\n        self._global_background_vector = background_vector\n        self._global_normalization_vector = normalization_vector\n        self._global_normalization_loaded = True\n\n    def _load_iterative_normalization_vectors(self):\n        \"\"\"Load or calculate iterative normalization and background vectors.\"\"\"\n        normalization_vector = self._datastore.iterative_normalization_vector\n        background_vector = self._datastore.iterative_background_vector\n\n        if normalization_vector is not None and background_vector is not None:\n            background_vector = np.nan_to_num(background_vector, 0.0)\n            normalization_vector = np.nan_to_num(normalization_vector, 1.0)\n            self._iterative_normalization_vector = cp.asarray(normalization_vector)\n            self._iterative_background_vector = cp.asarray(background_vector)\n            self._iterative_normalization_loaded = True\n        else:\n            self._iterative_normalization_vectors()\n\n    def _iterative_normalization_vectors(self):\n        \"\"\"Calculate iterative normalization and background vectors.\"\"\"\n        df_barcodes_loaded_no_blanks = self._df_barcodes_loaded[\n            ~self._df_barcodes_loaded[\"gene_id\"].str.startswith(\"Blank\")\n        ]\n\n        bit_columns = [\n            col\n            for col in df_barcodes_loaded_no_blanks.columns\n            if col.startswith(\"bit\") and col.endswith(\"_mean_intensity\")\n        ]\n\n        barcode_intensities = []\n        barcode_background = []\n        for index, row in df_barcodes_loaded_no_blanks.iterrows():\n            selected_columns = [\n                f'bit{row[\"on_bit_1\"]:02d}_mean_intensity',\n                f'bit{row[\"on_bit_2\"]:02d}_mean_intensity',\n                f'bit{row[\"on_bit_3\"]:02d}_mean_intensity',\n                f'bit{row[\"on_bit_4\"]:02d}_mean_intensity',\n            ]\n\n            selected_dict = {\n                col: (row[col] if col in selected_columns else None)\n                for col in bit_columns\n            }\n            not_selected_dict = {\n                col: (row[col] if col not in selected_columns else None)\n                for col in bit_columns\n            }\n\n            barcode_intensities.append(selected_dict)\n            barcode_background.append(not_selected_dict)\n\n        df_barcode_intensities = pd.DataFrame(barcode_intensities)\n        df_barcode_background = pd.DataFrame(barcode_background)\n\n        df_barcode_intensities = df_barcode_intensities.reindex(\n            sorted(df_barcode_intensities.columns), axis=1\n        )\n        df_barcode_background = df_barcode_background.reindex(\n            sorted(df_barcode_background.columns), axis=1\n        )\n\n        barcode_based_normalization_vector = np.round(\n            df_barcode_intensities.median(skipna=True).to_numpy(\n                dtype=np.float32, copy=True\n            ),\n            1,\n        )\n        barcode_based_background_vector = np.round(\n            df_barcode_background.median(skipna=True).to_numpy(\n                dtype=np.float32, copy=True\n            ),\n            1,\n        )\n\n        barcode_based_normalization_vector = np.nan_to_num(\n            barcode_based_normalization_vector, 1.0\n        )\n        barcode_based_normalization_vector = np.where(\n            barcode_based_normalization_vector == 0.0,\n            1.0,\n            barcode_based_normalization_vector,\n        )\n        barcode_based_background_vector = np.nan_to_num(\n            barcode_based_background_vector, 0.0\n        )\n\n        if (\n            self._iterative_background_vector is None\n            and self._iterative_normalization_vector is None\n        ):\n            old_iterative_background_vector = np.round(\n                cp.asnumpy(self._global_background_vector[0 : self._n_merfish_bits]), 1\n            )\n            old_iterative_normalization_vector = np.round(\n                cp.asnumpy(self._global_normalization_vector[0 : self._n_merfish_bits]),\n                1,\n            )\n        else:\n            old_iterative_background_vector = np.asarray(\n                cp.asnumpy(self._iterative_background_vector)\n            )\n            old_iterative_normalization_vector = np.asarray(\n                cp.asnumpy(self._iterative_normalization_vector)\n            )\n\n        diff_iterative_background_vector = np.round(\n            np.abs(barcode_based_background_vector - old_iterative_background_vector), 1\n        )\n        diff_iterative_normalization_vector = np.round(\n            np.abs(\n                barcode_based_normalization_vector - old_iterative_normalization_vector\n            ),\n            1,\n        )\n        self._datastore.iterative_background_vector = (\n            barcode_based_background_vector.astype(np.float32)\n        )\n        self._datastore.iterative_normalization_vector = (\n            barcode_based_normalization_vector.astype(np.float32)\n        )\n\n        if self._verbose &gt; 1:\n            print(\"---\")\n            print(\"Background\")\n            print(diff_iterative_background_vector)\n            print(barcode_based_background_vector)\n            print(\"Foreground\")\n            print(diff_iterative_normalization_vector)\n            print(barcode_based_normalization_vector)\n            print(\"---\")\n\n        self._iterative_normalization_vector = barcode_based_normalization_vector\n        self._iterative_background_vector = barcode_based_background_vector\n        self._datastore.iterative_normalization_vector = (\n            barcode_based_normalization_vector\n        )\n        self._datastore.iterative_background_vector = barcode_based_background_vector\n\n        self._iterative_normalization_loaded = True\n\n        del df_barcodes_loaded_no_blanks\n        gc.collect()\n\n    def _load_bit_data(self, ufish_threshold: Optional[float] = 0.5):\n        \"\"\"Load raw data for all bits in the tile.\n\n        Parameters\n        ----------\n        ufish_threshold : Optional[float], default 0.5\n            Threshold for ufish image.\n        \"\"\"\n\n        if self._verbose &gt; 1:\n            print(\"load raw data\")\n            iterable_bits = tqdm(\n                self._datastore.bit_ids[0 : self._n_merfish_bits],\n                desc=\"bit\",\n                leave=False,\n            )\n        elif self._verbose &gt;= 1:\n            iterable_bits = tqdm(\n                self._datastore.bit_ids[0 : self._n_merfish_bits],\n                desc=\"loading\",\n                leave=False,\n            )\n        else:\n            iterable_bits = self._datastore.bit_ids[0 : self._n_merfish_bits]\n\n        images = []\n        self._em_wvl = []\n        for bit_id in iterable_bits:\n            decon_image = self._datastore.load_local_registered_image(\n                tile=self._tile_idx,\n                bit=bit_id,\n            )\n            ufish_image = self._datastore.load_local_ufish_image(\n                tile=self._tile_idx,\n                bit=bit_id,\n            )\n\n            if self._z_crop:\n                current_mask = np.asarray(\n                    ufish_image[self._z_range[0] : self._z_range[1], :].result(),\n                    dtype=np.float32,\n                )\n                images.append(\n                    np.where(\n                        current_mask &gt; ufish_threshold,\n                        np.asarray(\n                            decon_image[\n                                self._z_range[0] : self._z_range[1], :\n                            ].result(),\n                            dtype=np.float32,\n                        ),\n                        0,\n                    )\n                )\n            else:\n                current_mask = np.asarray(ufish_image.result(), dtype=np.float32)\n                images.append(\n                    np.where(\n                        current_mask &gt; ufish_threshold,\n                        np.asarray(decon_image.result(), dtype=np.float32),\n                        0,\n                    )\n                )\n            self._em_wvl.append(\n                self._datastore.load_local_wavelengths_um(\n                    tile=self._tile_idx,\n                    bit=bit_id,\n                )[1]\n            )\n\n        self._image_data = np.stack(images, axis=0)\n        voxel_size_zyx_um = self._datastore.voxel_size_zyx_um\n        self._pixel_size = voxel_size_zyx_um[1]\n        self._axial_step = voxel_size_zyx_um[0]\n\n        affine, origin, spacing = self._datastore.load_global_coord_xforms_um(\n            tile=self._tile_idx\n        )\n        if affine is None or origin is None or spacing is None:\n            if self._is_3D:\n                affine = np.eye(4)\n                origin = self._datastore.load_local_stage_position_zyx_um(\n                    tile=self._tile_idx, round=0\n                )\n                spacing = self._datastore.voxel_size_zyx_um\n            else:\n                affine = np.eye(4)\n                origin = self._datastore.load_local_stage_position_zyx_um(\n                    tile=self._tile_idx, round=0\n                )\n                origin = [0, origin[0], origin[1]]\n                spacing = self._datastore.voxel_size_zyx_um\n\n        self._affine = affine\n        self._origin = origin\n        self._spacing = spacing\n\n        del images\n        gc.collect()\n\n    def _lp_filter(self, sigma=(3, 1, 1)):\n        \"\"\"Apply low-pass filter to the raw data.\n\n        Parameters\n        ----------\n        sigma : Tuple[int, int, int], default [3,1,1]\n            Sigma values for Gaussian filter.\n        \"\"\"\n\n        self._image_data_lp = self._image_data.copy()\n\n        if self._verbose &gt; 1:\n            print(\"lowpass filter\")\n            iterable_lp = tqdm(\n                range(self._image_data_lp.shape[0]), desc=\"bit\", leave=False\n            )\n        elif self._verbose &gt;= 1:\n            iterable_lp = tqdm(\n                range(self._image_data_lp.shape[0]), desc=\"lowpass\", leave=False\n            )\n        else:\n            iterable_lp = self._image_data_lp\n\n        for i in iterable_lp:\n            if self._is_3D:\n                image_data_cp = cp.asarray(self._image_data[i, :], dtype=cp.float32)\n                max_image_data = cp.asnumpy(\n                    cp.max(image_data_cp, axis=(0, 1, 2))\n                ).astype(np.float32)\n                if max_image_data == 0:\n                    self._image_data_lp[i, :, :, :] = 0\n                else:\n                    self._image_data_lp[i, :, :, :] = cp.asnumpy(\n                        gaussian_filter(image_data_cp, sigma=sigma)\n                    ).astype(np.float32)\n                    max_image_data_lp = np.max(\n                        self._image_data_lp[i, :, :, :], axis=(0, 1, 2)\n                    )\n                    self._image_data_lp[i, :, :, :] = self._image_data_lp[\n                        i, :, :, :\n                    ] * (max_image_data / max_image_data_lp)\n            else:\n                for z_idx in range(self._image_data.shape[1]):\n                    image_data_cp = cp.asarray(\n                        self._image_data[i, z_idx, :], dtype=cp.float32\n                    )\n                    max_image_data = cp.asnumpy(\n                        cp.max(image_data_cp, axis=(0, 1))\n                    ).astype(np.float32)\n                    if max_image_data == 0:\n                        self._image_data_lp[i, z_idx, :, :] = 0\n                    else:\n                        self._image_data_lp[i, z_idx, :, :] = cp.asnumpy(\n                            gaussian_filter(image_data_cp, sigma=(sigma[1], sigma[2]))\n                        ).astype(np.float32)\n                        max_image_data_lp = np.max(\n                            self._image_data_lp[i, z_idx, :, :], axis=(0, 1)\n                        )\n                        self._image_data_lp[i, z_idx, :, :] = self._image_data_lp[\n                            i, z_idx, :, :\n                        ] * (max_image_data / max_image_data_lp)\n\n        self._filter_type = \"lp\"\n\n        del image_data_cp\n        del self._image_data\n        gc.collect()\n        cp.get_default_memory_pool().free_all_blocks()\n\n    @staticmethod\n    def _scale_pixel_traces(\n        pixel_traces: Union[np.ndarray, cp.ndarray],\n        background_vector: Union[np.ndarray, cp.ndarray],\n        normalization_vector: Union[np.ndarray, cp.ndarray],\n        merfish_bits=16,\n    ) -&gt; cp.ndarray:\n        \"\"\"Scale pixel traces using background and normalization vectors.\n\n        Parameters\n        ----------\n        pixel_traces : Union[np.ndarray, cp.ndarray]\n            Pixel traces to scale.\n        background_vector : Union[np.ndarray, cp.ndarray]\n            Background vector.\n        normalization_vector : Union[np.ndarray, cp.ndarray]\n            Normalization vector.\n        merfish_bits : int = 16\n            Number of MERFISH bits. Default 16. Assume MERFISH bits are [0, merfish_bits].\n\n        Returns\n        -------\n        scaled_traces : cp.ndarray\n            Scaled pixel traces.\n        \"\"\"\n\n        if isinstance(pixel_traces, np.ndarray):\n            pixel_traces = cp.asarray(pixel_traces, dtype=cp.float32)\n        if isinstance(background_vector, np.ndarray):\n            background_vector = cp.asarray(background_vector, dtype=cp.float32)\n        if isinstance(normalization_vector, np.ndarray):\n            normalization_vector = cp.asarray(normalization_vector, dtype=cp.float32)\n\n        background_vector = background_vector[0:merfish_bits]\n        normalization_vector = normalization_vector[0:merfish_bits]\n\n        return (pixel_traces - background_vector[:, cp.newaxis]) / normalization_vector[\n            :, cp.newaxis\n        ]\n\n    @staticmethod\n    def _clip_pixel_traces(\n        pixel_traces: Union[np.ndarray, cp.ndarray],\n        clip_lower: float = 0.0,\n        clip_upper: float = 1.0,\n    ) -&gt; cp.ndarray:\n        \"\"\"Clip pixel traces to a range.\n\n        Parameters\n        ----------\n        pixel_traces : Union[np.ndarray, cp.ndarray]\n            Pixel traces to clip.\n        clip_lower : float, default 0.0\n            clip lower bound.\n        clip_upper : float, default 1.0\n            clip upper bound.\n\n        Returns\n        -------\n        clipped_traces : cp.ndarray\n            Clipped pixel traces.\n        \"\"\"\n\n        return cp.clip(pixel_traces, clip_lower, clip_upper, pixel_traces)\n\n    @staticmethod\n    def _normalize_pixel_traces(\n        pixel_traces: Union[np.ndarray, cp.ndarray],\n    ) -&gt; Tuple[cp.ndarray, cp.ndarray]:\n        \"\"\"Normalize pixel traces by L2 norm.\n\n        Parameters\n        ----------\n        pixel_traces : Union[np.ndarray, cp.ndarray]\n            Pixel traces to normalize.\n\n        Returns\n        -------\n        normalized_traces : cp.ndarray\n            Normalized pixel traces.\n        norms : cp.ndarray\n            L2 norms of pixel traces.    \n        \"\"\"\n\n        if isinstance(pixel_traces, np.ndarray):\n            pixel_traces = cp.asarray(pixel_traces, dtype=cp.float32)\n\n        norms = cp.linalg.norm(pixel_traces, axis=0)\n        norms = cp.where(norms == 0, np.inf, norms)\n        normalized_traces = pixel_traces / norms\n        norms = cp.where(norms == np.inf, -1, norms)\n\n        return normalized_traces, norms\n\n    @staticmethod\n    def _calculate_distances(\n        pixel_traces: Union[np.ndarray, cp.ndarray],\n        codebook_matrix: Union[np.ndarray, cp.ndarray],\n    ) -&gt; Tuple[cp.ndarray, cp.ndarray]:\n        \"\"\"Calculate distances between pixel traces and codebook matrix.\n\n        Parameters\n        ----------\n        pixel_traces : Union[np.ndarray, cp.ndarray]\n            Pixel traces.\n        codebook_matrix : Union[np.ndarray, cp.ndarray]\n            Codebook matrix.\n\n        Returns\n        -------\n        min_distances : cp.ndarray\n            Minimum distances.\n        min_indices : cp.ndarray\n            Minimum indices.\n        \"\"\"\n\n        if isinstance(pixel_traces, np.ndarray):\n            pixel_traces = cp.asarray(pixel_traces, dtype=cp.float32)\n        if isinstance(codebook_matrix, np.ndarray):\n            codebook_matrix = cp.asarray(codebook_matrix, dtype=cp.float32)\n\n        distances = cdist(\n            cp.ascontiguousarray(pixel_traces.T),\n            cp.ascontiguousarray(codebook_matrix),\n            metric=\"euclidean\",\n        )\n\n        min_indices = cp.argmin(distances, axis=1)\n        min_distances = cp.min(distances, axis=1)\n\n        del pixel_traces, codebook_matrix\n        gc.collect()\n        cp.get_default_memory_pool().free_all_blocks()\n\n        return min_distances, min_indices\n\n    def _decode_pixels(\n        self, distance_threshold: float = 0.5172, \n        magnitude_threshold: float = 1.0\n    ):\n        \"\"\"Decode pixels using the decoding matrix.\n\n        Parameters\n        ----------\n        distance_threshold : float, default 0.5172.\n            Distance threshold for decoding. The default is for a 4-bit,\n            4-distance Hamming codebook.\n        magnitude_threshold : float, default 1.0.\n            Magnitude threshold for decoding. \n        \"\"\"\n\n        if self._filter_type == \"lp\":\n            original_shape = self._image_data_lp.shape\n            self._decoded_image = np.zeros((original_shape[1:]), dtype=np.int16)\n            self._magnitude_image = np.zeros((original_shape[1:]), dtype=np.float16)\n            self._scaled_pixel_images = np.zeros((original_shape), dtype=np.float16)\n            self._distance_image = np.zeros((original_shape[1:]), dtype=np.float16)\n        else:\n            original_shape = self._image_data.shape\n            self._decoded_image = np.zeros((original_shape[1:]), dtype=np.int16)\n            self._magnitude_image = np.zeros((original_shape[1:]), dtype=np.float16)\n            self._scaled_pixel_images = np.zeros((original_shape), dtype=np.float16)\n            self._distance_image = np.zeros((original_shape[1:]), dtype=np.float16)\n\n        if self._verbose &gt; 1:\n            print(\"decode pixels\")\n            iterable_z = tqdm(range(original_shape[1]), desc=\"z\", leave=False)\n        elif self._verbose &gt;= 1:\n            iterable_z = tqdm(range(original_shape[1]), desc=\"decoding\", leave=False)\n        else:\n            iterable_z = range(original_shape[1])\n\n        for z_idx in iterable_z:\n            if self._filter_type == \"lp\":\n                z_plane_shape = self._image_data_lp[:, z_idx, :].shape\n                scaled_pixel_traces = (\n                    cp.asarray(self._image_data_lp[:, z_idx, :])\n                    .reshape(self._n_merfish_bits, -1)\n                    .astype(cp.float32)\n                )\n            else:\n                z_plane_shape = self._image_data[:, z_idx, :].shape\n                scaled_pixel_traces = (\n                    cp.asarray(self._image_data[:, z_idx, :])\n                    .reshape(self._n_merfish_bits, -1)\n                    .astype(cp.float32)\n                )\n\n            if self._iterative_normalization_loaded:\n                scaled_pixel_traces = self._scale_pixel_traces(\n                    scaled_pixel_traces,\n                    self._iterative_background_vector,\n                    self._iterative_normalization_vector,\n                    self._n_merfish_bits,\n                )\n            elif self._global_normalization_loaded:\n                scaled_pixel_traces = self._scale_pixel_traces(\n                    scaled_pixel_traces,\n                    self._global_background_vector,\n                    self._global_normalization_vector,\n                    self._n_merfish_bits,\n                )\n\n            scaled_pixel_traces = self._clip_pixel_traces(scaled_pixel_traces)\n            normalized_pixel_traces, pixel_magnitude_trace = (\n                self._normalize_pixel_traces(scaled_pixel_traces)\n            )\n            distance_trace, codebook_index_trace = self._calculate_distances(\n                normalized_pixel_traces, self._decoding_matrix\n            )\n\n            del normalized_pixel_traces\n            cp.get_default_memory_pool().free_all_blocks()\n            gc.collect()\n\n            decoded_trace = cp.full(distance_trace.shape[0], -1, dtype=cp.int16)\n            mask_trace = distance_trace &lt; distance_threshold\n            decoded_trace[mask_trace] = codebook_index_trace[mask_trace]\n            decoded_trace[pixel_magnitude_trace &lt;= magnitude_threshold] = -1\n\n            self._decoded_image[z_idx, :] = cp.asnumpy(\n                cp.reshape(cp.round(decoded_trace, 3), z_plane_shape[1:])\n            )\n            self._magnitude_image[z_idx, :] = cp.asnumpy(\n                cp.reshape(cp.round(pixel_magnitude_trace, 3), z_plane_shape[1:])\n            )\n            self._scaled_pixel_images[:, z_idx, :] = cp.asnumpy(\n                cp.reshape(cp.round(scaled_pixel_traces, 3), z_plane_shape)\n            )\n            self._distance_image[z_idx, :] = cp.asnumpy(\n                cp.reshape(cp.round(distance_trace, 3), z_plane_shape[1:])\n            )\n\n            del (\n                decoded_trace,\n                pixel_magnitude_trace,\n                scaled_pixel_traces,\n                distance_trace,\n            )\n            cp.get_default_memory_pool().free_all_blocks()\n            gc.collect()\n\n    @staticmethod\n    def _warp_pixel(\n        pixel_space_point: np.ndarray,\n        spacing: np.ndarray,\n        origin: np.ndarray,\n        affine: np.ndarray,\n    ) -&gt; np.ndarray:\n        \"\"\"Warp pixel space point to physical space point.\n\n        Parameters\n        ----------\n        pixel_space_point : np.ndarray\n            Pixel space point.\n        spacing : np.ndarray\n            Spacing.\n        origin : np.ndarray\n            Origin.\n        affine : np.ndarray \n            Affine transformation matrix.\n\n        Returns\n        -------\n        registered_space_point : np.ndarray\n            Registered space point.\n        \"\"\"\n\n        physical_space_point = pixel_space_point * spacing + origin\n        registered_space_point = (\n            np.array(affine) @ np.array(list(physical_space_point) + [1])\n        )[:-1]\n\n        return registered_space_point\n\n    def _extract_barcodes(\n        self, \n        minimum_pixels: int = 2, \n        maximum_pixels: int = 100\n    ):\n        \"\"\"Extract barcodes from decoded image.\n\n        Parameters\n        ----------\n        minimum_pixels : int, default 2\n            Minimum number of pixels for a barcode. \n        maximum_pixels : int, default 100\n            Maximum number of pixels for a barcode. \n        \"\"\"\n\n        if self._verbose &gt; 1:\n            print(\"extract barcodes\")\n        if self._verbose &gt;= 1:\n            iterable_barcode = tqdm(\n                range(self._codebook_matrix.shape[0]), desc=\"barcode\", leave=False\n            )\n        else:\n            iterable_barcode = range(self._codebook_matrix.shape[0])\n        decoded_image = cp.asarray(self._decoded_image, dtype=cp.int16)\n        if self._optimize_normalization_weights:\n            if self._filter_type == \"lp\":\n                intensity_image = np.concatenate(\n                    [np.expand_dims(self._distance_image, axis=0), self._image_data_lp],\n                    axis=0,\n                ).transpose(1, 2, 3, 0)\n            else:\n                intensity_image = np.concatenate(\n                    [np.expand_dims(self._distance_image, axis=0), self._image_data],\n                    axis=0,\n                ).transpose(1, 2, 3, 0)\n        else:\n            intensity_image = np.concatenate(\n                [\n                    np.expand_dims(self._distance_image, axis=0),\n                    self._scaled_pixel_images,\n                ],\n                axis=0,\n            ).transpose(1, 2, 3, 0)\n\n        for barcode_index in iterable_barcode:\n            on_bits_indices = np.where(self._codebook_matrix[barcode_index])[0]\n\n            if len(on_bits_indices) == 1:\n                break\n\n            if self._is_3D:\n                if self._verbose &gt; 1:\n                    print(\"\")\n                    print(\"label image\")\n                labeled_image = label(decoded_image == barcode_index, connectivity=3)\n\n                if self._verbose &gt; 1:\n                    print(\"remove large\")\n                pixel_counts = cp.bincount(labeled_image.ravel())\n                large_labels = cp.where(pixel_counts &gt;= maximum_pixels)[0]\n                large_label_mask = cp.zeros_like(labeled_image, dtype=bool)\n                large_label_mask = cp.isin(labeled_image, large_labels)\n                labeled_image[large_label_mask] = 0\n\n                if self._verbose &gt; 1:\n                    print(\"remove small\")\n                labeled_image = remove_small_objects(\n                    labeled_image, min_size=(minimum_pixels - 1)\n                )\n                if self._verbose &gt; 1:\n                    print(\"regionprops table\")\n\n                props = regionprops_table(\n                    cp.asnumpy(labeled_image).astype(np.int32),\n                    intensity_image=intensity_image,\n                    properties=[\n                        \"area\",\n                        \"centroid\",\n                        \"intensity_mean\",\n                        \"moments_normalized\",\n                        \"inertia_tensor_eigvals\",\n                    ],\n                )\n\n                del labeled_image\n                gc.collect()\n                cp.get_default_memory_pool().free_all_blocks()\n\n                df_barcode = pd.DataFrame(props)\n\n                df_barcode[\"on_bit_1\"] = on_bits_indices[0] + 1\n                df_barcode[\"on_bit_2\"] = on_bits_indices[1] + 1\n                df_barcode[\"on_bit_3\"] = on_bits_indices[2] + 1\n                df_barcode[\"on_bit_4\"] = on_bits_indices[3] + 1\n                df_barcode[\"barcode_id\"] = df_barcode.apply(\n                    lambda x: (barcode_index + 1), axis=1\n                )\n                df_barcode[\"gene_id\"] = df_barcode.apply(\n                    lambda x: self._gene_ids[barcode_index], axis=1\n                )\n                df_barcode[\"tile_idx\"] = self._tile_idx\n\n                df_barcode.rename(columns={\"centroid-0\": \"z\"}, inplace=True)\n                df_barcode.rename(columns={\"centroid-1\": \"y\"}, inplace=True)\n                df_barcode.rename(columns={\"centroid-2\": \"x\"}, inplace=True)\n\n                if self._z_crop:\n                    df_barcode[\"z\"] = df_barcode[\"z\"] + self._z_range[0]\n\n                df_barcode[\"tile_z\"] = np.round(df_barcode[\"z\"], 0).astype(int)\n                df_barcode[\"tile_y\"] = np.round(df_barcode[\"y\"], 0).astype(int)\n                df_barcode[\"tile_x\"] = np.round(df_barcode[\"x\"], 0).astype(int)\n                pts = df_barcode[[\"z\", \"y\", \"x\"]].to_numpy()\n                for pt_idx, pt in enumerate(pts):\n                    pts[pt_idx, :] = self._warp_pixel(\n                        pts[pt_idx, :].copy(), self._spacing, self._origin, self._affine\n                    )\n\n                df_barcode[\"global_z\"] = np.round(pts[:, 0], 2)\n                df_barcode[\"global_y\"] = np.round(pts[:, 1], 2)\n                df_barcode[\"global_x\"] = np.round(pts[:, 2], 2)\n\n                df_barcode.rename(\n                    columns={\"intensity_mean-0\": \"distance_mean\"}, inplace=True\n                )\n                for i in range(1, self._n_merfish_bits + 1):\n                    df_barcode.rename(\n                        columns={\n                            \"intensity_mean-\" + str(i): \"bit\"\n                            + str(i).zfill(2)\n                            + \"_mean_intensity\"\n                        },\n                        inplace=True,\n                    )\n\n                on_bits = on_bits_indices + np.ones(4)\n\n                signal_mean_columns = [\n                    f\"bit{int(bit):02d}_mean_intensity\" for bit in on_bits\n                ]\n                bkd_mean_columns = [\n                    f\"bit{int(bit):02d}_mean_intensity\"\n                    for bit in range(1, self._n_merfish_bits + 1)\n                    if bit not in on_bits\n                ]\n\n                df_barcode[\"signal_mean\"] = df_barcode[signal_mean_columns].mean(axis=1)\n                df_barcode[\"bkd_mean\"] = df_barcode[bkd_mean_columns].mean(axis=1)\n                df_barcode[\"s-b_mean\"] = (\n                    df_barcode[\"signal_mean\"] - df_barcode[\"bkd_mean\"]\n                )\n\n                del props\n                gc.collect()\n\n                if self._verbose &gt; 1:\n                    print(\"dataframe aggregation\")\n                if barcode_index == 0:\n                    self._df_barcodes = df_barcode.copy()\n                else:\n                    self._df_barcodes = pd.concat([self._df_barcodes, df_barcode])\n                    self._df_barcodes.reset_index(drop=True, inplace=True)\n\n                del df_barcode\n                gc.collect()\n            else:\n                for z_idx in range(decoded_image.shape[0]):\n                    if self._verbose &gt; 1:\n                        print(\"\")\n                        print(\"label image\")\n                    labeled_image = label(\n                        decoded_image[z_idx, :] == barcode_index, connectivity=2\n                    )\n\n                    if self._verbose &gt; 1:\n                        print(\"remove large\")\n                    pixel_counts = cp.bincount(labeled_image.ravel())\n                    large_labels = cp.where(pixel_counts &gt; maximum_pixels)[0]\n                    large_label_mask = cp.zeros_like(labeled_image, dtype=bool)\n                    large_label_mask = cp.isin(labeled_image, large_labels)\n                    labeled_image[large_label_mask] = 0\n\n                    if self._verbose &gt; 1:\n                        print(\"remove small\")\n                    labeled_image = remove_small_objects(\n                        labeled_image, min_size=minimum_pixels\n                    )\n                    if self._verbose &gt; 1:\n                        print(\"regionprops table\")\n                    props = regionprops_table(\n                        cp.asnumpy(labeled_image).astype(np.int32),\n                        intensity_image=intensity_image[z_idx, :],\n                        properties=[\n                            \"area\",\n                            \"centroid\",\n                            \"intensity_mean\",\n                            \"moments_normalized\",\n                            \"inertia_tensor_eigvals\",\n                        ],\n                    )\n\n                    del labeled_image\n                    gc.collect()\n                    cp.get_default_memory_pool().free_all_blocks()\n\n                    df_barcode = pd.DataFrame(props)\n\n                    df_barcode[\"on_bit_1\"] = on_bits_indices[0] + 1\n                    df_barcode[\"on_bit_2\"] = on_bits_indices[1] + 1\n                    df_barcode[\"on_bit_3\"] = on_bits_indices[2] + 1\n                    df_barcode[\"on_bit_4\"] = on_bits_indices[3] + 1\n                    df_barcode[\"barcode_id\"] = df_barcode.apply(\n                        lambda x: (barcode_index + 1), axis=1\n                    )\n                    df_barcode[\"gene_id\"] = df_barcode.apply(\n                        lambda x: self._gene_ids[barcode_index], axis=1\n                    )\n                    df_barcode[\"tile_idx\"] = self._tile_idx\n\n                    df_barcode[\"z\"] = z_idx\n                    df_barcode.rename(columns={\"centroid-0\": \"y\"}, inplace=True)\n                    df_barcode.rename(columns={\"centroid-1\": \"x\"}, inplace=True)\n\n                    if self._z_crop:\n                        df_barcode[\"z\"] = df_barcode[\"z\"] + self._z_range[0]\n\n                    df_barcode[\"tile_z\"] = np.round(df_barcode[\"z\"], 0).astype(int)\n                    df_barcode[\"tile_y\"] = np.round(df_barcode[\"y\"], 0).astype(int)\n                    df_barcode[\"tile_x\"] = np.round(df_barcode[\"x\"], 0).astype(int)\n\n                    pts = df_barcode[[\"z\", \"y\", \"x\"]].to_numpy()\n                    for pt_idx, pt in enumerate(pts):\n                        pts[pt_idx, :] = self._warp_pixel(\n                            pts[pt_idx, :].copy(),\n                            self._spacing,\n                            self._origin,\n                            self._affine,\n                        )\n\n                    df_barcode[\"global_z\"] = np.round(pts[:, 0], 2)\n                    df_barcode[\"global_y\"] = np.round(pts[:, 1], 2)\n                    df_barcode[\"global_x\"] = np.round(pts[:, 2], 2)\n\n                    df_barcode.rename(\n                        columns={\"intensity_mean-0\": \"distance_mean\"}, inplace=True\n                    )\n                    for i in range(1, self._n_merfish_bits + 1):\n                        df_barcode.rename(\n                            columns={\n                                \"intensity_mean-\" + str(i): \"bit\"\n                                + str(i).zfill(2)\n                                + \"_mean_intensity\"\n                            },\n                            inplace=True,\n                        )\n\n                    on_bits = on_bits_indices + np.ones(4)\n\n                    signal_mean_columns = [\n                        f\"bit{int(bit):02d}_mean_intensity\" for bit in on_bits\n                    ]\n                    bkd_mean_columns = [\n                        f\"bit{int(bit):02d}_mean_intensity\"\n                        for bit in range(1, self._n_merfish_bits + 1)\n                        if bit not in on_bits\n                    ]\n\n                    df_barcode[\"signal_mean\"] = df_barcode[signal_mean_columns].mean(\n                        axis=1\n                    )\n                    df_barcode[\"bkd_mean\"] = df_barcode[bkd_mean_columns].mean(axis=1)\n                    df_barcode[\"s-b_mean\"] = (\n                        df_barcode[\"signal_mean\"] - df_barcode[\"bkd_mean\"]\n                    )\n\n                    del props\n                    gc.collect()\n\n                    if self._verbose &gt; 1:\n                        print(\"dataframe aggregation\")\n                    if barcode_index == 0:\n                        self._df_barcodes = df_barcode.copy()\n                    else:\n                        self._df_barcodes = pd.concat([self._df_barcodes, df_barcode])\n                        self._df_barcodes.reset_index(drop=True, inplace=True)\n\n                    del df_barcode\n                    gc.collect()\n\n        del decoded_image, intensity_image\n        gc.collect()\n        cp.get_default_memory_pool().free_all_blocks()\n\n    def _save_barcodes(self):\n        \"\"\"Save barcodes to datastore.\"\"\"\n\n        if self._verbose &gt; 1:\n            print(\"save barcodes\")\n\n        if self._optimize_normalization_weights:\n            decoded_dir_path = self._temp_dir\n            decoded_dir_path.mkdir(parents=True, exist_ok=True)\n            temp_decoded_path = decoded_dir_path / Path(\n                \"tile\" + str(self._tile_idx).zfill(3) + \"_temp_decoded.parquet\"\n            )\n            self._df_barcodes.to_parquet(temp_decoded_path)\n        else:\n            if not (self._barcodes_filtered):\n                self._datastore.save_local_decoded_spots(\n                    self._df_barcodes, tile=self._tile_idx\n                )\n            else:\n                self._datastore.save_global_filtered_decoded_spots(\n                    self._df_filtered_barcodes\n                )\n\n    def _reformat_barcodes_for_baysor(self):\n        \"\"\"Reformat barcodes for Baysor and save to datastore.\"\"\"\n\n        if self._barcodes_filtered:\n            missing_columns = [\n                col\n                for col in [\n                    \"gene_id\",\n                    \"global_z\",\n                    \"global_y\",\n                    \"global_x\",\n                    \"cell_id\",\n                    \"tile_idx\",\n                    \"distance_mean\",\n                ]\n                if col not in self._df_filtered_barcodes.columns\n            ]\n            if missing_columns:\n                print(f\"The following columns are missing: {missing_columns}\")\n            baysor_df = self._df_filtered_barcodes[\n                [\n                    \"gene_id\",\n                    \"global_z\",\n                    \"global_y\",\n                    \"global_x\",\n                    \"cell_id\",\n                    \"tile_idx\",\n                    \"distance_mean\",\n                ]\n            ].copy()\n            baysor_df.rename(\n                columns={\n                    \"gene_id\": \"feature_name\",\n                    \"global_x\": \"x_location\",\n                    \"global_y\": \"y_location\",\n                    \"global_z\": \"z_location\",\n                    \"barcode_id\": \"codeword_index\",\n                    \"tile_idx\": \"fov_name\",\n                    \"distance_mean\": \"qv\",\n                },\n                inplace=True,\n            )\n\n            baysor_df[\"cell_id\"] = baysor_df[\"cell_id\"] + 1\n            baysor_df[\"transcript_id\"] = pd.util.hash_pandas_object(\n                baysor_df, index=False\n            )\n            baysor_df[\"is_gene\"] = ~baysor_df[\"feature_name\"].str.contains(\n                \"Blank\", na=False\n            )\n            self._datastore.save_spots_prepped_for_baysor(baysor_df)\n\n    def _load_all_barcodes(self):\n        \"\"\"Load all barcodes from datastore.\"\"\"\n\n        if self._optimize_normalization_weights:\n            decoded_dir_path = self._temp_dir\n\n            tile_files = decoded_dir_path.glob(\"*.parquet\")\n            tile_files = sorted(tile_files, key=lambda x: x.name)\n\n            if self._verbose &gt;= 1:\n                iterable_files = tqdm(tile_files, desc=\"tile\", leave=False)\n            else:\n                iterable_files = tile_files\n\n            tile_data = [\n                pd.read_parquet(parquet_file) for parquet_file in iterable_files\n            ]\n            self._df_barcodes_loaded = pd.concat(tile_data)\n        elif self._load_tile_decoding:\n            tile_data = []\n            for tile_id in self._datastore.tile_ids:\n                tile_data.append(self._datastore.load_local_decoded_spots(tile_id))\n            self._df_barcodes_loaded = pd.concat(tile_data)\n        else:\n            self._df_filtered_barcodes = (\n                self._datastore.load_global_filtered_decoded_spots()\n            )\n            self._barcodes_filtered = True\n\n    @staticmethod\n    def calculate_fdr(\n        df: pd.DataFrame, \n        threshold: float, \n        blank_count: int, \n        barcode_count: int, \n        verbose: bool = False) -&gt; float:\n        \"\"\"Calculate false discovery rate.\n\n        (# noncoding found ) / (# noncoding in codebook) / (# coding found) / (# coding in codebook)\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n            Dataframe containing decoded spots.\n        threshold : float\n            Threshold for predicted probability.\n        blank_count : int\n            Number of blank barcodes.\n        barcode_count : int\n            Number of barcodes.\n        verbose : bool = False\n            Verbose output. Default False.\n\n        Returns\n        -------\n        fdr : float\n            False discovery rate.\n        \"\"\"\n\n        if threshold &gt;= 0:\n            df[\"prediction\"] = df[\"predicted_probability\"] &gt; threshold\n\n            coding = df[\n                (~df[\"gene_id\"].str.startswith(\"Blank\"))\n                &amp; (df[\"predicted_probability\"] &gt; threshold)\n            ].shape[0]\n            noncoding = df[\n                (df[\"gene_id\"].str.startswith(\"Blank\"))\n                &amp; (df[\"predicted_probability\"] &gt; threshold)\n            ].shape[0]\n        else:\n            coding = df[(~df[\"gene_id\"].str.startswith(\"Blank\"))].shape[0]\n            noncoding = df[(df[\"gene_id\"].str.startswith(\"Blank\"))].shape[0]\n\n        if coding &gt; 0:\n            fdr = (noncoding / blank_count) / (coding / (barcode_count - blank_count))\n        else:\n            fdr = np.inf\n\n        if verbose &gt; 1:\n            print(f\"threshold: {threshold}\")\n            print(f\"coding: {coding}\")\n            print(f\"noncoding: {noncoding}\")\n            print(f\"fdr: {fdr}\")\n\n        return fdr\n\n    def _filter_all_barcodes(self, fdr_target: float = 0.05):\n        \"\"\"Filter barcodes using a classifier and FDR target.\n\n        Uses a MLP classifier to predict whether a barcode is a blank or not.\n\n        TO DO: evaluate other classifiers.\n\n        Parameters\n        ----------\n        fdr_target : float, default 0.05\n            False discovery rate target. \n        \"\"\"\n\n        from sklearn.model_selection import train_test_split\n        from sklearn.preprocessing import StandardScaler\n        from sklearn.neural_network import MLPClassifier\n        from sklearn.metrics import classification_report\n        from imblearn.over_sampling import SMOTE\n\n        self._df_barcodes_loaded[\"X\"] = ~self._df_barcodes_loaded[\n            \"gene_id\"\n        ].str.startswith(\"Blank\")\n        if self._is_3D:\n            columns = [\n                \"X\",\n                \"signal_mean\",\n                \"s-b_mean\",\n                \"distance_mean\",\n                \"moments_normalized-0-0-2\",\n                \"moments_normalized-0-0-3\",\n                \"moments_normalized-0-1-1\",\n                \"moments_normalized-0-1-2\",\n                \"moments_normalized-0-1-3\",\n                \"moments_normalized-0-2-0\",\n                \"moments_normalized-0-2-1\",\n                \"moments_normalized-0-2-3\",\n                \"moments_normalized-0-3-0\",\n                \"moments_normalized-0-3-1\",\n                \"moments_normalized-0-3-2\",\n                \"moments_normalized-0-3-3\",\n                \"inertia_tensor_eigvals-0\",\n                \"inertia_tensor_eigvals-1\",\n                \"inertia_tensor_eigvals-2\",\n            ]\n        else:\n            columns = [\n                \"X\",\n                \"signal_mean\",\n                \"s-b_mean\",\n                \"distance_mean\",\n                \"moments_normalized-0-2\",\n                \"moments_normalized-0-3\",\n                \"moments_normalized-1-1\",\n                \"moments_normalized-1-2\",\n                \"moments_normalized-1-3\",\n                \"moments_normalized-2-0\",\n                \"moments_normalized-2-1\",\n                \"moments_normalized-2-2\",\n                \"moments_normalized-2-3\",\n                \"moments_normalized-3-0\",\n                \"moments_normalized-3-1\",\n                \"moments_normalized-3-2\",\n                \"moments_normalized-3-3\",\n                \"inertia_tensor_eigvals-0\",\n                \"inertia_tensor_eigvals-1\",\n            ]\n        df_true = self._df_barcodes_loaded[self._df_barcodes_loaded[\"X\"] == True][ #noqa\n            columns\n        ]  # noqa\n        df_false = self._df_barcodes_loaded[self._df_barcodes_loaded[\"X\"] == False][ #noqa\n            columns\n        ]  # noqa\n\n        if len(df_false) &gt; 0:\n            df_true_sampled = df_true.sample(n=len(df_false), random_state=42)\n            df_combined = pd.concat([df_true_sampled, df_false])\n            x = df_combined.drop(\"X\", axis=1)\n            y = df_combined[\"X\"]\n            X_train, X_test, y_train, y_test = train_test_split(\n                x, y, test_size=0.1, random_state=42\n            )\n\n            if self._verbose &gt; 1:\n                print(\"generating synthetic samples for class balance\")\n            smote = SMOTE(random_state=42)\n            X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\n            if self._verbose &gt; 1:\n                print(\"scaling features\")\n            scaler = StandardScaler()\n            X_train_scaled = scaler.fit_transform(X_train_resampled)\n            X_test_scaled = scaler.transform(X_test)\n\n            if self._verbose &gt; 1:\n                print(\"training classifier\")\n            # logistic = LogisticRegression(solver='liblinear', random_state=42)\n            mlp = MLPClassifier(solver=\"adam\", max_iter=10000, random_state=42)\n            mlp.fit(X_train_scaled, y_train_resampled)\n            predictions = mlp.predict(X_test_scaled)\n\n            if self._verbose &gt; 1:\n                print(classification_report(y_test, predictions))\n\n            if self._verbose &gt; 1:\n                print(\"predicting on full data\")\n\n            full_data_scaled = scaler.transform(self._df_barcodes_loaded[columns[1:]])\n            self._df_barcodes_loaded[\"predicted_probability\"] = mlp.predict_proba(\n                full_data_scaled\n            )[:, 1]\n\n            if self._verbose &gt; 1:\n                print(\"filtering blanks\")\n\n            coarse_threshold = 0\n            for threshold in np.arange(0, 1, 0.1):  # Coarse step: 0.1\n                fdr = self.calculate_fdr(\n                    self._df_barcodes_loaded,\n                    threshold,\n                    self._blank_count,\n                    self._barcode_count,\n                    self._verbose,\n                )\n                if fdr &lt;= fdr_target:\n                    coarse_threshold = threshold\n                    break\n\n            fine_threshold = coarse_threshold\n            for threshold in np.arange(\n                coarse_threshold - 0.1, coarse_threshold + 0.1, 0.01\n            ):\n                fdr = self.calculate_fdr(\n                    self._df_barcodes_loaded,\n                    threshold,\n                    self._blank_count,\n                    self._barcode_count,\n                    self._verbose,\n                )\n                if fdr &lt;= fdr_target:\n                    fine_threshold = threshold\n                    break\n\n            df_above_threshold = self._df_barcodes_loaded[\n                self._df_barcodes_loaded[\"predicted_probability\"] &gt; fine_threshold\n            ]\n            self._df_filtered_barcodes = df_above_threshold[\n                [\n                    \"tile_idx\",\n                    \"gene_id\",\n                    \"global_z\",\n                    \"global_y\",\n                    \"global_x\",\n                    \"distance_mean\",\n                ]\n            ].copy()\n            self._df_filtered_barcodes[\"cell_id\"] = -1\n            self._barcodes_filtered = True\n\n            if self._verbose &gt; 1:\n                print(f\"fdr : {fdr}\")\n                print(f\"retained barcodes: {len(self._df_filtered_barcodes)}\")\n\n            del df_above_threshold, full_data_scaled\n            del (\n                mlp,\n                predictions,\n                X_train,\n                X_test,\n                y_test,\n                y_train,\n                X_train_scaled,\n                X_test_scaled,\n            )\n            del df_true, df_false, df_true_sampled, df_combined\n            gc.collect()\n        else:\n            self._df_filtered_barcodes = self._df_barcodes_loaded.copy()\n            self._df_filtered_barcodes[\"cell_id\"] = -1\n            self._df_filtered_barcodes.drop(\"X\", axis=1, inplace=True)\n            self._barcodes_filtered = True\n\n    def _filter_all_barcodes_LR(self, fdr_target: float = 0.05):\n        \"\"\"Filter barcodes using a classifier and FDR target.\n\n        Uses a logistic regression classifier to predict whether a barcode is a blank or not.\n\n        Parameters\n        ----------\n        fdr_target : float, default 0.05\n            False discovery rate target. \n        \"\"\"\n\n        from sklearn.model_selection import train_test_split\n        from sklearn.preprocessing import StandardScaler\n        from sklearn.linear_model import LogisticRegression\n        from sklearn.metrics import classification_report\n        from imblearn.over_sampling import SMOTE\n\n        self._df_barcodes_loaded[\"X\"] = ~self._df_barcodes_loaded[\n            \"gene_id\"\n        ].str.startswith(\"Blank\")\n\n        if self._is_3D:\n            columns = [\n                \"X\",\n                \"area\",\n                \"signal_mean\",\n                \"s-b_mean\",\n                \"distance_mean\",\n                \"moments_normalized-0-0-2\",\n                \"moments_normalized-0-0-3\",\n                \"moments_normalized-0-1-1\",\n                \"moments_normalized-0-1-2\",\n                \"moments_normalized-0-1-3\",\n                \"moments_normalized-0-2-0\",\n                \"moments_normalized-0-2-1\",\n                \"moments_normalized-0-2-3\",\n                \"moments_normalized-0-3-0\",\n                \"moments_normalized-0-3-1\",\n                \"moments_normalized-0-3-2\",\n                \"moments_normalized-0-3-3\",\n                \"inertia_tensor_eigvals-0\",\n                \"inertia_tensor_eigvals-1\",\n                \"inertia_tensor_eigvals-2\",\n            ]\n        else:\n            columns = [\n                \"X\",\n                \"area\",\n                \"signal_mean\",\n                \"s-b_mean\",\n                \"distance_mean\",\n                \"moments_normalized-0-2\",\n                \"moments_normalized-0-3\",\n                \"moments_normalized-1-1\",\n                \"moments_normalized-1-2\",\n                \"moments_normalized-1-3\",\n                \"moments_normalized-2-0\",\n                \"moments_normalized-2-1\",\n                \"moments_normalized-2-2\",\n                \"moments_normalized-2-3\",\n                \"moments_normalized-3-0\",\n                \"moments_normalized-3-1\",\n                \"moments_normalized-3-2\",\n                \"moments_normalized-3-3\",\n                \"inertia_tensor_eigvals-0\",\n                \"inertia_tensor_eigvals-1\",\n            ]\n\n        df_true = self._df_barcodes_loaded[self._df_barcodes_loaded[\"X\"] == True][columns] #noqa\n        df_false = self._df_barcodes_loaded[self._df_barcodes_loaded[\"X\"] == False][columns] #noqa\n\n        if len(df_false) &gt; 0:\n            df_true_sampled = df_true.sample(n=len(df_false), random_state=42)\n            df_combined = pd.concat([df_true_sampled, df_false])\n            x = df_combined.drop(\"X\", axis=1)\n            y = df_combined[\"X\"]\n            X_train, X_test, y_train, y_test = train_test_split(\n                x, y, test_size=0.1, random_state=42\n            )\n\n            if self._verbose &gt; 1:\n                print(\"generating synthetic samples for class balance\")\n            smote = SMOTE(random_state=42)\n            X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\n            if self._verbose &gt; 1:\n                print(\"scaling features\")\n            scaler = StandardScaler()\n            X_train_scaled = scaler.fit_transform(X_train_resampled)\n            X_test_scaled = scaler.transform(X_test)\n\n            if self._verbose &gt; 1:\n                print(\"training classifier\")\n            logistic = LogisticRegression(solver='liblinear', random_state=42)\n            logistic.fit(X_train_scaled, y_train_resampled)\n            predictions = logistic.predict(X_test_scaled)\n\n            if self._verbose &gt; 1:\n                print(classification_report(y_test, predictions))\n\n            if self._verbose &gt; 1:\n                print(\"predicting on full data\")\n\n            full_data_scaled = scaler.transform(self._df_barcodes_loaded[columns[1:]])\n            self._df_barcodes_loaded[\"predicted_probability\"] = logistic.predict_proba(\n                full_data_scaled\n            )[:, 1]\n\n            if self._verbose &gt; 1:\n                print(\"filtering blanks\")\n\n            coarse_threshold = 0\n            for threshold in np.arange(0, 1, 0.1):\n                fdr = self.calculate_fdr(\n                    self._df_barcodes_loaded,\n                    threshold,\n                    self._blank_count,\n                    self._barcode_count,\n                    self._verbose,\n                )\n                if fdr &lt;= fdr_target:\n                    coarse_threshold = threshold\n                    break\n\n            fine_threshold = coarse_threshold\n            for threshold in np.arange(\n                coarse_threshold - 0.1, coarse_threshold + 0.1, 0.01\n            ):\n                fdr = self.calculate_fdr(\n                    self._df_barcodes_loaded,\n                    threshold,\n                    self._blank_count,\n                    self._barcode_count,\n                    self._verbose,\n                )\n                if fdr &lt;= fdr_target:\n                    fine_threshold = threshold\n                    break\n\n            df_above_threshold = self._df_barcodes_loaded[\n                self._df_barcodes_loaded[\"predicted_probability\"] &gt; fine_threshold\n            ]\n            self._df_filtered_barcodes = df_above_threshold[\n                [\n                    \"tile_idx\",\n                    \"gene_id\",\n                    \"global_z\",\n                    \"global_y\",\n                    \"global_x\",\n                    \"distance_mean\",\n                ]\n            ].copy()\n            self._df_filtered_barcodes[\"cell_id\"] = -1\n            self._barcodes_filtered = True\n\n            if self._verbose &gt; 1:\n                print(f\"fdr : {fdr}\")\n                print(f\"retained barcodes: {len(self._df_filtered_barcodes)}\")\n\n            del df_above_threshold, full_data_scaled\n            del (\n                logistic,\n                predictions,\n                X_train,\n                X_test,\n                y_test,\n                y_train,\n                X_train_scaled,\n                X_test_scaled,\n            )\n            del df_true, df_false, df_true_sampled, df_combined\n            gc.collect()\n        else:\n            self._df_filtered_barcodes = self._df_barcodes_loaded.copy()\n            self._df_filtered_barcodes[\"cell_id\"] = -1\n            self._df_filtered_barcodes.drop(\"X\", axis=1, inplace=True)\n            self._barcodes_filtered = True\n            print(\"Insufficient Blank barcodes called for filtering.\")\n\n    @staticmethod\n    def _roi_to_shapely(roi):\n        return Polygon(roi.subpixel_coordinates[:, ::-1])\n\n    def _assign_cells(self):\n        \"\"\"Assign cells to barcodes using Cellpose ROIs.\"\"\"\n\n        cellpose_roi_path = (\n            self._datastore._datastore_path\n            / Path(\"segmentation\")\n            / Path(\"cellpose\")\n            / Path(\"imagej_rois\")\n            / Path(\"global_coords_rois.zip\")\n        )\n\n        try:\n            rois = roiread(cellpose_roi_path)\n        except (FileNotFoundError, IOError, ValueError) as e:\n            print(f\"Failed to read ROIs: {e}\")\n            return\n\n        shapely_polygons = []\n        for roi in rois:\n            shapely_polygon = self._roi_to_shapely(roi)\n            if shapely_polygon:\n                shapely_polygons.append(shapely_polygon)\n\n        rtree_index = rtree.index.Index()\n        for polygon_idx, polygon in enumerate(shapely_polygons):\n            try:\n                rtree_index.insert(polygon_idx, polygon.bounds)\n            except rtree.RTreeError as e:\n                print(f\"Failed to insert polygon into R-tree: {e}\")\n\n        def check_point(row):\n            \"\"\"Check if point is within a polygon.\n\n            Parameters\n            ----------\n            row : pd.Series\n                Row containing global coordinates.\n\n            Returns\n            -------\n            cell_id : int\n                Cell ID. Returns 0 if not found.\n            \"\"\"\n            point = Point(row[\"global_y\"], row[\"global_x\"])\n\n            candidate_ids = list(rtree_index.intersection(point.bounds))\n            for candidate_id in candidate_ids:\n                if shapely_polygons[candidate_id].contains(point):\n                    return candidate_id + 1\n            return 0\n\n        self._df_filtered_barcodes[\"cell_id\"] = self._df_filtered_barcodes.apply(\n            check_point, axis=1\n        )\n\n    def _remove_duplicates_in_tile_overlap(self, radius: float = 0.75):\n        \"\"\"Remove duplicates in tile overlap.\n\n        Parameters\n        ----------\n        radius : float, default 0.75 \n            3D radius, in microns, for duplicate removal. \n        \"\"\"\n\n        self._df_filtered_barcodes.reset_index(drop=True, inplace=True)\n\n        coords = self._df_filtered_barcodes[[\"global_z\", \"global_y\", \"global_x\"]].values\n        tile_idxs = self._df_filtered_barcodes[\"tile_idx\"].values\n\n        tree = cKDTree(coords)\n        pairs = tree.query_pairs(radius)\n\n        rows_to_drop = set()\n        distances = []\n        for i, j in pairs:\n            if tile_idxs[i] != tile_idxs[j]:\n                if (\n                    self._df_filtered_barcodes.loc[i, \"distance_mean\"]\n                    &lt;= self._df_filtered_barcodes.loc[j, \"distance_mean\"]\n                ):\n                    rows_to_drop.add(j)\n                    distances.append(self._df_filtered_barcodes.loc[j, \"distance_mean\"])\n                else:\n                    rows_to_drop.add(i)\n                    distances.append(self._df_filtered_barcodes.loc[i, \"distance_mean\"])\n\n        self._df_filtered_barcodes.drop(rows_to_drop, inplace=True)\n        self._df_filtered_barcodes.reset_index(drop=True, inplace=True)\n\n        avg_distance = np.mean(distances) if distances else 0\n        dropped_count = len(rows_to_drop)\n\n        if self._verbose &gt; 1:\n            print(\n                \"Average distance metric of dropped points (overlap): \"\n                + str(avg_distance)\n            )\n            print(\"Dropped points: \" + str(dropped_count))\n\n    def _display_results(self):\n        \"\"\"Display results using Napari.\"\"\"\n\n        import napari\n        from qtpy.QtWidgets import QApplication\n\n        def on_close_callback():\n            viewer.layers.clear()\n            gc.collect()\n\n        viewer = napari.Viewer()\n        app = QApplication.instance()\n\n        app.lastWindowClosed.connect(on_close_callback)\n\n        viewer.add_image(\n            self._scaled_pixel_images,\n            scale=[self._axial_step, self._pixel_size, self._pixel_size],\n            name=\"pixels\",\n        )\n\n        viewer.add_image(\n            self._decoded_image,\n            scale=[self._axial_step, self._pixel_size, self._pixel_size],\n            name=\"decoded\",\n        )\n\n        viewer.add_image(\n            self._magnitude_image,\n            scale=[self._axial_step, self._pixel_size, self._pixel_size],\n            name=\"magnitude\",\n        )\n\n        viewer.add_image(\n            self._distance_image,\n            scale=[self._axial_step, self._pixel_size, self._pixel_size],\n            name=\"distance\",\n        )\n\n        napari.run()\n\n    def _cleanup(self):\n        \"\"\"Cleanup memory.\"\"\"\n        try:\n            if self._filter_type == \"lp\":\n                del self._image_data_lp\n            else:\n                del self._image_data\n        except AttributeError:\n            pass\n\n        try:\n            del (\n                self._scaled_pixel_images,\n                self._decoded_image,\n                self._distance_image,\n                self._magnitude_image,\n            )\n        except AttributeError:\n            pass\n\n        try:\n            del self._df_barcodes\n        except AttributeError:\n            pass\n        if self._barcodes_filtered:\n            del self._df_filtered_barcodes\n\n        gc.collect()\n        cp.get_default_memory_pool().free_all_blocks()\n\n    def decode_one_tile(\n        self,\n        tile_idx: int = 0,\n        display_results: bool = False,\n        lowpass_sigma: Optional[Sequence[float]] = (3, 1, 1),\n        minimum_pixels: Optional[float] = 3.0,\n        use_normalization: Optional[bool] = True,\n        ufish_threshold: Optional[float] = 0.5,\n    ):\n        \"\"\"Decode one tile.\n\n        Helper function to decode one tile. Can also display results in napari.\n\n        Parameters\n        ----------\n        tile_idx : int, default 0\n            Tile index.\n        display_results : bool, default False\n            Display results in napari. \n        lowpass_sigma : Optional[Sequence[float]], default (3, 1, 1)\n            Lowpass sigma. \n        minimum_pixels : Optional[float], default 3.0\n            Minimum number of pixels for a barcode. \n        use_normalization : Optional[bool], default True\n            Use normalization. \n        ufish_threshold : Optional[float], default 0.5\n            Ufish threshold. \n        \"\"\"\n\n        if use_normalization:\n            self._load_iterative_normalization_vectors()\n\n        self._tile_idx = tile_idx\n        self._load_bit_data(ufish_threshold=ufish_threshold)\n        if not (np.any(lowpass_sigma == 0)):\n            self._lp_filter(sigma=lowpass_sigma)\n        self._decode_pixels(\n            distance_threshold=self._distance_threshold,\n            magnitude_threshold=self._magnitude_threshold,\n        )\n        if display_results:\n            self._display_results()\n        if not (self._optimize_normalization_weights):\n            self._cleanup()\n        else:\n            self._extract_barcodes(minimum_pixels=minimum_pixels)\n\n    def optimize_normalization_by_decoding(\n        self,\n        n_random_tiles: int = 10,\n        n_iterations: int = 10,\n        minimum_pixels: float = 3.0,\n        ufish_threshold: float = 0.5,\n        lowpass_sigma: Optional[Sequence[float]] = (3, 1, 1),\n    ):\n        \"\"\"Optimize normalization by decoding.\n\n        Helper function to iteratively optimize normalization by decoding.\n\n        Parameters\n        ----------\n        n_random_tiles : int, default 10\n            Number of random tiles. \n        n_iterations : int, default 10\n            Number of iterations. \n        minimum_pixels : float, default 3.0\n            Minimum number of pixels for a barcode. \n        ufish_threshold : float, default 0.5\n            Ufish threshold. \n        lowpass_sigma : Optional[Sequence[float]], default (3, 1, 1)\n            Lowpass sigma. \n        \"\"\"\n\n        self._optimize_normalization_weights = True\n        self._temp_dir = Path(tempfile.mkdtemp())\n\n        if len(self._datastore.tile_ids) &gt; n_random_tiles and not(n_random_tiles==1):\n            random_tiles = sample(range(len(self._datastore.tile_ids)), n_random_tiles)\n        else:\n            random_tiles = range(len(self._datastore.tile_ids))\n\n        if self._verbose &gt;= 1:\n            iterable_iteration = tqdm(range(n_iterations), desc=\"iteration\", leave=True)\n        else:\n            iterable_iteration = range(n_iterations)\n\n        self._load_global_normalization_vectors()\n        self._iterative_background_vector = None\n        self._iterative_normalization_vector = None\n        for iteration in iterable_iteration:\n            if self._verbose &gt;= 1:\n                iterable_tiles = tqdm(random_tiles, desc=\"tile\", leave=True)\n            else:\n                iterable_tiles = random_tiles\n            if iteration &gt; 0:\n                self._load_iterative_normalization_vectors()\n            for tile_idx in iterable_tiles:\n                if iteration == 0:\n                    use_normalization = False\n                else:\n                    use_normalization = True\n                self.decode_one_tile(\n                    tile_idx=tile_idx,\n                    display_results=False,\n                    lowpass_sigma=lowpass_sigma,\n                    minimum_pixels=minimum_pixels,\n                    ufish_threshold=ufish_threshold,\n                    use_normalization=use_normalization,\n                )\n                self._save_barcodes()\n            self._load_all_barcodes()\n            if self._verbose &gt;= 1:\n                print(\"---\")\n                print(\"Total # of barcodes: \" + str(len(self._df_barcodes_loaded)))\n                print(\"---\")\n            self._iterative_normalization_vectors()\n        self._cleanup()\n        self._optimize_normalization_weights = False\n        shutil.rmtree(self._temp_dir)\n\n    def decode_all_tiles(\n        self,\n        assign_to_cells: bool = True,\n        prep_for_baysor: bool = True,\n        lowpass_sigma: Optional[Sequence[float]] = (3, 1, 1),\n        minimum_pixels: Optional[float] = 2.0,\n        ufish_threshold: Optional[float] = 0.5,\n        fdr_target: Optional[float] = 0.05,\n    ):\n        \"\"\"Decode all tiles.\n\n        Helper function to decode all tiles. Assumes iterative normalization has been performed.\n\n        Parameters\n        ----------\n        assign_to_cells : bool, default True\n            Assign barcodes to cells. \n        prep_for_baysor : bool, default True\n            Prepare barcodes for Baysor. \n        lowpass_sigma : Optional[Sequence[float]], default (3, 1, 1)\n            Lowpass sigma. \n        minimum_pixels : Optional[float], default 2.0\n            Minimum number of pixels for a barcode. \n        ufish_threshold : Optional[float], default 0.5\n            Ufish threshold. \n        fdr_target : Optional[float], default 0.05\n            False discovery rate target. \n        \"\"\"\n\n        if self._verbose &gt;= 1:\n            iterable_tile_id = enumerate(\n                tqdm(self._datastore.tile_ids, desc=\"tile\", leave=False)\n            )\n        else:\n            iterable_tile_id = enumerate(self._datastore.tile_ids)\n\n        self._optimize_normalization_weights = False\n        self._load_iterative_normalization_vectors()\n\n        if not (self._iterative_normalization_loaded):\n            raise ValueError(\"Perform iterative normalization before decoding.\")\n\n        for tile_idx, _ in iterable_tile_id:\n            self._tile_idx = tile_idx\n            self._load_bit_data(ufish_threshold=ufish_threshold)\n            if not (np.any(lowpass_sigma == 0)):\n                self._lp_filter(sigma=lowpass_sigma)\n            self._decode_pixels(\n                distance_threshold=self._distance_threshold,\n                magnitude_threshold=self._magnitude_threshold,\n            )\n            self._extract_barcodes(minimum_pixels=minimum_pixels)\n            self._save_barcodes()\n            self._cleanup()\n\n        self._load_tile_decoding = True\n        self._load_all_barcodes()\n        self._load_tile_decoding = False\n        self._verbose = 2\n        self._filter_all_barcodes_LR(fdr_target=fdr_target)\n        self._verbose = 1\n        self._remove_duplicates_in_tile_overlap()\n        if assign_to_cells:\n            self._assign_cells()\n        self._save_barcodes()\n        if prep_for_baysor:\n            self._reformat_barcodes_for_baysor()\n        self._cleanup()\n\n    def optimize_filtering(\n        self,\n        assign_to_cells: bool = False,\n        prep_for_baysor: bool = True,\n        fdr_target: Optional[float] = 0.05,\n    ):\n        \"\"\"Optimize filtering.\n\n        Helper function to opimize filtering for already decoded spots.\n\n        Parameters\n        ----------\n        assign_to_cells : bool, default False\n            Assign barcodes to cells. \n        prep_for_baysor : bool, default True\n            Prepare barcodes for Baysor. \n        fdr_target : Optional[float], default 0.05\n            False discovery rate target. \n        \"\"\"\n\n        self._load_tile_decoding = True\n        self._load_all_barcodes()\n        self._load_tile_decoding = False\n        self._verbose = 2\n        self._filter_all_barcodes(fdr_target=fdr_target)\n        self._verbose = 1\n        self._remove_duplicates_in_tile_overlap()\n        if assign_to_cells:\n            self._assign_cells()\n        self._save_barcodes(format=\"parquet\")\n        if prep_for_baysor:\n            self._reformat_barcodes_for_baysor()\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder.calculate_fdr","title":"<code>calculate_fdr(df, threshold, blank_count, barcode_count, verbose=False)</code>  <code>staticmethod</code>","text":"<p>Calculate false discovery rate.</p> <p>(# noncoding found ) / (# noncoding in codebook) / (# coding found) / (# coding in codebook)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe containing decoded spots.</p> required <code>threshold</code> <code>float</code> <p>Threshold for predicted probability.</p> required <code>blank_count</code> <code>int</code> <p>Number of blank barcodes.</p> required <code>barcode_count</code> <code>int</code> <p>Number of barcodes.</p> required <code>verbose</code> <code>bool = False</code> <p>Verbose output. Default False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>fdr</code> <code>float</code> <p>False discovery rate.</p> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>@staticmethod\ndef calculate_fdr(\n    df: pd.DataFrame, \n    threshold: float, \n    blank_count: int, \n    barcode_count: int, \n    verbose: bool = False) -&gt; float:\n    \"\"\"Calculate false discovery rate.\n\n    (# noncoding found ) / (# noncoding in codebook) / (# coding found) / (# coding in codebook)\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Dataframe containing decoded spots.\n    threshold : float\n        Threshold for predicted probability.\n    blank_count : int\n        Number of blank barcodes.\n    barcode_count : int\n        Number of barcodes.\n    verbose : bool = False\n        Verbose output. Default False.\n\n    Returns\n    -------\n    fdr : float\n        False discovery rate.\n    \"\"\"\n\n    if threshold &gt;= 0:\n        df[\"prediction\"] = df[\"predicted_probability\"] &gt; threshold\n\n        coding = df[\n            (~df[\"gene_id\"].str.startswith(\"Blank\"))\n            &amp; (df[\"predicted_probability\"] &gt; threshold)\n        ].shape[0]\n        noncoding = df[\n            (df[\"gene_id\"].str.startswith(\"Blank\"))\n            &amp; (df[\"predicted_probability\"] &gt; threshold)\n        ].shape[0]\n    else:\n        coding = df[(~df[\"gene_id\"].str.startswith(\"Blank\"))].shape[0]\n        noncoding = df[(df[\"gene_id\"].str.startswith(\"Blank\"))].shape[0]\n\n    if coding &gt; 0:\n        fdr = (noncoding / blank_count) / (coding / (barcode_count - blank_count))\n    else:\n        fdr = np.inf\n\n    if verbose &gt; 1:\n        print(f\"threshold: {threshold}\")\n        print(f\"coding: {coding}\")\n        print(f\"noncoding: {noncoding}\")\n        print(f\"fdr: {fdr}\")\n\n    return fdr\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder.decode_all_tiles","title":"<code>decode_all_tiles(assign_to_cells=True, prep_for_baysor=True, lowpass_sigma=(3, 1, 1), minimum_pixels=2.0, ufish_threshold=0.5, fdr_target=0.05)</code>","text":"<p>Decode all tiles.</p> <p>Helper function to decode all tiles. Assumes iterative normalization has been performed.</p> <p>Parameters:</p> Name Type Description Default <code>assign_to_cells</code> <code>bool</code> <p>Assign barcodes to cells.</p> <code>True</code> <code>prep_for_baysor</code> <code>bool</code> <p>Prepare barcodes for Baysor.</p> <code>True</code> <code>lowpass_sigma</code> <code>Optional[Sequence[float]]</code> <p>Lowpass sigma.</p> <code>(3, 1, 1)</code> <code>minimum_pixels</code> <code>Optional[float]</code> <p>Minimum number of pixels for a barcode.</p> <code>2.0</code> <code>ufish_threshold</code> <code>Optional[float]</code> <p>Ufish threshold.</p> <code>0.5</code> <code>fdr_target</code> <code>Optional[float]</code> <p>False discovery rate target.</p> <code>0.05</code> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def decode_all_tiles(\n    self,\n    assign_to_cells: bool = True,\n    prep_for_baysor: bool = True,\n    lowpass_sigma: Optional[Sequence[float]] = (3, 1, 1),\n    minimum_pixels: Optional[float] = 2.0,\n    ufish_threshold: Optional[float] = 0.5,\n    fdr_target: Optional[float] = 0.05,\n):\n    \"\"\"Decode all tiles.\n\n    Helper function to decode all tiles. Assumes iterative normalization has been performed.\n\n    Parameters\n    ----------\n    assign_to_cells : bool, default True\n        Assign barcodes to cells. \n    prep_for_baysor : bool, default True\n        Prepare barcodes for Baysor. \n    lowpass_sigma : Optional[Sequence[float]], default (3, 1, 1)\n        Lowpass sigma. \n    minimum_pixels : Optional[float], default 2.0\n        Minimum number of pixels for a barcode. \n    ufish_threshold : Optional[float], default 0.5\n        Ufish threshold. \n    fdr_target : Optional[float], default 0.05\n        False discovery rate target. \n    \"\"\"\n\n    if self._verbose &gt;= 1:\n        iterable_tile_id = enumerate(\n            tqdm(self._datastore.tile_ids, desc=\"tile\", leave=False)\n        )\n    else:\n        iterable_tile_id = enumerate(self._datastore.tile_ids)\n\n    self._optimize_normalization_weights = False\n    self._load_iterative_normalization_vectors()\n\n    if not (self._iterative_normalization_loaded):\n        raise ValueError(\"Perform iterative normalization before decoding.\")\n\n    for tile_idx, _ in iterable_tile_id:\n        self._tile_idx = tile_idx\n        self._load_bit_data(ufish_threshold=ufish_threshold)\n        if not (np.any(lowpass_sigma == 0)):\n            self._lp_filter(sigma=lowpass_sigma)\n        self._decode_pixels(\n            distance_threshold=self._distance_threshold,\n            magnitude_threshold=self._magnitude_threshold,\n        )\n        self._extract_barcodes(minimum_pixels=minimum_pixels)\n        self._save_barcodes()\n        self._cleanup()\n\n    self._load_tile_decoding = True\n    self._load_all_barcodes()\n    self._load_tile_decoding = False\n    self._verbose = 2\n    self._filter_all_barcodes_LR(fdr_target=fdr_target)\n    self._verbose = 1\n    self._remove_duplicates_in_tile_overlap()\n    if assign_to_cells:\n        self._assign_cells()\n    self._save_barcodes()\n    if prep_for_baysor:\n        self._reformat_barcodes_for_baysor()\n    self._cleanup()\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder.decode_one_tile","title":"<code>decode_one_tile(tile_idx=0, display_results=False, lowpass_sigma=(3, 1, 1), minimum_pixels=3.0, use_normalization=True, ufish_threshold=0.5)</code>","text":"<p>Decode one tile.</p> <p>Helper function to decode one tile. Can also display results in napari.</p> <p>Parameters:</p> Name Type Description Default <code>tile_idx</code> <code>int</code> <p>Tile index.</p> <code>0</code> <code>display_results</code> <code>bool</code> <p>Display results in napari.</p> <code>False</code> <code>lowpass_sigma</code> <code>Optional[Sequence[float]]</code> <p>Lowpass sigma.</p> <code>(3, 1, 1)</code> <code>minimum_pixels</code> <code>Optional[float]</code> <p>Minimum number of pixels for a barcode.</p> <code>3.0</code> <code>use_normalization</code> <code>Optional[bool]</code> <p>Use normalization.</p> <code>True</code> <code>ufish_threshold</code> <code>Optional[float]</code> <p>Ufish threshold.</p> <code>0.5</code> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def decode_one_tile(\n    self,\n    tile_idx: int = 0,\n    display_results: bool = False,\n    lowpass_sigma: Optional[Sequence[float]] = (3, 1, 1),\n    minimum_pixels: Optional[float] = 3.0,\n    use_normalization: Optional[bool] = True,\n    ufish_threshold: Optional[float] = 0.5,\n):\n    \"\"\"Decode one tile.\n\n    Helper function to decode one tile. Can also display results in napari.\n\n    Parameters\n    ----------\n    tile_idx : int, default 0\n        Tile index.\n    display_results : bool, default False\n        Display results in napari. \n    lowpass_sigma : Optional[Sequence[float]], default (3, 1, 1)\n        Lowpass sigma. \n    minimum_pixels : Optional[float], default 3.0\n        Minimum number of pixels for a barcode. \n    use_normalization : Optional[bool], default True\n        Use normalization. \n    ufish_threshold : Optional[float], default 0.5\n        Ufish threshold. \n    \"\"\"\n\n    if use_normalization:\n        self._load_iterative_normalization_vectors()\n\n    self._tile_idx = tile_idx\n    self._load_bit_data(ufish_threshold=ufish_threshold)\n    if not (np.any(lowpass_sigma == 0)):\n        self._lp_filter(sigma=lowpass_sigma)\n    self._decode_pixels(\n        distance_threshold=self._distance_threshold,\n        magnitude_threshold=self._magnitude_threshold,\n    )\n    if display_results:\n        self._display_results()\n    if not (self._optimize_normalization_weights):\n        self._cleanup()\n    else:\n        self._extract_barcodes(minimum_pixels=minimum_pixels)\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder.optimize_filtering","title":"<code>optimize_filtering(assign_to_cells=False, prep_for_baysor=True, fdr_target=0.05)</code>","text":"<p>Optimize filtering.</p> <p>Helper function to opimize filtering for already decoded spots.</p> <p>Parameters:</p> Name Type Description Default <code>assign_to_cells</code> <code>bool</code> <p>Assign barcodes to cells.</p> <code>False</code> <code>prep_for_baysor</code> <code>bool</code> <p>Prepare barcodes for Baysor.</p> <code>True</code> <code>fdr_target</code> <code>Optional[float]</code> <p>False discovery rate target.</p> <code>0.05</code> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def optimize_filtering(\n    self,\n    assign_to_cells: bool = False,\n    prep_for_baysor: bool = True,\n    fdr_target: Optional[float] = 0.05,\n):\n    \"\"\"Optimize filtering.\n\n    Helper function to opimize filtering for already decoded spots.\n\n    Parameters\n    ----------\n    assign_to_cells : bool, default False\n        Assign barcodes to cells. \n    prep_for_baysor : bool, default True\n        Prepare barcodes for Baysor. \n    fdr_target : Optional[float], default 0.05\n        False discovery rate target. \n    \"\"\"\n\n    self._load_tile_decoding = True\n    self._load_all_barcodes()\n    self._load_tile_decoding = False\n    self._verbose = 2\n    self._filter_all_barcodes(fdr_target=fdr_target)\n    self._verbose = 1\n    self._remove_duplicates_in_tile_overlap()\n    if assign_to_cells:\n        self._assign_cells()\n    self._save_barcodes(format=\"parquet\")\n    if prep_for_baysor:\n        self._reformat_barcodes_for_baysor()\n</code></pre>"},{"location":"reference/classes/PixelDecoder/#merfish3danalysis.PixelDecoder.PixelDecoder.optimize_normalization_by_decoding","title":"<code>optimize_normalization_by_decoding(n_random_tiles=10, n_iterations=10, minimum_pixels=3.0, ufish_threshold=0.5, lowpass_sigma=(3, 1, 1))</code>","text":"<p>Optimize normalization by decoding.</p> <p>Helper function to iteratively optimize normalization by decoding.</p> <p>Parameters:</p> Name Type Description Default <code>n_random_tiles</code> <code>int</code> <p>Number of random tiles.</p> <code>10</code> <code>n_iterations</code> <code>int</code> <p>Number of iterations.</p> <code>10</code> <code>minimum_pixels</code> <code>float</code> <p>Minimum number of pixels for a barcode.</p> <code>3.0</code> <code>ufish_threshold</code> <code>float</code> <p>Ufish threshold.</p> <code>0.5</code> <code>lowpass_sigma</code> <code>Optional[Sequence[float]]</code> <p>Lowpass sigma.</p> <code>(3, 1, 1)</code> Source code in <code>src/merfish3danalysis/PixelDecoder.py</code> <pre><code>def optimize_normalization_by_decoding(\n    self,\n    n_random_tiles: int = 10,\n    n_iterations: int = 10,\n    minimum_pixels: float = 3.0,\n    ufish_threshold: float = 0.5,\n    lowpass_sigma: Optional[Sequence[float]] = (3, 1, 1),\n):\n    \"\"\"Optimize normalization by decoding.\n\n    Helper function to iteratively optimize normalization by decoding.\n\n    Parameters\n    ----------\n    n_random_tiles : int, default 10\n        Number of random tiles. \n    n_iterations : int, default 10\n        Number of iterations. \n    minimum_pixels : float, default 3.0\n        Minimum number of pixels for a barcode. \n    ufish_threshold : float, default 0.5\n        Ufish threshold. \n    lowpass_sigma : Optional[Sequence[float]], default (3, 1, 1)\n        Lowpass sigma. \n    \"\"\"\n\n    self._optimize_normalization_weights = True\n    self._temp_dir = Path(tempfile.mkdtemp())\n\n    if len(self._datastore.tile_ids) &gt; n_random_tiles and not(n_random_tiles==1):\n        random_tiles = sample(range(len(self._datastore.tile_ids)), n_random_tiles)\n    else:\n        random_tiles = range(len(self._datastore.tile_ids))\n\n    if self._verbose &gt;= 1:\n        iterable_iteration = tqdm(range(n_iterations), desc=\"iteration\", leave=True)\n    else:\n        iterable_iteration = range(n_iterations)\n\n    self._load_global_normalization_vectors()\n    self._iterative_background_vector = None\n    self._iterative_normalization_vector = None\n    for iteration in iterable_iteration:\n        if self._verbose &gt;= 1:\n            iterable_tiles = tqdm(random_tiles, desc=\"tile\", leave=True)\n        else:\n            iterable_tiles = random_tiles\n        if iteration &gt; 0:\n            self._load_iterative_normalization_vectors()\n        for tile_idx in iterable_tiles:\n            if iteration == 0:\n                use_normalization = False\n            else:\n                use_normalization = True\n            self.decode_one_tile(\n                tile_idx=tile_idx,\n                display_results=False,\n                lowpass_sigma=lowpass_sigma,\n                minimum_pixels=minimum_pixels,\n                ufish_threshold=ufish_threshold,\n                use_normalization=use_normalization,\n            )\n            self._save_barcodes()\n        self._load_all_barcodes()\n        if self._verbose &gt;= 1:\n            print(\"---\")\n            print(\"Total # of barcodes: \" + str(len(self._df_barcodes_loaded)))\n            print(\"---\")\n        self._iterative_normalization_vectors()\n    self._cleanup()\n    self._optimize_normalization_weights = False\n    shutil.rmtree(self._temp_dir)\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/","title":"DataStore Class","text":"<p>Interface to qi2lab MERFISH datastore.</p> <p>This module provides methods and attributes to create or interact with the qi2lab MERFISH datastore. The filestore structure is further described in the merfish3d-analysis documentation.</p> History: <ul> <li>2024/12: Refactored repo structure.</li> <li>2024/12: Updated docstrings and exception types.</li> <li>2024/07: Initial commit.</li> </ul>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore","title":"<code>qi2labDataStore</code>","text":"<p>API to qi2lab MERFISH store.</p> <p>Parameters:</p> Name Type Description Default <code>datastore_path</code> <code>Union[str, Path]</code> <p>Path to qi2lab MERFISH store</p> required Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>class qi2labDataStore:\n    \"\"\"API to qi2lab MERFISH store.\n\n    Parameters\n    ----------\n    datastore_path : Union[str, Path]\n        Path to qi2lab MERFISH store\n\n    \"\"\"\n\n    def __init__(self, datastore_path: Union[str, Path]):\n        compressor = {\n            \"id\": \"blosc\",\n            \"cname\": \"zstd\",\n            \"clevel\": 5,\n            \"shuffle\": 2,\n        }\n        self._zarrv2_spec = {\n            \"driver\": \"zarr\",\n            \"kvstore\": None,\n            \"metadata\": {\"compressor\": compressor},\n            \"open\": True,\n            \"assume_metadata\": False,\n            \"create\": True,\n            \"delete_existing\": False,\n        }\n\n        self._datastore_path = Path(datastore_path)\n        if self._datastore_path.exists():\n            self._parse_datastore()\n        else:\n            self._init_datastore()\n\n    @property\n    def datastore_state(self) -&gt; Optional[dict]:\n        \"\"\"Datastore state.\n\n        Returns\n        -------\n        datastore_state : Optional[dict]\n            Datastore state.\n        \"\"\"\n\n        return getattr(self, \"_datastore_state\", None)\n\n    @datastore_state.setter\n    def datastore_state(self, value: dict):\n        \"\"\"Set the datastore state.\n\n        Parameters\n        ----------\n        value : dict\n            New datastore state.\n        \"\"\"\n\n        if not hasattr(self, \"_datastore_state\") or self._datastore_state is None:\n            self._datastore_state = value\n        else:\n            self._datastore_state.update(value)\n        self._save_to_json(self._datastore_state, self._datastore_state_json_path)\n\n    @property\n    def microscope_type(self) -&gt; Optional[str]:\n        \"\"\"Microscope type.\n\n        Returns\n        -------\n        microscope_type : Optional[str]\n            Microscope type.\n        \"\"\"\n\n        return getattr(self, \"_microscope_type\", None)\n\n    @microscope_type.setter\n    def microscope_type(self, value: str):\n        \"\"\"Set the microscope type.\n\n        Parameters\n        ----------\n        value : str\n            New microscope type.\n        \"\"\"\n\n        self._microscope_type = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"microscope_type\"] = value\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def camera_model(self) -&gt; Optional[str]:\n        \"\"\"Camera model.\n\n        Returns\n        -------\n        camera_model : Optional[str]\n            Camera model.\n        \"\"\"\n\n        return getattr(self, \"_camera_model\", None)\n\n    @camera_model.setter\n    def camera_model(self, value: str):\n        \"\"\"Set the camera model.\n\n        Parameters\n        ----------\n        value : str\n            New camera model.\n        \"\"\"\n        self._camera_model = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"camera_model\"] = value\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def num_rounds(self) -&gt; Optional[int]:\n        \"\"\"Number of rounds.\n\n        Returns\n        -------\n        num_rounds : int\n            Number of rounds.\n        \"\"\"\n\n        return getattr(self, \"_num_rounds\", None)\n\n    @num_rounds.setter\n    def num_rounds(self, value: int):\n        \"\"\"Set the number of rounds.\n\n        Parameters\n        ----------\n        value : int\n            New number of rounds.\n        \"\"\"\n\n        self._num_rounds = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"num_rounds\"] = value\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def num_bits(self) -&gt; int:\n        \"\"\"Number of bits.\n\n        Returns\n        -------\n        num_bits : int\n            Number of bits.\n        \"\"\"\n        return getattr(self, \"_num_bits\", None)\n\n    @property\n    def num_tiles(self) -&gt; Optional[int]:\n        \"\"\"Number of tiles.\n\n        Returns\n        -------\n        num_tiles : int\n            Number of tiles.\n        \"\"\"\n\n        return getattr(self, \"_num_tiles\", None)\n\n    @num_tiles.setter\n    def num_tiles(self, value: int):\n        \"\"\"Set the number of tiles.\n\n        Parameters\n        ----------\n        value : int\n            New number of tiles.\n        \"\"\"\n\n        self._num_tiles = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"num_tiles\"] = value\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n        self._tile_ids = []\n        for tile_idx in range(value):\n            self._tile_ids.append(\"tile\" + str(tile_idx).zfill(4))\n\n    @property\n    def channels_in_data(self) -&gt; Optional[Collection[int]]:\n        \"\"\"Channel indices.\n\n        Returns\n        -------\n        channels_in_data : Collection[int]\n            Channel indices.\n        \"\"\"\n\n        return getattr(self, \"_channels_in_data\", None)\n\n    @channels_in_data.setter\n    def channels_in_data(self, value: Collection[int]):\n        \"\"\"Set the channels in the data.\n\n        Parameters\n        ----------\n        value : Collection[int]\n            New channels in data (int values starting from zero).\n        \"\"\"\n\n        self._channels_in_data = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"channels_in_data\"] = value\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def tile_overlap(self) -&gt; Optional[float]:\n        \"\"\"XY tile overlap.\n\n        Returns\n        -------\n        tile_overlap : float\n            XY tile overlap.\n        \"\"\"\n\n        return getattr(self, \"_tile_overlap\", None)\n\n    @tile_overlap.setter\n    def tile_overlap(self, value: float):\n        \"\"\"Set the tile overlap.\n\n        Parameters\n        ----------\n        value : float\n            New tile overlap.\n        \"\"\"\n\n        self._tile_overlap = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"tile_overlap\"] = value\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def binning(self) -&gt; Optional[int]:\n        \"\"\"Camera binning.\n\n        Returns\n        -------\n        binning : int\n            Camera binning.\n        \"\"\"\n\n        return getattr(self, \"_binning\", None)\n\n    @binning.setter\n    def binning(self, value: int):\n        \"\"\"Set the camera binning.\n\n        Parameters\n        ----------\n        value : int\n            New camera binning.\n        \"\"\"\n\n        self._binning = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"binning\"] = value\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def e_per_ADU(self) -&gt; Optional[float]:\n        \"\"\"Electrons per camera ADU.\n\n        Returns\n        -------\n        e_per_ADU : float\n            Electrons per camera ADU.\"\"\"\n\n        return getattr(self, \"_e_per_ADU\", None)\n\n    @e_per_ADU.setter\n    def e_per_ADU(self, value: float):\n        \"\"\"Set the camera conversion (e- per ADU).\n\n        Parameters\n        ----------\n        value : float\n            New camera conversion (e- per ADU).\n        \"\"\"\n\n        self._e_per_ADU = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"e_per_ADU\"] = value\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def na(self) -&gt; Optional[float]:\n        \"\"\"Detection objective numerical aperture (NA).\n\n        Returns\n        -------\n        na : float\n            Detection objective numerical aperture (NA).\n        \"\"\"\n\n        return getattr(self, \"_na\", None)\n\n    @na.setter\n    def na(self, value: float):\n        \"\"\"Set detection objective numerical aperture (NA).\n\n        Parameters\n        ----------\n        value: float\n            New detection objective numerical aperture (NA)\n        \"\"\"\n\n        self._na = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"na\"] = value\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def ri(self) -&gt; Optional[float]:\n        \"\"\"Detection objective refractive index (RI).\n\n        Returns\n        -------\n        ri : float\n            Detection objective refractive index (RI).\n        \"\"\"\n\n        return getattr(self, \"_ri\", None)\n\n    @ri.setter\n    def ri(self, value: float):\n        \"\"\"Set detection objective refractive index (RI).\n\n        Parameters\n        ----------\n        value: float\n            New detection objective refractive index (RI)\n        \"\"\"\n\n        self._ri = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"ri\"] = value\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def noise_map(self) -&gt; Optional[ArrayLike]:\n        \"\"\"Camera noise image.\n\n        Returns\n        -------\n        noise_map : ArrayLike\n            Camera noise image.\n        \"\"\"\n\n        return getattr(self, \"_noise_map\", None)\n\n    @noise_map.setter\n    def noise_map(self, value: ArrayLike):\n        \"\"\"Set the camera noise image.\n\n        Parameters\n        ----------\n        value : ArrayLike\n            New camera noise image.\n        \"\"\"\n\n        self._noise_map = value\n        current_local_zarr_path = str(self._calibrations_zarr_path / Path(\"noise_map\"))\n\n        try:\n            self._save_to_zarr_array(\n                value,\n                self._get_kvstore_key(current_local_zarr_path),\n                self._zarrv2_spec,\n                return_future=False,\n            )\n        except (IOError, OSError, ZarrError):\n            print(r\"Could not access calibrations.zarr/noise_map\")\n\n    @property\n    def channel_shading_maps(self) -&gt; Optional[ArrayLike]:\n        \"\"\"Channel shaiding images.\n\n        Returns\n        -------\n        channel_shading_maps : ArrayLike\n            Channel shading images.\n        \"\"\"\n\n        return getattr(self, \"_shading_maps\", None)\n\n    @channel_shading_maps.setter\n    def channel_shading_maps(self, value: ArrayLike):\n        \"\"\"Set the channel shading images.\n\n        Parameters\n        ----------\n        value : ArrayLike\n            New channel shading images.\n        \"\"\"\n\n        self._shading_maps = value\n        current_local_zarr_path = str(\n            self._calibrations_zarr_path / Path(\"shading_maps\")\n        )\n\n        try:\n            self._save_to_zarr_array(\n                value,\n                self._get_kvstore_key(current_local_zarr_path),\n                self._zarrv2_spec,\n                return_future=False,\n            )\n        except (IOError, OSError, ZarrError):\n            print(r\"Could not access calibrations.zarr/shading_maps\")\n\n    @property\n    def channel_psfs(self) -&gt; Optional[ArrayLike]:\n        \"\"\"Channel point spread functions (PSF).\n\n        Return\n        ------\n        channel_psfs : ArrayLike\n            Channel point spread functions (PSF).\n        \"\"\"\n\n        return getattr(self, \"_psfs\", None)\n\n    @channel_psfs.setter\n    def channel_psfs(self, value: ArrayLike):\n        \"\"\"Set the channel point spread functions (PSF).\n\n        Parameters\n        ----------\n        value : ArrayLike\n            New channel point spread functions (PSF).\n        \"\"\"\n\n        self._psfs = value\n        current_local_zarr_path = str(self._calibrations_zarr_path / Path(\"psf_data\"))\n\n        try:\n            self._save_to_zarr_array(\n                value,\n                self._get_kvstore_key(current_local_zarr_path),\n                self._zarrv2_spec.copy(),\n                return_future=False,\n            )\n        except (IOError, ValueError):\n            print(r\"Could not access calibrations.zarr/psf_data\")\n\n    @property\n    def experiment_order(self) -&gt; Optional[pd.DataFrame]:\n        \"\"\"Round and bit order.\n\n        Returns\n        -------\n        experiment_order : pd.DataFrame\n            Round and bit order.\n        \"\"\"\n\n        return getattr(self, \"_experiment_order\", None)\n\n    @experiment_order.setter\n    def experiment_order(self, value: Union[ArrayLike, pd.DataFrame]):\n        \"\"\"Set the round and bit order.\n\n        Parameters\n        ----------\n        value : Union[ArrayLike, pd.DataFrame]\n            New round and bit order.\n        \"\"\"\n\n        if isinstance(value, pd.DataFrame):\n            self._experiment_order = value\n        else:\n            channel_list = []\n            for idx in range(len(self._channels_in_data)):\n                channel_list.append(str(self._channels_in_data[idx]))\n            self._experiment_order = pd.DataFrame(\n                value, columns=channel_list, dtype=\"int64\"\n            )\n\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"exp_order\"] = self._experiment_order.values.tolist()\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n        self._num_rounds = int(value[-1, 0])\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"num_round\"] = self._num_rounds\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n        self._num_bits = int(np.max(value[:, 1:]))\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"num_bits\"] = self._num_bits\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n        self._round_ids = []\n        for round_idx in range(self._num_rounds):\n            self._round_ids.append(\"round\" + str(round_idx + 1).zfill(3))\n\n        self._bit_ids = []\n        for bit_idx in range(self._num_bits):\n            self._bit_ids.append(\"bit\" + str(bit_idx + 1).zfill(3))\n\n    @property\n    def codebook(self) -&gt; Optional[pd.DataFrame]:\n        \"\"\"Codebook.\n\n        Returns\n        -------\n        codebook : pd.DataFrame\n            Codebook.\n        \"\"\"\n\n        data = getattr(self, \"_codebook\", None)\n\n        if data is None:\n            return None\n        num_columns = len(data[0]) if data else 0\n        columns = [\"gene_id\"] + [f\"bit{i:02d}\" for i in range(1, num_columns)]\n\n        return pd.DataFrame(data, columns=columns)\n\n    @codebook.setter\n    def codebook(self, value: pd.DataFrame):\n        \"\"\"Set the codebook.\n\n        Parameters\n        ----------\n        value : pd.DataFrame\n            New codebook.\n        \"\"\"\n\n        self._codebook = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"codebook\"] = self._codebook.values.tolist()\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def voxel_size_zyx_um(self) -&gt; Optional[ArrayLike]:\n        \"\"\"Voxel size, zyx order (microns).\n\n        Returns\n        -------\n        voxel_size_zyx_um : ArrayLike\n            Voxel size, zyx order (microns).\n        \"\"\"\n\n        return getattr(self, \"_voxel_size_zyx_um\", None)\n\n    @voxel_size_zyx_um.setter\n    def voxel_size_zyx_um(self, value: ArrayLike):\n        \"\"\"Set the voxel size, zyx order (microns).\n\n        Parameters\n        ----------\n        value : ArrayLike\n            New voxel size, zyx order (microns).\n        \"\"\"\n\n        self._voxel_size_zyx_um = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"voxel_size_zyx_um\"] = value\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def baysor_path(self) -&gt; Union[Path,str]:\n        \"\"\"Baysor path\n\n        Returns\n        -------\n        baysor_path : Union[Path,str]\n            Baysor path.\n        \"\"\"\n\n        return getattr(self,\"_baysor_path\",None)\n\n    @baysor_path.setter\n    def baysor_path(self, value: Union[Path,str]):\n        \"\"\"Set the baysor path.\n\n        Parameters\n        ----------\n        value : Union[Path,str]\n            New baysor path.\n        \"\"\"\n\n        if value is None:\n            self._baysor_path = None\n            self._datastore_state[\"BaysorPath\"] = None\n        else:\n            self._baysor_path = Path(value)\n            self._datastore_state[\"BaysorPath\"] = str(self._baysor_path)\n        self._save_to_json(self._datastore_state, self._datastore_state_json_path)\n\n    @property\n    def baysor_options(self) -&gt; Union[Path,str]:\n        \"\"\"Baysor options\n\n        Returns\n        -------\n        baysor_options : Union[Path,str]\n            Baysor options.\n        \"\"\"\n        return getattr(self,\"_baysor_options\",None)\n\n    @baysor_options.setter\n    def baysor_options(self, value: Union[Path,str]):\n        \"\"\"Set the baysor options.\n\n        Parameters\n        ----------\n        value : Union[Path,str]\n            New baysor options.\n        \"\"\"\n\n        if value is None:\n            self._baysor_path = None\n            self._datastore_state[\"BaysorPath\"] = None\n        else:\n            self._baysor_options = Path(value)\n            self._datastore_state[\"BaysorOptions\"] = str(self._baysor_options)\n        self._save_to_json(self._datastore_state, self._datastore_state_json_path)\n\n    @property\n    def julia_threads(self) -&gt; int:\n        \"\"\"Julia thread number\n\n        Returns\n        -------\n        julia_threads : int\n            Julia thread number.\n        \"\"\"\n\n        return getattr(self,\"_julia_threads\",None)\n\n    @julia_threads.setter\n    def julia_threads(self, value: int):\n        \"\"\"Set the julia thread number.\n\n        Parameters\n        ----------\n        value : int\n            New julia thread number.\n        \"\"\"\n\n        self._julia_threads = value\n        self._datastore_state[\"JuliaThreads\"] = str(self._julia_threads)\n        self._save_to_json(self._datastore_state, self._datastore_state_json_path)\n\n    @property\n    def global_normalization_vector(self) -&gt; Optional[ArrayLike]:\n        \"\"\"Global normalization vector.\n\n        Returns\n        -------\n        global_normalization_vector : ArrayLike\n            Global normalization vector.\n        \"\"\"\n\n        value = getattr(self, \"_global_normalization_vector\", None)\n        if value is None:\n            zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n            calib_zattrs = self._load_from_json(zattrs_path)\n\n            try:\n                value = np.asarray(\n                    calib_zattrs[\"global_normalization_vector\"], dtype=np.float32\n                )\n                return value\n            except KeyError:\n                print(\"Global normalization vector not calculated.\")\n                return None\n        else:\n            return value\n\n    @global_normalization_vector.setter\n    def global_normalization_vector(self, value: ArrayLike):\n        \"\"\"Set the global normalization vector.\n\n        Parameters\n        ----------\n        value : ArrayLike\n            New global normalization vector.\n        \"\"\"\n\n        self._global_normalization_vector = np.asarray(value, dtype=np.float32)\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"global_normalization_vector\"] = (\n            self._global_normalization_vector.tolist()\n        )\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def global_background_vector(self) -&gt; Optional[ArrayLike]:\n        \"\"\"Global background vector.\n\n        Returns\n        -------\n        global_background_vector : ArrayLike\n            Global background vector.\n        \"\"\"\n\n        value = getattr(self, \"_global_background_vector\", None)\n        if value is None:\n            zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n            calib_zattrs = self._load_from_json(zattrs_path)\n            try:\n                value = np.asarray(\n                    calib_zattrs[\"global_background_vector\"], dtype=np.float32\n                )\n                return value\n            except KeyError:\n                print(\"Global background vector not calculated.\")\n                return None\n        else:\n            return value\n\n    @global_background_vector.setter\n    def global_background_vector(self, value: ArrayLike):\n        \"\"\"Set the global background vector.\n\n        Parameters\n        ----------\n        value : ArrayLike\n            New global background vector.\n        \"\"\"\n\n        self._global_background_vector = np.asarray(value, dtype=np.float32)\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"global_background_vector\"] = (\n            self._global_background_vector.tolist()\n        )\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def iterative_normalization_vector(self) -&gt; Optional[ArrayLike]:\n        \"\"\"Iterative normalization vector.\n\n        Returns\n        -------\n        iterative_normalization_vector : ArrayLike\n            Iterative normalization vector.\n        \"\"\"\n\n        value = getattr(self, \"_iterative_normalization_vector\", None)\n        if value is None:\n            zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n            calib_zattrs = self._load_from_json(zattrs_path)\n            try:\n                value = np.asarray(\n                    calib_zattrs[\"iterative_normalization_vector\"], dtype=np.float32\n                )\n            except KeyError:\n                value = None\n\n            if value is None:\n                print(\"Iterative normalization vector not calculated.\")\n                return None\n\n            return value\n        else:\n            return value\n\n    @iterative_normalization_vector.setter\n    def iterative_normalization_vector(self, value: ArrayLike):\n        \"\"\"Set the iterative normalization vector.\n\n        Parameters\n        ----------\n        value : ArrayLike\n            New iterative normalization vector.\n        \"\"\"\n\n        self._iterative_normalization_vector = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"iterative_normalization_vector\"] = (\n            self._iterative_normalization_vector.tolist()\n        )\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def iterative_background_vector(self) -&gt; Optional[ArrayLike]:\n        \"\"\"Iterative background vector.\n\n        Returns\n        -------\n        iterative_background_vector : ArrayLike\n            Iterative background vector.\n        \"\"\"\n\n        value = getattr(self, \"_iterative_background_vector\", None)\n        if value is None:\n            zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n            calib_zattrs = self._load_from_json(zattrs_path)\n            try:\n                value = np.asarray(\n                    calib_zattrs[\"iterative_background_vector\"], dtype=np.float32\n                )\n            except KeyError:\n                value = None\n            if value is None:\n                print(\"Iterative background vector not calculated.\")\n                return None\n\n            return value\n        else:\n            return value\n\n    @iterative_background_vector.setter\n    def iterative_background_vector(self, value: ArrayLike):\n        \"\"\"Set the iterative background vector.\n\n        Parameters\n        ----------\n        value : ArrayLike\n            New iterative background vector.\n        \"\"\"\n\n        self._iterative_background_vector = value\n        zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n        calib_zattrs = self._load_from_json(zattrs_path)\n        calib_zattrs[\"iterative_background_vector\"] = (\n            self._iterative_background_vector.tolist()\n        )\n        self._save_to_json(calib_zattrs, zattrs_path)\n\n    @property\n    def tile_ids(self) -&gt; Optional[Collection[str]]:\n        \"\"\"Tile IDs.\n\n        Returns\n        -------\n        tile_ids : Collection[str]\n            Tile IDs.\n        \"\"\"\n\n        return getattr(self, \"_tile_ids\", None)\n\n    @property\n    def round_ids(self) -&gt; Optional[Collection[str]]:\n        \"\"\"Round IDs.\n\n        Returns\n        -------\n        round_ids : Collection[str]\n            Round IDs.\n        \"\"\"\n\n        return getattr(self, \"_round_ids\", None)\n\n    @property\n    def bit_ids(self) -&gt; Optional[Collection[str]]:\n        \"\"\"Bit IDs.\n\n        Returns\n        -------\n        bit_ids : Collection[str]\n            Bit IDs.\n        \"\"\"\n\n        return getattr(self, \"_bit_ids\", None)\n\n    def _init_datastore(self):\n        \"\"\"Initialize datastore.\n\n        Create directory structure and initialize datastore state.\n        \"\"\"\n\n        self._datastore_path.mkdir(parents=True)\n        self._calibrations_zarr_path = self._datastore_path / Path(r\"calibrations.zarr\")\n        self._calibrations_zarr_path.mkdir()\n        calibrations_zattrs_path = self._calibrations_zarr_path / Path(r\".zattrs\")\n        empty_zattrs = {}\n        self._save_to_json(empty_zattrs, calibrations_zattrs_path)\n        self._polyDT_root_path = self._datastore_path / Path(r\"polyDT\")\n        self._polyDT_root_path.mkdir()\n        self._readouts_root_path = self._datastore_path / Path(r\"readouts\")\n        self._readouts_root_path.mkdir()\n        self._ufish_localizations_root_path = self._datastore_path / Path(\n            r\"ufish_localizations\"\n        )\n        self._ufish_localizations_root_path.mkdir()\n        self._decoded_root_path = self._datastore_path / Path(r\"decoded\")\n        self._decoded_root_path.mkdir()\n        self._fused_root_path = self._datastore_path / Path(r\"fused\")\n        self._fused_root_path.mkdir()\n        self._segmentation_root_path = self._datastore_path / Path(r\"segmentation\")\n        self._segmentation_root_path.mkdir()\n        self._mtx_output_root_path = self._datastore_path / Path(r\"mtx_output\")\n        self._mtx_output_root_path.mkdir()\n        self._baysor_path = r\"\"\n        self._baysor_options = r\"\"\n        self._julia_threads = 0\n\n        # initialize datastore state\n        self._datastore_state_json_path = self._datastore_path / Path(\n            r\"datastore_state.json\"\n        )\n        self._datastore_state = {\n            \"Version\": 0.3,\n            \"Initialized\": True,\n            \"Calibrations\": False,\n            \"Corrected\": False,\n            \"LocalRegistered\": False,\n            \"GlobalRegistered\": False,\n            \"Fused\": False,\n            \"SegmentedCells\": False,\n            \"DecodedSpots\": False,\n            \"FilteredSpots\": False,\n            \"RefinedSpots\": False,\n            \"mtxOutput\": False,\n            \"BaysorPath\": str(self._baysor_path),\n            \"BaysorOptions\": str(self._baysor_options),\n            \"JuliaThreads\": str(self._julia_threads)\n        }\n\n        self._save_to_json(self._datastore_state, self._datastore_state_json_path)\n\n    @staticmethod\n    def _get_kvstore_key(path: Union[Path, str]) -&gt; dict:\n        \"\"\"Convert datastore location to tensorstore kvstore key.\n\n        Parameters\n        ----------\n        path : Union[Path, str]\n            Datastore location.\n\n        Returns\n        -------\n        kvstore_key : dict\n            Tensorstore kvstore key.\n        \"\"\"\n\n        path_str = str(path)\n        if path_str.startswith(\"s3://\") or \"s3.amazonaws.com\" in path_str:\n            return {\"driver\": \"s3\", \"path\": path_str}\n        elif path_str.startswith(\"gs://\") or \"storage.googleapis.com\" in path_str:\n            return {\"driver\": \"gcs\", \"path\": path_str}\n        elif path_str.startswith(\"azure://\") or \"blob.core.windows.net\" in path_str:\n            return {\"driver\": \"azure\", \"path\": path_str}\n        elif path_str.startswith(\"http://\") or path_str.startswith(\"https://\"):\n            raise ValueError(\"Unsupported cloud storage provider in URL\")\n        else:\n            return {\"driver\": \"file\", \"path\": path_str}\n\n    @staticmethod\n    def _load_from_json(dictionary_path: Union[Path, str]) -&gt; dict:\n        \"\"\"Load json as dictionary.\n\n        Parameters\n        ----------\n        dictionary_path : Union[Path, str]\n            Path to json file.\n\n        Returns\n        -------\n        dictionary : dict\n            Dictionary from json file.\n        \"\"\"\n\n        try:\n            with open(dictionary_path, \"r\") as f:\n                dictionary = json.load(f)\n        except (FileNotFoundError, json.JSONDecodeError):\n            dictionary = {}\n        return dictionary\n\n    @staticmethod\n    def _save_to_json(dictionary: dict, dictionary_path: Union[Path, str]):\n        \"\"\"Save dictionary to json.\n\n        Parameters\n        ----------\n        data : dict\n            The data to be saved.\n        path : Union[Path,str]\n            The path to the JSON file where the data will be saved.\n        \"\"\"\n\n        with open(dictionary_path, \"w\") as file:\n            json.dump(dictionary, file, indent=4)\n\n    @staticmethod\n    def _load_from_microjson(dictionary_path: Union[Path, str]) -&gt; dict:\n        \"\"\"Load cell outlines outlines microjson as dictionary.\n\n        Parameters\n        ----------\n        dictionary_path : Union[Path, str]\n            Path to microjson file.\n\n        Returns\n        -------\n        outlines : dict\n            Cell outlines dictionary.\n        \"\"\"\n\n        try:\n            with open(dictionary_path, \"r\") as f:\n                data = json.load(f)\n                outlines = {}\n                for feature in data[\"features\"]:\n                    cell_id = feature[\"properties\"][\"cell_id\"]\n                    coordinates = feature[\"geometry\"][\"coordinates\"][0]\n                    outlines[cell_id] = np.array(coordinates)\n        except (FileNotFoundError, json.JSONDecodeError, KeyError, TypeError, ValueError):\n            outlines = {}\n        return outlines\n\n    @staticmethod\n    def _check_for_zarr_array(kvstore: Union[Path, str], spec: dict):\n        \"\"\"Check if zarr array exists using Tensortore.\n\n        Parameters\n        ----------\n        kvstore : Union[Path, str]\n            Datastore location.\n        spec : dict\n            Zarr specification.\n        \"\"\"\n\n        current_zarr = ts.open(\n            {\n                **spec,\n                \"kvstore\": kvstore,\n            }\n        ).result()\n\n        del current_zarr\n\n    @staticmethod\n    def _load_from_zarr_array(\n        kvstore: dict, spec: dict, return_future=True\n    ) -&gt; ArrayLike:\n        \"\"\"Return tensorstore array from zarr\n\n        Defaults to returning future result.\n\n        Parameters\n        ----------\n        kvstore : dict\n            Tensorstore kvstore specification.\n        spec : dict\n            Tensorstore zarr specification.\n        return_future : bool\n            Return future (True) or immediately read (False).\n\n        Returns\n        -------\n        array : ArrayLike\n            Delayed (future) or immediate array.\n        \"\"\"\n\n        current_zarr = ts.open(\n            {\n                **spec,\n                \"kvstore\": kvstore,\n            }\n        ).result()\n\n        read_future = current_zarr.read()\n\n        if return_future:\n            return read_future\n        else:\n            return read_future.result()\n\n    @staticmethod\n    def _save_to_zarr_array(\n        array: ArrayLike,\n        kvstore: dict,\n        spec: dict,\n        return_future: Optional[bool] = False,\n    ) -&gt; Optional[ArrayLike]:\n        \"\"\"Save array to zarr using tensorstore.\n\n        Defaults to returning future result.\n\n        Parameters\n        ----------\n        array : ArrayLike\n            Array to save.\n        kvstore : dict\n            Tensorstore kvstore specification.\n        spec : dict\n            Tensorstore zarr specification.\n        return_future : Optional[bool]\n            Return future (True) or immediately write (False).\n\n        Returns\n        -------\n        write_future : Optional[ArrayLike]\n            Delayed (future) if return_future is True.\n        \"\"\"\n\n        # check datatype\n        if str(array.dtype) == \"uint8\":\n            array_dtype = \"&lt;u1\"\n        elif str(array.dtype) == \"uint16\":\n            array_dtype = \"&lt;u2\"\n        elif str(array.dtype) == \"float16\":\n            array_dtype = \"&lt;f2\"\n        elif str(array.dtype) == \"float32\":\n            array_dtype = \"&lt;f4\"\n        else:\n            print(\"Unsupported data type: \" + str(array.dtype))\n            return None\n\n        # check array dimension\n        spec[\"metadata\"][\"shape\"] = array.shape\n        if len(array.shape) == 2:\n            spec[\"metadata\"][\"chunks\"] = [array.shape[0], array.shape[1]]\n        elif len(array.shape) == 3:\n            spec[\"metadata\"][\"chunks\"] = [1, array.shape[1], array.shape[2]]\n        elif len(array.shape) == 4:\n            spec[\"metadata\"][\"chunks\"] = [1, 1, array.shape[1], array.shape[2]]\n        spec[\"metadata\"][\"dtype\"] = array_dtype\n\n        try:\n            current_zarr = ts.open(\n                {\n                    **spec,\n                    \"kvstore\": kvstore,\n                }\n            ).result()\n\n            write_future = current_zarr.write(array)\n\n            if return_future:\n                return write_future\n            else:\n                write_future.result()\n                return None\n        except (IOError, OSError, TimeoutError):\n            print(\"Error writing zarr array.\")\n\n    @staticmethod\n    def _load_from_parquet(parquet_path: Union[Path, str]) -&gt; pd.DataFrame:\n        \"\"\"Load dataframe from parquet.\n\n        Parameters\n        ----------\n        parquet_path : Union[Path, str]\n            Path to parquet file.\n\n        Returns\n        -------\n        df : pd.DataFrame\n            Dataframe from parquet file.\n        \"\"\"\n\n        return pd.read_parquet(parquet_path)\n\n    @staticmethod\n    def _save_to_parquet(df: pd.DataFrame, parquet_path: Union[Path, str]):\n        \"\"\"Save dataframe to parquet.\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n            Dataframe to save.\n        parquet_path : Union[Path, str]\n            Path to parquet file.\n        \"\"\"\n\n        df.to_parquet(parquet_path)\n\n    def _parse_datastore(self):\n        \"\"\"Parse datastore to discover available components.\"\"\"\n\n        # directory structure as defined by qi2lab spec\n        self._calibrations_zarr_path = self._datastore_path / Path(r\"calibrations.zarr\")\n        self._polyDT_root_path = self._datastore_path / Path(r\"polyDT\")\n        self._readouts_root_path = self._datastore_path / Path(r\"readouts\")\n        self._ufish_localizations_root_path = self._datastore_path / Path(\n            r\"ufish_localizations\"\n        )\n        self._decoded_root_path = self._datastore_path / Path(r\"decoded\")\n        self._fused_root_path = self._datastore_path / Path(r\"fused\")\n        self._segmentation_root_path = self._datastore_path / Path(r\"segmentation\")\n        self._mtx_output_root_path = self._datastore_path / Path(r\"mtx_output\")\n        self._datastore_state_json_path = self._datastore_path / Path(\n            r\"datastore_state.json\"\n        )\n\n        # read in .json in root directory that indicates what steps have been run\n        with open(self._datastore_state_json_path, \"r\") as json_file:\n            self._datastore_state = json.load(json_file)\n\n        # validate calibrations.zarr\n        if self._datastore_state[\"Calibrations\"]:\n            if not (self._calibrations_zarr_path.exists()):\n                print(\"Calibration data error.\")\n            try:\n                zattrs_path = self._calibrations_zarr_path / Path(\".zattrs\")\n                attributes = self._load_from_json(zattrs_path)\n            except (FileNotFoundError, json.JSONDecodeError):\n                print(\"Calibration attributes not found\")\n\n            keys_to_check = [\n                \"num_rounds\",\n                \"num_tiles\",\n                \"channels_in_data\",\n                \"tile_overlap\",\n                \"binning\",\n                \"e_per_ADU\",\n                \"na\",\n                \"ri\",\n                \"exp_order\",\n                \"codebook\",\n                \"num_bits\"\n            ]\n            if self._datastore_state[\"Version\"] == 0.3:\n                keys_to_check.append(\"microscope_type\")\n                keys_to_check.append(\"camera_model\")\n                keys_to_check.append(\"voxel_size_zyx_um\")\n            for key in keys_to_check:\n                if key not in attributes.keys():\n                    raise KeyError(\"Calibration attributes incomplete\")\n                else:\n                    setattr(self, \"_\" + key, attributes[key])\n\n            current_local_zarr_path = str(\n                self._calibrations_zarr_path / Path(\"psf_data\")\n            )\n\n            try:\n                self._psfs = (\n                    self._load_from_zarr_array(\n                        kvstore=self._get_kvstore_key(current_local_zarr_path),\n                        spec=self._zarrv2_spec.copy(),\n                    )\n                ).result()\n            except (IOError, OSError, ZarrError):\n                print(\"Calibration psfs missing.\")\n\n            del current_local_zarr_path\n\n            # current_local_zarr_path = str(\n            #     self._calibrations_zarr_path / Path(\"noise_map\")\n            # )\n\n            # try:\n            #     self._noise_map = (\n            #         self._load_from_zarr_array(\n            #             kvstore=self._get_kvstore_key(current_local_zarr_path),\n            #             spec=self._zarrv2_spec,\n            #         )\n            #     ).result()\n            # except Exception:\n            #     print(\"Calibration noise map missing.\")\n\n        # validate polyDT and readout bits data\n        if self._datastore_state[\"Corrected\"]:\n            if not (self._polyDT_root_path.exists()):\n                raise FileNotFoundError(\"PolyDT directory not initialized\")\n            else:\n                polyDT_tile_ids = sorted(\n                    [\n                        entry.name\n                        for entry in self._polyDT_root_path.iterdir()\n                        if entry.is_dir()\n                    ],\n                    key=lambda x: int(x.split(\"tile\")[1].split(\".zarr\")[0]),\n                )\n                current_tile_dir_path = self._polyDT_root_path / Path(\n                    polyDT_tile_ids[0]\n                )\n                self._round_ids = sorted(\n                    [\n                        entry.name.split(\".\")[0]\n                        for entry in current_tile_dir_path.iterdir()\n                        if entry.is_dir()\n                    ],\n                    key=lambda x: int(x.split(\"round\")[1].split(\".zarr\")[0]),\n                )\n            if not (self._readouts_root_path.exists()):\n                raise FileNotFoundError(\"Readout directory not initialized\")\n            else:\n                readout_tile_ids = sorted(\n                    [\n                        entry.name\n                        for entry in self._readouts_root_path.iterdir()\n                        if entry.is_dir()\n                    ],\n                    key=lambda x: int(x.split(\"tile\")[1].split(\".zarr\")[0]),\n                )\n                current_tile_dir_path = self._readouts_root_path / Path(\n                    readout_tile_ids[0]\n                )\n                self._bit_ids = sorted(\n                    [\n                        entry.name.split(\".\")[0]\n                        for entry in current_tile_dir_path.iterdir()\n                        if entry.is_dir()\n                    ],\n                    key=lambda x: int(x.split(\"bit\")[1].split(\".zarr\")[0]),\n                )\n            assert (\n                polyDT_tile_ids == readout_tile_ids\n            ), \"polyDT and readout tile ids do not match. Conversion error.\"\n            self._tile_ids = polyDT_tile_ids.copy()\n            del polyDT_tile_ids, readout_tile_ids\n\n            for tile_id, round_id in product(self._tile_ids, self._round_ids):\n                try:\n                    zattrs_path = str(\n                        self._polyDT_root_path\n                        / Path(tile_id)\n                        / Path(round_id + \".zarr\")\n                        / Path(\".zattrs\")\n                    )\n                    attributes = self._load_from_json(zattrs_path)\n                except (FileNotFoundError, json.JSONDecodeError):\n                    print(\"polyDT tile attributes not found\")\n\n                keys_to_check = [\n                    \"stage_zyx_um\",\n                    \"excitation_um\",\n                    \"emission_um\",\n                    \"bit_linker\",\n                    # \"exposure_ms\",\n                    \"psf_idx\",\n                ]\n\n                for key in keys_to_check:\n                    if key not in attributes.keys():\n                        print(tile_id, round_id, key)\n                        raise KeyError(\"Corrected polyDT attributes incomplete\")\n\n                current_local_zarr_path = str(\n                    self._polyDT_root_path\n                    / Path(tile_id)\n                    / Path(round_id + \".zarr\")\n                    / Path(\"corrected_data\")\n                )\n\n                try:\n                    self._check_for_zarr_array(\n                        self._get_kvstore_key(current_local_zarr_path),\n                        self._zarrv2_spec.copy(),\n                    )\n                except (IOError, OSError, ZarrError):\n                    print(tile_id, round_id)\n                    print(\"Corrected polyDT data missing.\")\n\n            for tile_id, bit_id in product(self._tile_ids, self._bit_ids):\n                try:\n                    zattrs_path = str(\n                        self._readouts_root_path\n                        / Path(tile_id)\n                        / Path(bit_id + \".zarr\")\n                        / Path(\".zattrs\")\n                    )\n                    attributes = self._load_from_json(zattrs_path)\n                except (FileNotFoundError, json.JSONDecodeError):\n                    print(\"Readout tile attributes not found\")\n\n                keys_to_check = [\n                    \"excitation_um\",\n                    \"emission_um\",\n                    \"round_linker\",\n                    # \"exposure_ms\",\n                    \"psf_idx\",\n                ]\n                for key in keys_to_check:\n                    if key not in attributes.keys():\n                        raise KeyError(\"Corrected readout attributes incomplete\")\n\n                current_local_zarr_path = str(\n                    self._readouts_root_path\n                    / Path(tile_id)\n                    / Path(bit_id + \".zarr\")\n                    / Path(\"corrected_data\")\n                )\n\n                try:\n                    self._check_for_zarr_array(\n                        self._get_kvstore_key(current_local_zarr_path),\n                        self._zarrv2_spec.copy(),\n                    )\n                except (IOError, OSError, ZarrError):\n                    print(tile_id, bit_id)\n                    print(\"Corrected readout data missing.\")\n\n        # check and validate local registered data\n        if self._datastore_state[\"LocalRegistered\"]:\n            for tile_id, round_id in product(self._tile_ids, self._round_ids):\n                if round_id is not self._round_ids[0]:\n                    try:\n                        zattrs_path = str(\n                            self._polyDT_root_path\n                            / Path(tile_id)\n                            / Path(round_id + \".zarr\")\n                            / Path(\".zattrs\")\n                        )\n                        with open(zattrs_path, \"r\") as f:\n                            attributes = json.load(f)\n                    except (FileNotFoundError, json.JSONDecodeError):\n                        print(\"polyDT tile attributes not found\")\n\n                    keys_to_check = [\"rigid_xform_xyz_px\"]\n\n                    for key in keys_to_check:\n                        if key not in attributes.keys():\n                            raise KeyError(\"Rigid registration missing\")\n\n                    current_local_zarr_path = str(\n                        self._polyDT_root_path\n                        / Path(tile_id)\n                        / Path(round_id + \".zarr\")\n                        / Path(\"of_xform_px\")\n                    )\n\n                    try:\n                        self._check_for_zarr_array(\n                            self._get_kvstore_key(current_local_zarr_path),\n                            self._zarrv2_spec.copy(),\n                        )\n                    except (IOError, OSError, ZarrError):\n                        print(tile_id, round_id)\n                        print(\"Optical flow registration data missing.\")\n\n                current_local_zarr_path = str(\n                    self._polyDT_root_path\n                    / Path(tile_id)\n                    / Path(round_id + \".zarr\")\n                    / Path(\"registered_decon_data\")\n                )\n                if round_id is self._round_ids[0]:\n                    try:\n                        self._check_for_zarr_array(\n                            self._get_kvstore_key(current_local_zarr_path),\n                            self._zarrv2_spec.copy(),\n                        )\n                    except (IOError, OSError, ZarrError):\n                        print(tile_id, round_id)\n                        print(\"Registered polyDT data missing.\")\n\n            for tile_id, bit_id in product(self._tile_ids, self._bit_ids):\n                current_local_zarr_path = str(\n                    self._readouts_root_path\n                    / Path(tile_id)\n                    / Path(bit_id + \".zarr\")\n                    / Path(\"registered_decon_data\")\n                )\n\n                try:\n                    self._check_for_zarr_array(\n                        self._get_kvstore_key(current_local_zarr_path),\n                        self._zarrv2_spec.copy(),\n                    )\n                except (IOError, OSError, ZarrError):\n                    print(tile_id, round_id)\n                    print(\"Registered readout data missing.\")\n\n                current_local_zarr_path = str(\n                    self._readouts_root_path\n                    / Path(tile_id)\n                    / Path(bit_id + \".zarr\")\n                    / Path(\"registered_ufish_data\")\n                )\n\n                try:\n                    self._check_for_zarr_array(\n                        self._get_kvstore_key(current_local_zarr_path),\n                        self._zarrv2_spec.copy(),\n                    )\n                except (IOError, OSError, ZarrError):\n                    print(tile_id, round_id)\n                    print(\"Registered ufish prediction missing.\")\n\n            for tile_id, bit_id in product(self._tile_ids, self._bit_ids):\n                current_ufish_path = (\n                    self._ufish_localizations_root_path\n                    / Path(tile_id)\n                    / Path(bit_id + \".parquet\")\n                )\n                if not (current_ufish_path.exists()):\n                    raise FileNotFoundError(\n                        tile_id + \" \" + bit_id + \" ufish localization missing\"\n                    )\n\n        # check and validate global registered data\n        if self._datastore_state[\"GlobalRegistered\"]:\n            for tile_id in self._tile_ids:\n                try:\n                    zattrs_path = str(\n                        self._polyDT_root_path\n                        / Path(tile_id)\n                        / Path(self._round_ids[0] + \".zarr\")\n                        / Path(\".zattrs\")\n                    )\n                    with open(zattrs_path, \"r\") as f:\n                        attributes = json.load(f)\n                except (FileNotFoundError, json.JSONDecodeError):\n                    print(\"polyDT tile attributes not found\")\n\n                keys_to_check = [\"affine_zyx_um\", \"origin_zyx_um\", \"spacing_zyx_um\"]\n\n                for key in keys_to_check:\n                    if key not in attributes.keys():\n                        raise KeyError(\"Global registration missing\")\n\n        # check and validate fused\n        if self._datastore_state[\"Fused\"]:\n            try:\n                zattrs_path = str(\n                    self._fused_root_path\n                    / Path(\"fused.zarr\")\n                    / Path(\"fused_polyDT_iso_zyx\")\n                    / Path(\".zattrs\")\n                )\n                with open(zattrs_path, \"r\") as f:\n                    attributes = json.load(f)\n            except (FileNotFoundError, json.JSONDecodeError):\n                print(\"Fused image attributes not found\")\n\n            keys_to_check = [\"affine_zyx_um\", \"origin_zyx_um\", \"spacing_zyx_um\"]\n\n            for key in keys_to_check:\n                if key not in attributes.keys():\n                    raise KeyError(\"Fused image metadata missing\")\n\n            current_local_zarr_path = str(\n                self._fused_root_path\n                / Path(\"fused.zarr\")\n                / Path(\"fused_polyDT_iso_zyx\")\n            )\n\n            try:\n                self._check_for_zarr_array(\n                    self._get_kvstore_key(current_local_zarr_path),\n                    self._zarrv2_spec.copy(),\n                )\n            except (IOError, OSError, ZarrError):\n                print(\"Fused data missing.\")\n\n        # check and validate cellpose segmentation\n        if self._datastore_state[\"SegmentedCells\"]:\n            current_local_zarr_path = str(\n                self._segmentation_root_path\n                / Path(\"cellpose\")\n                / Path(\"cellpose.zarr\")\n                / Path(\"masks_polyDT_iso_zyx\")\n            )\n\n            try:\n                self._check_for_zarr_array(\n                    self._get_kvstore_key(current_local_zarr_path),\n                    self._zarrv2_spec.copy(),\n                )\n            except (IOError, OSError, ZarrError):\n                print(\"Cellpose data missing.\")\n\n            cell_outlines_path = (\n                self._segmentation_root_path\n                / Path(\"cellpose\")\n                / Path(\"imagej_rois\")\n                / Path(\"global_coords_rois.zip\")\n            )\n            if not (cell_outlines_path.exists()):\n                raise FileNotFoundError(\"Cellpose cell outlines missing.\")\n\n        # check and validate decoded spots\n        if self._datastore_state[\"DecodedSpots\"]:\n            for tile_id in self._tile_ids:\n                decoded_path = self._decoded_root_path / Path(\n                    tile_id + \"_decoded_features.parquet\"\n                )\n\n                if not (decoded_path.exists()):\n                    raise FileNotFoundError(tile_id + \" decoded spots missing.\")\n\n        # check and validate filtered decoded spots\n        if self._datastore_state[\"FilteredSpots\"]:\n            filtered_path = self._decoded_root_path / Path(\n                \"all_tiles_filtered_decoded_features.parquet\"\n            )\n\n            if not (filtered_path.exists()):\n                raise FileNotFoundError(\"filtered decoded spots missing.\")\n\n        if self._datastore_state[\"RefinedSpots\"]:\n            baysor_spots_path = (\n                self._segmentation_root_path\n                / Path(\"baysor\")\n                / Path(\"segmentation.csv\")\n            )\n\n            if not (baysor_spots_path.exists()):\n                raise FileNotFoundError(\"Baysor filtered decoded spots missing.\")\n\n        # check and validate mtx\n        if self._datastore_state[\"mtxOutput\"]:\n            mtx_barcodes_path = self._mtx_output_root_path / Path(\"barcodes.tsv.gz\")\n            mtx_features_path = self._mtx_output_root_path / Path(\"features.tsv.gz\")\n            mtx_matrix_path = self._mtx_output_root_path / Path(\"matrix.tsv.gz\")\n\n            if (\n                not (mtx_barcodes_path.exists())\n                or not (mtx_features_path.exists())\n                or not (mtx_matrix_path.exists())\n            ):\n                raise FileNotFoundError(\"mtx output missing.\")\n\n        try:\n            self._baysor_path = Path(str(self._datastore_state[\"BaysorPath\"]))\n            self._baysor_options = Path(str(self._datastore_state[\"BaysorOptions\"]))\n            self._julia_threads = int(self._datastore_state[\"JuliaThreads\"])\n        except KeyError:\n            self._baysor_path = r\"\"\n            self._baysor_options = r\"\"\n            self._julia_threads = 1\n\n    def load_codebook_parsed(\n        self,\n    ) -&gt; Optional[tuple[Collection[str], ArrayLike]]:\n        \"\"\"Load and split codebook into gene_ids and codebook matrix.\n\n        Returns\n        -------\n        gene_ids : Collection[str]\n            Gene IDs.\n        codebook_matrix : ArrayLike\n            Codebook matrix.\n        \"\"\"\n\n        try:\n            data = getattr(self, \"_codebook\", None)\n\n            if data is None:\n                return None\n            num_columns = len(data[0]) if data else 0\n            columns = [\"gene_id\"] + [f\"bit{i:02d}\" for i in range(1, num_columns)]\n            codebook_df = pd.DataFrame(data, columns=columns)\n\n            gene_ids = codebook_df.iloc[:, 0].tolist()\n            codebook_matrix = codebook_df.iloc[:, 1:].to_numpy().astype(int)\n            del data, codebook_df\n            return gene_ids, codebook_matrix\n        except (KeyError, ValueError, TypeError):\n            print(\"Error parsing codebook.\")\n            return None\n\n    def initialize_tile(\n        self,\n        tile: Union[int, str],\n    ):\n        \"\"\"Initialize directory structure for a tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n        \"\"\"\n\n        if getattr(self, \"_experiment_order\", None) is None:\n            print(\"Assign experimental order before creating tiles.\")\n            return None\n\n        if getattr(self, \"_num_tiles\", None) is None:\n            print(\"Assign number of tiles before creating tiles.\")\n            return None\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tile id.\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        try:\n            polyDT_tile_path = self._polyDT_root_path / Path(tile_id)\n            polyDT_tile_path.mkdir()\n            for round_idx, round_id in enumerate(self._round_ids):\n                polyDT_round_path = polyDT_tile_path / Path(round_id + \".zarr\")\n                polyDT_round_path.mkdir()\n                polydt_round_attrs_path = polyDT_round_path / Path(\".zattrs\")\n                round_attrs = {\n                    \"bit_linker\": self._experiment_order.to_numpy()[round_idx, 1:]\n                    .astype(int)\n                    .tolist(),\n                }\n                self._save_to_json(round_attrs, polydt_round_attrs_path)\n        except FileExistsError:\n            print(\"Error creating polyDT tile. Does it exist already?\")\n\n        try:\n            readout_tile_path = self._readouts_root_path / Path(tile_id)\n            readout_tile_path.mkdir()\n            for bit_idx, bit_id in enumerate(self._bit_ids):\n                readout_bit_path = readout_tile_path / Path(bit_id + \".zarr\")\n                readout_bit_path.mkdir()\n                readout_bit_attrs_path = readout_bit_path / Path(\".zattrs\")\n                fiducial_channel = str(self._channels_in_data[0])\n                readout_one_channel = str(self._channels_in_data[1])\n\n                if len(self._channels_in_data) == 3:\n                    readout_two_channel = str(self._channels_in_data[2])\n                    condition_one = self._experiment_order[readout_one_channel] == (\n                        bit_idx + 1\n                    )\n                    condition_two = self._experiment_order[readout_two_channel] == (\n                        bit_idx + 1\n                    )\n                    combined_condition = condition_one | condition_two\n\n                else:\n                    combined_condition = self._experiment_order[\n                        readout_one_channel\n                    ] == (bit_idx + 1)\n                matching_rows = self._experiment_order.loc[combined_condition]\n\n                bit_attrs = {\n                    \"round_linker\": int(matching_rows[fiducial_channel].values[0])\n                }\n                self._save_to_json(bit_attrs, readout_bit_attrs_path)\n        except FileExistsError:\n            print(\"Error creating readout tile. Does it exist already?\")\n\n    def load_local_bit_linker(\n        self,\n        tile: Union[int, str],\n        round: Union[int, str],\n    ) -&gt; Optional[Sequence[int]]:\n        \"\"\"Load readout bits linked to fidicual round for one tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n        round : Union[int, str]\n            Round index or round id.\n\n        Returns\n        -------\n        bit_linker : Optional[Sequence[int]]\n            Readout bits linked to fidicual round for one tile.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id.\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                round_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id.\")\n                return None\n            else:\n                round_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n\n        try:\n            zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(round_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n            attributes = self._load_from_json(zattrs_path)\n            return attributes[\"bits\"][1:]\n        except (FileNotFoundError, json.JSONDecodeError):\n            print(tile_id, round_id)\n            print(\"Bit linker attribute not found.\")\n            return None\n\n    def save_local_bit_linker(\n        self,\n        bit_linker: Sequence[int],\n        tile: Union[int, str],\n        round: Union[int, str],\n    ):\n        \"\"\"Save readout bits linked to fidicual round for one tile.\n\n        Parameters\n        ----------\n        bit_linker : Sequence[int]\n            Readout bits linked to fidicual round for one tile.\n        tile : Union[int, str]\n            Tile index or tile id.\n        round : Union[int, str]\n            Round index or round id.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id.\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                round_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id.\")\n                return None\n            else:\n                round_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n\n        try:\n            zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(round_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n            attributes = self._load_from_json(zattrs_path)\n            attributes[\"bits\"] = bit_linker\n            self._save_to_json(attributes, zattrs_path)\n        except (FileNotFoundError, json.JSONDecodeError):\n            print(tile_id, round_id)\n            print(\"Error writing bit linker attribute.\")\n            return None\n\n    def load_local_round_linker(\n        self,\n        tile: Union[int, str],\n        bit: Union[int, str],\n    ) -&gt; Optional[Sequence[int]]:\n        \"\"\"Load fidicual round linked to readout bit for one tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n        bit : Union[int, str]\n            Bit index or bit id.\n\n        Returns\n        -------\n        round_linker : Optional[Sequence[int]]\n            Fidicual round linked to readout bit for one tile.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id.\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(bit, int):\n            if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                return None\n            else:\n                bit_id = self._bit_ids[bit]\n        elif isinstance(bit, str):\n            if bit not in self._bit_ids:\n                print(\"Set valid bit id.\")\n                return None\n            else:\n                bit_id = bit\n        else:\n            print(\"'bit' must be integer index or string identifier\")\n            return None\n\n        try:\n            zattrs_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(bit_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n            attributes = self._load_from_json(zattrs_path)\n            return int(attributes[\"round_linker\"])\n        except FileNotFoundError:\n            print(tile_id, bit_id)\n            print(\"Round linker attribute not found.\")\n            return None\n\n    def save_local_round_linker(\n        self,\n        round_linker: int,\n        tile: Union[int, str],\n        bit: Union[int, str],\n    ):\n        \"\"\"Save fidicual round linker attribute to readout bit for one tile.\n\n        Parameters\n        ----------\n        round_linker : int\n            Fidicual round linked to readout bit for one tile.\n        tile : Union[int, str]\n            Tile index or tile id.\n        bit : Union[int, str]\n            Bit index or bit id.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id.\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(bit, int):\n            if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                return None\n            else:\n                bit_id = self._bit_ids[bit]\n        elif isinstance(bit, str):\n            if bit not in self._bit_ids:\n                print(\"Set valid bit id.\")\n                return None\n            else:\n                bit_id = bit\n        else:\n            print(\"'bit' must be integer index or string identifier\")\n            return None\n\n        try:\n            zattrs_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(bit_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n            attributes = self._load_from_json(zattrs_path)\n            attributes[\"round\"] = int(round_linker)\n            self._save_to_json(attributes, zattrs_path)\n        except (FileNotFoundError, json.JSONDecodeError):\n            print(tile_id, bit_id)\n            print(\"Error writing round linker attribute.\")\n            return None\n\n    def load_local_stage_position_zyx_um(\n        self,\n        tile: Union[int, str],\n        round: Union[int, str],\n    ) -&gt; Optional[ArrayLike]:\n        \"\"\"Load tile stage position for one tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n        round : Union[int, str]\n            Round index or round id.\n\n        Returns\n        -------\n        stage_zyx_um : Optional[ArrayLike]\n            Tile stage position for one tile.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id.\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                round_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id.\")\n                return None\n            else:\n                round_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n\n        try:\n            zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(round_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n            attributes = self._load_from_json(zattrs_path)\n            return np.asarray(attributes[\"stage_zyx_um\"], dtype=np.float32)\n        except FileNotFoundError:\n            print(tile_id, round_id)\n            print(\"Stage position attribute not found.\")\n            return None\n\n    def save_local_stage_position_zyx_um(\n        self,\n        stage_zyx_um: ArrayLike,\n        tile: Union[int, str],\n        round: Union[int, str],\n    ):\n        \"\"\"Save tile stage position for one tile.\n\n        Parameters\n        ----------\n        stage_zyx_um : ArrayLike\n            Tile stage position for one tile.\n        tile : Union[int, str]\n            Tile index or tile id.\n        round : Union[int, str]\n            Round index or round id.\n\n        Returns\n        -------\n        stage_zyx_um : Optional[ArrayLike]\n            Tile stage position for one tile.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id.\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                round_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id.\")\n                return None\n            else:\n                round_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n\n        try:\n            zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(round_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n            attributes = self._load_from_json(zattrs_path)\n            attributes[\"stage_zyx_um\"] = stage_zyx_um.tolist()\n            self._save_to_json(attributes, zattrs_path)\n        except (FileNotFoundError, json.JSONDecodeError):\n            print(tile_id, round_id)\n            print(\"Error writing stage position attribute.\")\n            return None\n\n    def load_local_wavelengths_um(\n        self,\n        tile: Union[int, str],\n        round: Optional[Union[int, str]] = None,\n        bit: Optional[Union[int, str]] = None,\n    ) -&gt; Optional[tuple[float, float]]:\n        \"\"\"Load wavelengths for fidicual OR readout bit for one tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n        round : Optional[Union[int, str]]   \n            Round index or round id.\n        bit : Optional[Union[int, str]]\n            Bit index or bit id.\n\n        Returns\n        -------\n        wavelengths_um : Optional[tuple[float, float]]\n            Wavelengths for fidicual OR readout bit for one tile.\n        \"\"\"\n\n        if (round is None and bit is None) or (round is not None and bit is not None):\n            print(\"Provide either 'round' or 'bit', but not both\")\n            return None\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if bit is not None:\n            if isinstance(bit, int):\n                if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                    print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                    return None\n                else:\n                    local_id = self._bit_ids[bit]\n            elif isinstance(bit, str):\n                if bit not in self._bit_ids:\n                    print(\"Set valid bit id\")\n                    return None\n                else:\n                    local_id = bit\n            else:\n                print(\"'bit' must be integer index or string identifier\")\n                return None\n            zattrs_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n        else:\n            if isinstance(round, int):\n                if round &lt; 0:\n                    print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                    return None\n                else:\n                    local_id = self._round_ids[round]\n            elif isinstance(round, str):\n                if round not in self._round_ids:\n                    print(\"Set valid round id\")\n                    return None\n                else:\n                    local_id = round\n            else:\n                print(\"'round' must be integer index or string identifier\")\n                return None\n            zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n\n        try:\n            attributes = self._load_from_json(zattrs_path)\n            ex_wavelength_um = attributes[\"excitation_um\"]\n            em_wavelength_um = attributes[\"emission_um\"]\n            return (ex_wavelength_um, em_wavelength_um)\n        except KeyError:\n            print(\"Wavelength attributes not found.\")\n            return None\n\n    def save_local_wavelengths_um(\n        self,\n        wavelengths_um: tuple[float, float],\n        tile: Union[int, str],\n        round: Optional[Union[int, str]] = None,\n        bit: Optional[Union[int, str]] = None,\n    ) -&gt; Optional[tuple[float, float]]:\n        \"\"\"Save wavelengths for fidicual OR readout bit for one tile.\n\n        Parameters\n        ----------\n        wavelengths_um : tuple[float, float]\n            Wavelengths for fidicual OR readout bit for one tile.\n        tile : Union[int, str]\n            Tile index or tile id.\n        round : Optional[Union[int, str]]\n            Round index or round id.\n        bit : Optional[Union[int, str]]\n            Bit index or bit id.\n\n        Returns\n        -------\n        wavelengths_um : Optional[tuple[float, float]]\n            Wavelengths for fidicual OR readout bit for one tile.\n        \"\"\"\n\n        if (round is None and bit is None) or (round is not None and bit is not None):\n            print(\"Provide either 'round' or 'bit', but not both\")\n            return None\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if bit is not None:\n            if isinstance(bit, int):\n                if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                    print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                    return None\n                else:\n                    local_id = self._bit_ids[bit]\n            elif isinstance(bit, str):\n                if bit not in self._bit_ids:\n                    print(\"Set valid bit id\")\n                    return None\n                else:\n                    local_id = bit\n            else:\n                print(\"'bit' must be integer index or string identifier\")\n                return None\n            zattrs_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n        else:\n            if isinstance(round, int):\n                if round &lt; 0:\n                    print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                    return None\n                else:\n                    local_id = self._round_ids[round]\n            elif isinstance(round, str):\n                if round not in self._round_ids:\n                    print(\"Set valid round id\")\n                    return None\n                else:\n                    local_id = round\n            else:\n                print(\"'round' must be integer index or string identifier\")\n                return None\n            zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n\n        try:\n            attributes = self._load_from_json(zattrs_path)\n            attributes[\"excitation_um\"] = float(wavelengths_um[0])\n            attributes[\"emission_um\"] = float(wavelengths_um[1])\n            self._save_to_json(attributes, zattrs_path)\n        except (FileNotFoundError, json.JSONDecodeError):\n            print(\"Error writing wavelength attributes.\")\n            return None\n\n    def load_local_corrected_image(\n        self,\n        tile: Union[int, str],\n        round: Optional[Union[int, str]] = None,\n        bit: Optional[Union[int, str]] = None,\n        return_future: Optional[bool] = True,\n    ) -&gt; Optional[ArrayLike]:\n        \"\"\"Load gain and offset corrected image for fiducial OR readout bit for one tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n        round : Optional[Union[int, str]]\n            Round index or round id.\n        bit : Optional[Union[int, str]]\n            Bit index or bit id.\n        return_future : Optional[bool]\n            Return future array.\n\n        Returns\n        -------\n        corrected_image : Optional[ArrayLike]\n            Gain and offset corrected image for fiducial OR readout bit for one tile.\n        \"\"\"\n\n        if (round is None and bit is None) or (round is not None and bit is not None):\n            print(\"Provide either 'round' or 'bit', but not both\")\n            return None\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if bit is not None:\n            if isinstance(bit, int):\n                if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                    print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                    return None\n                else:\n                    local_id = self._bit_ids[bit]\n            elif isinstance(bit, str):\n                if bit not in self._bit_ids:\n                    print(\"Set valid bit id\")\n                    return None\n                else:\n                    local_id = bit\n            else:\n                print(\"'bit' must be integer index or string identifier\")\n                return None\n            current_local_zarr_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\"corrected_data\")\n            )\n        else:\n            if isinstance(round, int):\n                if round &lt; 0:\n                    print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                    return None\n                else:\n                    local_id = self._round_ids[round]\n            elif isinstance(round, str):\n                if round not in self._round_ids:\n                    print(\"Set valid round id\")\n                    return None\n                else:\n                    local_id = round\n            else:\n                print(\"'round' must be integer index or string identifier\")\n                return None\n            current_local_zarr_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\"corrected_data\")\n            )\n\n        if not Path(current_local_zarr_path).exists():\n            print(\"Corrected image not found.\")\n            return None\n\n        try:\n            spec = self._zarrv2_spec.copy()\n            spec[\"metadata\"][\"dtype\"] = \"&lt;u2\"\n            corrected_image = self._load_from_zarr_array(\n                self._get_kvstore_key(current_local_zarr_path),\n                spec,\n                return_future,\n            )\n            return corrected_image\n        except (IOError, OSError, ZarrError):\n            print(\"Error loading corrected image.\")\n            return None\n\n    def save_local_corrected_image(\n        self,\n        image: ArrayLike,\n        tile: Union[int, str],\n        gain_correction: bool = True,\n        hotpixel_correction: bool = True,\n        shading_correction: bool = False,\n        psf_idx: int = 0,\n        round: Optional[Union[int, str]] = None,\n        bit: Optional[Union[int, str]] = None,\n        return_future: Optional[bool] = False,\n    ):\n        \"\"\"Save gain and offset corrected image.\n\n        Parameters\n        ----------\n        image : ArrayLike\n            Local corrected image.\n        tile : Union[int, str]\n            Tile index or tile id.\n        gain_correction : bool\n            Gain correction applied (True) or not (False).\n        hotpixel_correction : bool\n            Hotpixel correction applied (True) or not (False).\n        shading_correction : bool\n            Shading correction applied (True) or not (False).\n        psf_idx : int\n            PSF index.\n        round : Optional[Union[int, str]]\n            Round index or round id.\n        bit : Optional[Union[int, str]]\n            Bit index or bit id.\n        return_future : Optional[bool]\n            Return future array.\n   \"\"\"\n\n        if (round is None and bit is None) or (round is not None and bit is not None):\n            print(\"Provide either 'round' or 'bit', but not both\")\n            return None\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if bit is not None:\n            if isinstance(bit, int):\n                if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                    print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                    return None\n                else:\n                    local_id = self._bit_ids[bit]\n            elif isinstance(bit, str):\n                if bit not in self._bit_ids:\n                    print(\"Set valid bit id\")\n                    return None\n                else:\n                    local_id = bit\n            else:\n                print(\"'bit' must be integer index or string identifier\")\n                return None\n            current_local_zarr_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\"corrected_data\")\n            )\n            current_local_zattrs_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n        else:\n            if isinstance(round, int):\n                if round &lt; 0:\n                    print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                    return None\n                else:\n                    local_id = self._round_ids[round]\n            elif isinstance(round, str):\n                if round not in self._round_ids:\n                    print(\"Set valid round id\")\n                    return None\n                else:\n                    local_id = round\n            else:\n                print(\"'round' must be integer index or string identifier\")\n                return None\n            current_local_zarr_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\"corrected_data\")\n            )\n            current_local_zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n\n        try:\n            self._save_to_zarr_array(\n                image,\n                self._get_kvstore_key(current_local_zarr_path),\n                self._zarrv2_spec,\n                return_future,\n            )\n            attributes = self._load_from_json(current_local_zattrs_path)\n            attributes[\"gain_correction\"] = (gain_correction,)\n            attributes[\"hotpixel_correction\"] = (hotpixel_correction,)\n            attributes[\"shading_correction\"] = (shading_correction,)\n            attributes[\"psf_idx\"] = psf_idx\n            self._save_to_json(attributes, current_local_zattrs_path)\n        except (IOError, OSError, TimeoutError) as e:\n            print(e)\n            print(\"Error saving corrected image.\")\n            return None\n\n    def load_local_rigid_xform_xyz_px(\n        self,\n        tile: Union[int, str],\n        round: Union[int, str],\n    ) -&gt; Optional[ArrayLike]:\n        \"\"\"Load calculated rigid registration transform for one round and tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n        round : Union[int, str]\n            Round index or round id.\n\n        Returns\n        -------\n        rigid_xform_xyz_px : Optional[ArrayLike]\n            Local rigid registration transform for one round and tile.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                round_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id\")\n                return None\n            else:\n                round_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n        try:\n            zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(round_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n            attributes = self._load_from_json(zattrs_path)\n            rigid_xform_xyz_px = np.asarray(\n                attributes[\"rigid_xform_xyz_px\"], dtype=np.float32\n            )\n            return rigid_xform_xyz_px\n        except (FileNotFoundError, json.JSONDecodeError):\n            print(tile_id, round_id)\n            print(\"Rigid transform mapping back to first round not found.\")\n            return None\n\n    def save_local_rigid_xform_xyz_px(\n        self,\n        rigid_xform_xyz_px: ArrayLike,\n        tile: Union[int, str],\n        round: Union[int, str],\n    ) -&gt; Optional[ArrayLike]:\n        \"\"\"Save calculated rigid registration transform for one round and tile.\n\n        Parameters\n        ----------\n        rigid_xform_xyz_px : ArrayLike\n            Local rigid registration transform for one round and tile.\n        tile : Union[int, str]\n            Tile index or tile id.\n        round : Union[int, str]\n            Round index or round id.\n\n        Returns\n        -------\n        rigid_xform_xyz_px : Optional[ArrayLike]\n            Local rigid registration transform for one round and tile.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                round_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id\")\n                return None\n            else:\n                round_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n        try:\n            zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(round_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n            attributes = self._load_from_json(zattrs_path)\n            attributes[\"rigid_xform_xyz_px\"] = rigid_xform_xyz_px.tolist()\n            self._save_to_json(attributes, zattrs_path)\n        except (FileNotFoundError, json.JSONDecodeError):\n            print(\"Error writing rigid transform attribute.\")\n            return None\n\n    def load_coord_of_xform_px(\n        self,\n        tile: Optional[Union[int, str]],\n        round: Optional[Union[int, str]],\n        return_future: Optional[bool] = True,\n    ) -&gt; Optional[tuple[ArrayLike, ArrayLike]]:\n        \"\"\"Local fidicual optical flow matrix for one round and tile.\n\n        Parameters\n        ----------\n        tile : Optional[Union[int, str]]\n            Tile index or tile id.\n        round : Optional[Union[int, str]]\n            Round index or round id.\n        return_future : Optional[bool]\n            Return future array.\n\n        Returns\n        -------\n        of_xform_px : Optional[ArrayLike]\n            Local fidicual optical flow matrix for one round and tile.\n        downsampling : Optional[ArrayLike]\n            Downsampling factor.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                round_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id\")\n                return None\n            else:\n                round_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n\n        current_local_zarr_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(round_id + \".zarr\")\n            / Path(\"of_xform_px\")\n        )\n        zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(round_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n\n        if not Path(current_local_zarr_path).exists():\n            print(\"Optical flow transform mapping back to first round not found.\")\n            return None\n\n        try:\n            compressor = {\n                \"id\": \"blosc\",\n                \"cname\": \"zstd\",\n                \"clevel\": 5,\n                \"shuffle\": 2,\n            }\n            spec_of = {\n                \"driver\": \"zarr\",\n                \"kvstore\": None,\n                \"metadata\": {\"compressor\": compressor},\n                \"open\": True,\n                \"assume_metadata\": False,\n                \"create\": True,\n                \"delete_existing\": False,\n            }\n            spec_of[\"metadata\"][\"dtype\"] = \"&lt;f4\"\n            of_xform_px = self._load_from_zarr_array(\n                self._get_kvstore_key(current_local_zarr_path),\n                spec_of.copy(),\n                return_future,\n            )\n            attributes = self._load_from_json(zattrs_path)\n            downsampling = np.asarray(\n                attributes[\"opticalflow_downsampling\"], dtype=np.float32\n            )\n\n            return of_xform_px, downsampling\n        except (IOError, OSError, ZarrError) as e:\n            print(e)\n            print(\"Error loading optical flow transform.\")\n            return None\n\n    def save_coord_of_xform_px(\n        self,\n        of_xform_px: ArrayLike,\n        tile: Union[int, str],\n        downsampling: Sequence[float],\n        round: Union[int, str],\n        return_future: Optional[bool] = False,\n    ):\n        \"\"\"Save fidicual optical flow matrix for one round and tile.\n\n        Parameters\n        ----------\n        of_xform_px : ArrayLike\n            Local fidicual optical flow matrix for one round and tile.\n        tile : Union[int, str]\n            Tile index or tile id.\n        downsampling : Sequence[float]\n            Downsampling factor.\n        round : Union[int, str] \n            Round index or round id.\n        return_future : Optional[bool]\n            Return future array.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                local_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id\")\n                return None\n            else:\n                local_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n        current_local_zarr_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\"of_xform_px\")\n        )\n        current_local_zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n\n        try:\n            compressor = {\n                \"id\": \"blosc\",\n                \"cname\": \"zstd\",\n                \"clevel\": 5,\n                \"shuffle\": 2,\n            }\n            spec_of = {\n                \"driver\": \"zarr\",\n                \"kvstore\": None,\n                \"metadata\": {\"compressor\": compressor},\n                \"open\": True,\n                \"assume_metadata\": False,\n                \"create\": True,\n                \"delete_existing\": False,\n            }\n            self._save_to_zarr_array(\n                of_xform_px,\n                self._get_kvstore_key(current_local_zarr_path),\n                spec_of.copy(),\n                return_future,\n            )\n            attributes = self._load_from_json(current_local_zattrs_path)\n            attributes[\"opticalflow_downsampling\"] = downsampling\n            self._save_to_json(attributes, current_local_zattrs_path)\n        except (IOError, OSError, TimeoutError):\n            print(\"Error saving optical flow transform.\")\n            return None\n\n    def load_local_registered_image(\n        self,\n        tile: Union[int, str],\n        round: Optional[Union[int, str]] = None,\n        bit: Optional[Union[int, str]] = None,\n        return_future: Optional[bool] = True,\n    ) -&gt; Optional[ArrayLike]:\n        \"\"\"Local registered, deconvolved image for fidiculial OR readout bit for one tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n        round : Optional[Union[int, str]]\n            Round index or round id.\n        bit : Optional[Union[int, str]]\n            Bit index or bit id.\n        return_future : Optional[bool]\n            Return future array.\n\n        Returns\n        -------\n        registered_decon_image : Optional[ArrayLike]\n            Registered, deconvolved image for fidiculial OR readout bit for one tile.\n        \"\"\"\n\n        if (round is None and bit is None) or (round is not None and bit is not None):\n            print(\"Provide either 'round' or 'bit', but not both\")\n            return None\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if bit is not None:\n            if isinstance(bit, int):\n                if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                    print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                    return None\n                else:\n                    local_id = self._bit_ids[bit]\n            elif isinstance(bit, str):\n                if bit not in self._bit_ids:\n                    print(\"Set valid bit id\")\n                    return None\n                else:\n                    local_id = bit\n            else:\n                print(\"'bit' must be integer index or string identifier\")\n                return None\n            current_local_zarr_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\"registered_decon_data\")\n            )\n        else:\n            if isinstance(round, int):\n                if round &lt; 0:\n                    print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                    return None\n                else:\n                    local_id = self._round_ids[round]\n            elif isinstance(round, str):\n                if round not in self._round_ids:\n                    print(\"Set valid round id\")\n                    return None\n                else:\n                    local_id = round\n            else:\n                print(\"'round' must be integer index or string identifier\")\n                return None\n            current_local_zarr_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\"registered_decon_data\")\n            )\n\n        if not Path(current_local_zarr_path).exists():\n            print(\"Registered deconvolved image not found.\")\n            return None\n\n        try:\n            spec = self._zarrv2_spec.copy()\n            spec[\"metadata\"][\"dtype\"] = \"&lt;u2\"\n            registered_decon_image = self._load_from_zarr_array(\n                self._get_kvstore_key(current_local_zarr_path),\n                spec,\n                return_future,\n            )\n            return registered_decon_image\n        except (IOError, OSError, ZarrError) as e:\n            print(e)\n            print(\"Error loading registered deconvolved image.\")\n            return None\n\n    def save_local_registered_image(\n        self,\n        registered_image: ArrayLike,\n        tile: Union[int, str],\n        deconvolution: bool = True,\n        round: Optional[Union[int, str]] = None,\n        bit: Optional[Union[int, str]] = None,\n        return_future: Optional[bool] = False,\n    ):\n        \"\"\"Save registered, deconvolved image.\n\n        Parameters\n        ----------\n        registered_image : ArrayLike\n            Registered, deconvolved image.\n        tile : Union[int, str]\n            Tile index or tile id.\n        deconvolution : bool\n            Deconvolution applied (True) or not (False).\n        round : Optional[Union[int, str]]\n            Round index or round id.\n        bit : Optional[Union[int, str]]\n            Bit index or bit id.\n        return_future : Optional[bool]\n            Return future array.\n        \"\"\"\n\n        if (round is None and bit is None) or (round is not None and bit is not None):\n            print(\"Provide either 'round' or 'bit', but not both\")\n            return None\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if bit is not None:\n            if isinstance(bit, int):\n                if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                    print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                    return None\n                else:\n                    local_id = self._bit_ids[bit]\n            elif isinstance(bit, str):\n                if bit not in self._bit_ids:\n                    print(\"Set valid bit id\")\n                    return None\n                else:\n                    local_id = bit\n            else:\n                print(\"'bit' must be integer index or string identifier\")\n                return None\n            current_local_zarr_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\"registered_decon_data\")\n            )\n            current_local_zattrs_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n        else:\n            if isinstance(round, int):\n                if round &lt; 0:\n                    print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                    return None\n                else:\n                    local_id = self._round_ids[round]\n            elif isinstance(round, str):\n                if round not in self._round_ids:\n                    print(\"Set valid round id\")\n                    return None\n                else:\n                    local_id = round\n            else:\n                print(\"'round' must be integer index or string identifier\")\n                return None\n            current_local_zarr_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\"registered_decon_data\")\n            )\n            current_local_zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\".zattrs\")\n            )\n\n        try:\n            spec = self._zarrv2_spec.copy()\n            spec[\"metadata\"][\"dtype\"] = \"&lt;u2\"\n            self._save_to_zarr_array(\n                registered_image,\n                self._get_kvstore_key(current_local_zarr_path),\n                spec,\n                return_future,\n            )\n            attributes = self._load_from_json(current_local_zattrs_path)\n            attributes[\"deconvolution\"] = deconvolution\n            self._save_to_json(attributes, current_local_zattrs_path)\n        except (IOError, OSError, TimeoutError):\n            print(\"Error saving corrected image.\")\n            return None\n\n    def load_local_ufish_image(\n        self,\n        tile: Union[int, str],\n        bit: Union[int, str],\n        return_future: Optional[bool] = True,\n    ) -&gt; Optional[ArrayLike]:\n        \"\"\"Load readout bit U-FISH prediction image for one tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n        bit : Union[int, str]\n            Bit index or bit id.\n        return_future : Optional[bool]\n\n        Returns\n        -------\n        registered_ufish_image : Optional[ArrayLike]\n            U-FISH prediction image for one tile.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(bit, int):\n            if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                return None\n            else:\n                bit_id = self._bit_ids[bit]\n        elif isinstance(bit, str):\n            if bit not in self._bit_ids:\n                print(\"Set valid bit id\")\n                return None\n            else:\n                bit_id = bit\n        else:\n            print(\"'bit' must be integer index or string identifier\")\n            return None\n\n        current_local_zarr_path = str(\n            self._readouts_root_path\n            / Path(tile_id)\n            / Path(bit_id + \".zarr\")\n            / Path(\"registered_ufish_data\")\n        )\n\n        if not Path(current_local_zarr_path).exists():\n            print(\"U-FISH prediction image not found.\")\n            return None\n\n        try:\n            spec = self._zarrv2_spec.copy()\n            spec[\"metadata\"][\"dtype\"] = \"&lt;f4\"\n            registered_ufish_image = self._load_from_zarr_array(\n                self._get_kvstore_key(current_local_zarr_path),\n                spec,\n                return_future,\n            )\n            return registered_ufish_image\n        except (IOError, OSError, ZarrError) as e:\n            print(e)\n            print(\"Error loading U-FISH image.\")\n            return None\n\n    def save_local_ufish_image(\n        self,\n        ufish_image: ArrayLike,\n        tile: Union[int, str],\n        bit: Union[int, str],\n        return_future: Optional[bool] = False,\n    ):\n        \"\"\"Save U-FISH prediction image.\n\n        Parameters\n        ----------\n        ufish_image : ArrayLike\n            U-FISH prediction image.\n        tile : Union[int, str]\n            Tile index or tile id.\n        bit : Union[int, str]\n            Bit index or bit id.\n        return_future : Optional[bool]\n            Return future array.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if bit is not None:\n            if isinstance(bit, int):\n                if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                    print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                    return None\n                else:\n                    local_id = self._bit_ids[bit]\n            elif isinstance(bit, str):\n                if bit not in self._bit_ids:\n                    print(\"Set valid bit id\")\n                    return None\n                else:\n                    local_id = bit\n            else:\n                print(\"'bit' must be integer index or string identifier\")\n                return None\n            current_local_zarr_path = str(\n                self._readouts_root_path\n                / Path(tile_id)\n                / Path(local_id + \".zarr\")\n                / Path(\"registered_ufish_data\")\n            )\n\n        try:\n            self._save_to_zarr_array(\n                ufish_image,\n                self._get_kvstore_key(current_local_zarr_path),\n                self._zarrv2_spec.copy(),\n                return_future,\n            )\n        except (IOError, OSError, ZarrError) as e:\n            print(e)\n            print(\"Error saving U-Fish image.\")\n            return None\n\n    def load_local_ufish_spots(\n        self,\n        tile: Union[int, str],\n        bit: Union[int, str],\n    ) -&gt; Optional[pd.DataFrame]:\n        \"\"\"Load U-FISH spot localizations and features for one tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n        bit : Union[int, str]\n            Bit index or bit id.\n\n        Returns\n        -------\n        ufish_localizations : Optional[pd.DataFrame]\n            U-FISH localizations and features for one tile.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(bit, int):\n            if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                return None\n            else:\n                bit_id = self._bit_ids[bit]\n        elif isinstance(bit, str):\n            if bit not in self._bit_ids:\n                print(\"Set valid bit id\")\n                return None\n            else:\n                bit_id = bit\n        else:\n            print(\"'bit' must be integer index or string identifier\")\n            return None\n\n        current_ufish_localizations_path = (\n            self._ufish_localizations_root_path\n            / Path(tile_id)\n            / Path(bit_id + \".parquet\")\n        )\n\n        if not current_ufish_localizations_path.exists():\n            print(\"U-FISH localizations not found.\")\n            return None\n        else:\n            ufish_localizations = self._load_from_parquet(\n                current_ufish_localizations_path\n            )\n            return ufish_localizations\n\n    def save_local_ufish_spots(\n        self,\n        spot_df: pd.DataFrame,\n        tile: Union[int, str],\n        bit: Union[int, str],\n    ):\n        \"\"\"Save U-FISH localizations and features.\n\n        Parameters\n        ----------\n        spot_df : pd.DataFrame\n            U-FISH localizations and features.\n        tile : Union[int, str]\n            Tile index or tile id.\n        bit : Union[int, str]\n            Bit index or bit id.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        if isinstance(bit, int):\n            if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                return None\n            else:\n                bit_id = self._bit_ids[bit]\n        elif isinstance(bit, str):\n            if bit not in self._bit_ids:\n                print(\"Set valid bit id\")\n                return None\n            else:\n                bit_id = bit\n        else:\n            print(\"'bit' must be integer index or string identifier\")\n            return None\n\n        if not (self._ufish_localizations_root_path / Path(tile_id)).exists():\n            (self._ufish_localizations_root_path / Path(tile_id)).mkdir()\n\n        current_ufish_localizations_path = (\n            self._ufish_localizations_root_path\n            / Path(tile_id)\n            / Path(bit_id + \".parquet\")\n        )\n\n        try:\n            self._save_to_parquet(spot_df, current_ufish_localizations_path)\n        except (IOError, OSError) as e:\n            print(e)\n            print(\"Error saving U-FISH localizations.\")\n            return None\n\n    def load_global_coord_xforms_um(\n        self,\n        tile: Union[int, str],\n    ) -&gt; Optional[tuple[ArrayLike, ArrayLike, ArrayLike]]:\n        \"\"\"Load global registration transform for one tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n\n        Returns\n        -------\n        affine_zyx_um : Optional[ArrayLike]\n            Global affine registration transform for one tile.\n        origin_zyx_um : Optional[ArrayLike]\n            Global origin registration transform for one tile.\n        spacing_zyx_um : Optional[ArrayLike]\n            Global spacing registration transform for one tile.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None, None, None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None, None, None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        try:\n            zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(self._round_ids[0] + \".zarr\")\n                / Path(\".zattrs\")\n            )\n            attributes = self._load_from_json(zattrs_path)\n            affine_zyx_um = np.asarray(attributes[\"affine_zyx_um\"], dtype=np.float32)\n            origin_zyx_um = np.asarray(attributes[\"origin_zyx_um\"], dtype=np.float32)\n            spacing_zyx_um = np.asarray(attributes[\"spacing_zyx_um\"], dtype=np.float32)\n            return (affine_zyx_um, origin_zyx_um, spacing_zyx_um)\n        except (FileNotFoundError, json.JSONDecodeError):\n            print(tile_id, self._round_ids[0])\n            print(\"Global coordinate transforms not found\")\n            return None, None, None\n\n    def save_global_coord_xforms_um(\n        self,\n        affine_zyx_um: ArrayLike,\n        origin_zyx_um: ArrayLike,\n        spacing_zyx_um: ArrayLike,\n        tile: Union[int, str],\n    ) -&gt; None:\n        \"\"\"Save global registration transform for one tile.\n\n        Parameters\n        ----------\n        affine_zyx_um : ArrayLike\n            Global affine registration transform for one tile.\n        origin_zyx_um : ArrayLike\n            Global origin registration transform for one tile.\n        spacing_zyx_um : ArrayLike\n            Global spacing registration transform for one tile.\n        tile : Union[int, str]\n            Tile index or tile id.\n        \"\"\"\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        try:\n            zattrs_path = str(\n                self._polyDT_root_path\n                / Path(tile_id)\n                / Path(self._round_ids[0] + \".zarr\")\n                / Path(\".zattrs\")\n            )\n            attributes = self._load_from_json(zattrs_path)\n            attributes[\"affine_zyx_um\"] = affine_zyx_um.tolist()\n            attributes[\"origin_zyx_um\"] = origin_zyx_um.tolist()\n            attributes[\"spacing_zyx_um\"] = spacing_zyx_um.tolist()\n            self._save_to_json(attributes, zattrs_path)\n        except (FileNotFoundError, json.JSONDecodeError) as e:\n            print(e)\n            print(\"Could not save global coordinate transforms.\")\n\n    def load_global_fidicual_image(\n        self,\n        return_future: Optional[bool] = True,\n    ) -&gt; Optional[tuple[ArrayLike, ArrayLike, ArrayLike, ArrayLike]]:\n        \"\"\"Load downsampled, fused fidicual image.\n\n        Parameters\n        ----------\n        return_future : Optional[bool]\n            Return future array.\n\n        Returns\n        -------\n        fused_image : Optional[ArrayLike]\n            Downsampled, fused fidicual image.\n        affine_zyx_um : Optional[ArrayLike]\n            Global affine registration transform for fused image.\n        origin_zyx_um : Optional[ArrayLike]\n            Global origin registration transform for fused image.\n        spacing_zyx_um : Optional[ArrayLike]\n            Global spacing registration transform for fused image.\n        \"\"\"\n\n        current_local_zarr_path = str(\n            self._fused_root_path / Path(\"fused.zarr\") / Path(\"fused_polyDT_iso_zyx\")\n        )\n\n        if not Path(current_local_zarr_path).exists():\n            print(\"Globally registered, fused image not found.\")\n            return None\n\n        zattrs_path = str(current_local_zarr_path / Path(\".zattrs\"))\n\n        try:\n            fused_image = self._load_from_zarr_array(\n                self._get_kvstore_key(current_local_zarr_path),\n                self._zarrv2_spec.copy(),\n                return_future,\n            )\n            attributes = self._load_from_json(zattrs_path)\n            affine_zyx_um = np.asarray(attributes[\"affine_zyx_um\"], dtype=np.float32)\n            origin_zyx_um = np.asarray(attributes[\"origin_zyx_um\"], dtype=np.float32)\n            spacing_zyx_um = np.asarray(attributes[\"spacing_zyx_um\"], dtype=np.float32)\n            return fused_image, affine_zyx_um, origin_zyx_um, spacing_zyx_um\n        except (IOError, OSError, ZarrError):\n            print(\"Error loading globally registered, fused image.\")\n            return None\n\n    def save_global_fidicual_image(\n        self,\n        fused_image: ArrayLike,\n        affine_zyx_um: ArrayLike,\n        origin_zyx_um: ArrayLike,\n        spacing_zyx_um: ArrayLike,\n        fusion_type: str = \"polyDT\",\n        return_future: Optional[bool] = False,\n    ):\n        \"\"\"Save downsampled, fused fidicual image.\n\n        Parameters\n        ----------\n        fused_image : ArrayLike\n            Downsampled, fused fidicual image.\n        affine_zyx_um : ArrayLike\n            Global affine registration transform for fused image.\n        origin_zyx_um : ArrayLike\n            Global origin registration transform for fused image.\n        spacing_zyx_um : ArrayLike\n            Global spacing registration transform for fused image.\n        fusion_type : str\n            Type of fusion (polyDT or all_channels).\n        return_future : Optional[bool]\n            Return future array.\n        \"\"\"\n\n        if fusion_type == \"polyDT\":\n            filename = \"fused_polyDT_iso_zyx\"\n        else:\n            filename = \"fused_all_channels_zyx\"\n        current_local_zarr_path = str(\n            self._fused_root_path / Path(\"fused.zarr\") / Path(filename)\n        )\n        current_local_zattrs_path = str(\n            self._fused_root_path\n            / Path(\"fused.zarr\")\n            / Path(filename)\n            / Path(\".zattrs\")\n        )\n\n        attributes = {\n            \"affine_zyx_um\": affine_zyx_um.tolist(),\n            \"origin_zyx_um\": origin_zyx_um.tolist(),\n            \"spacing_zyx_um\": spacing_zyx_um.tolist(),\n        }\n        try:\n            self._save_to_zarr_array(\n                fused_image.astype(np.uint16),\n                self._get_kvstore_key(current_local_zarr_path),\n                self._zarrv2_spec.copy(),\n                return_future,\n            )\n            self._save_to_json(attributes, current_local_zattrs_path)\n        except (IOError, OSError, TimeoutError):\n            print(\"Error saving fused image.\")\n            return None\n\n    def load_local_decoded_spots(\n        self,\n        tile: Union[int, str],\n    ) -&gt; Optional[pd.DataFrame]:\n        \"\"\"Load decoded spots and features for one tile.\n\n        Parameters\n        ----------\n        tile : Union[int, str]\n            Tile index or tile id.\n\n        Returns\n        -------\n        tile_features : Optional[pd.DataFrame]\n            Decoded spots and features for one tile.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        current_tile_features_path = self._decoded_root_path / Path(\n            tile_id + \"_decoded_features.parquet\"\n        )\n\n        if not current_tile_features_path.exists():\n            print(\"Decoded spots not found.\")\n            return None\n        else:\n            tile_features = self._load_from_parquet(current_tile_features_path)\n            return tile_features\n\n    def save_local_decoded_spots(\n        self,\n        features_df: pd.DataFrame,\n        tile: Union[int, str],\n    ) -&gt; None:\n        \"\"\"Save decoded spots and features for one tile.\n\n        Parameters\n        ----------\n        features_df : pd.DataFrame\n            Decoded spots and features for one tile.\n        tile : Union[int, str]\n            Tile index or tile id.\n        \"\"\"\n\n        if isinstance(tile, int):\n            if tile &lt; 0 or tile &gt; self._num_tiles:\n                print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n                return None\n            else:\n                tile_id = self._tile_ids[tile]\n        elif isinstance(tile, str):\n            if tile not in self._tile_ids:\n                print(\"set valid tiled id\")\n                return None\n            else:\n                tile_id = tile\n        else:\n            print(\"'tile' must be integer index or string identifier\")\n            return None\n\n        current_tile_features_path = self._decoded_root_path / Path(\n            tile_id + \"_decoded_features.parquet\"\n        )\n\n        self._save_to_parquet(features_df, current_tile_features_path)\n\n    def load_global_filtered_decoded_spots(\n        self,\n    ) -&gt; Optional[pd.DataFrame]:\n        \"\"\"Load all decoded and filtered spots.\n\n        Returns\n        -------\n        all_tiles_filtered : Optional[pd.DataFrame]\n            All decoded and filtered spots.\n        \"\"\"\n\n        current_global_filtered_decoded_dir_path = self._datastore_path / Path(\n            \"all_tiles_filtered_decoded_features\"\n        )\n        current_global_filtered_decoded_path = (\n            current_global_filtered_decoded_dir_path / Path(\"decoded_features.parquet\")\n        )\n\n        if not current_global_filtered_decoded_path.exists():\n            print(\"Global, filtered, decoded spots not found.\")\n            return None\n        else:\n            all_tiles_filtered = self._load_from_parquet(\n                current_global_filtered_decoded_path\n            )\n            return all_tiles_filtered\n\n    def save_global_filtered_decoded_spots(\n        self,\n        filtered_decoded_df: pd.DataFrame,\n    ):\n        \"\"\"Save all decoded and filtered spots.\n\n        Parameters\n        ----------\n        filtered_decoded_df : pd.DataFrame\n            All decoded and filtered spots.\n        \"\"\"\n\n        current_global_filtered_decoded_dir_path = self._datastore_path / Path(\n            \"all_tiles_filtered_decoded_features\"\n        )\n\n        if not current_global_filtered_decoded_dir_path.exists():\n            current_global_filtered_decoded_dir_path.mkdir()\n\n        current_global_filtered_decoded_path = (\n            current_global_filtered_decoded_dir_path / Path(\"decoded_features.parquet\")\n        )\n\n        self._save_to_parquet(filtered_decoded_df, current_global_filtered_decoded_path)\n\n    def load_global_cellpose_outlines(\n        self,\n    ) -&gt; Optional[dict]:\n        \"\"\"Load Cellpose max projection cell outlines.\n\n        Returns\n        -------\n        cellpose_outlines : Optional[dict]\n            Cellpose cell mask outlines.\n        \"\"\"\n\n        current_cellpose_outlines_path = (\n            self._segmentation_root_path / Path(\"cellpose\") / Path(\"cell_outlines.json\")\n        )\n\n        if not current_cellpose_outlines_path.exists():\n            print(\"Cellpose cell mask outlines not found.\")\n            return None\n        else:\n            cellpose_outlines = self._load_from_microjson(\n                current_cellpose_outlines_path\n            )\n            return cellpose_outlines\n\n    def load_global_cellpose_segmentation_image(\n        self,\n        return_future: Optional[bool] = True,\n    ) -&gt; Optional[ArrayLike]:\n        \"\"\"Load Cellpose max projection, downsampled segmentation image.\n\n        Parameters\n        ----------\n        return_future : Optional[bool]\n            Return future array.\n\n        Returns\n        -------\n        fused_image : Optional[ArrayLike]\n            Cellpose max projection, downsampled segmentation image.\n        \"\"\"\n\n        current_local_zarr_path = str(\n            self._segmentation_root_path\n            / Path(\"cellpose\")\n            / Path(\"cellpose.zarr\")\n            / Path(\"masks_polyDT_iso_zyx\")\n        )\n\n        if not current_local_zarr_path.exists():\n            print(\"Cellpose prediction on global fused image not found.\")\n            return None\n\n        try:\n            fused_image = self._load_from_zarr_array(\n                self._get_kvstore_key(current_local_zarr_path),\n                self._zarrv2_spec.copy(),\n                return_future,\n            )\n            return fused_image\n        except (IOError, OSError, ZarrError):\n            print(\"Error loading Cellpose image.\")\n            return None\n\n    def save_global_cellpose_segmentation_image(\n        self,\n        cellpose_image: ArrayLike,\n        downsampling: Sequence[float],\n        return_future: Optional[bool] = False,\n    ):\n        \"\"\"Save Cellpose max projection, downsampled segmentation image.\n\n        Parameters\n        ----------\n        cellpose_image : ArrayLike\n            Cellpose max projection, downsampled segmentation image.\n        downsampling : Sequence[float]\n            Downsample factors.\n        return_future : Optional[bool]\n            Return future array.\n        \"\"\"\n\n        current_local_zarr_path = str(\n            self._segmentation_root_path\n            / Path(\"cellpose\")\n            / Path(\"cellpose.zarr\")\n            / Path(\"masks_polyDT_iso_zyx\")\n        )\n        current_local_zattrs_path = str(\n            self._segmentation_root_path\n            / Path(\"cellpose\")\n            / Path(\"cellpose.zarr\")\n            / Path(\"masks_polyDT_iso_zyx\")\n            / Path(\".zattrs\")\n        )\n\n        attributes = {\"downsampling\": downsampling}\n\n        try:\n            self._save_to_zarr_array(\n                cellpose_image,\n                self._get_kvstore_key(current_local_zarr_path),\n                self._zarrv2_spec.copy(),\n                return_future,\n            )\n            self._save_to_json(attributes, current_local_zattrs_path)\n        except (IOError, OSError, TimeoutError):\n            print(\"Error saving Cellpose image.\")\n            return None\n\n    def save_spots_prepped_for_baysor(self, prepped_for_baysor_df: pd.DataFrame):\n        \"\"\"Save spots prepped for Baysor.\n\n        Parameters\n        ----------\n        prepped_for_baysor_df : pd.DataFrame\n            Spots prepped for Baysor.\n        \"\"\"\n\n        current_global_filtered_decoded_dir_path = self._datastore_path / Path(\n            \"all_tiles_filtered_decoded_features\"\n        )\n\n        if not current_global_filtered_decoded_dir_path.exists():\n            current_global_filtered_decoded_dir_path.mkdir()\n\n        current_global_filtered_decoded_path = (\n            current_global_filtered_decoded_dir_path / Path(\"transcripts.parquet\")\n        )\n\n        self._save_to_parquet(prepped_for_baysor_df, current_global_filtered_decoded_path)\n\n    def run_baysor(self):\n        \"\"\"Run Baysor\"\n\n        Assumes that spots are prepped for Baysor and the Baysor path and options are set.\n        Reformats ROIs into ImageJ style ROIs for later use.\n        \"\"\"\n\n        import subprocess\n\n        baysor_input_path = self._datastore_path / Path(\"all_tiles_filtered_decoded_features\") / Path(\"transcripts.parquet\")\n        baysor_output_path = self._segmentation_root_path / Path(\"baysor\")\n        baysor_output_path.mkdir(exist_ok=True)\n\n        julia_threading = r\"JULIA_NUM_THREADS=\"+str(self._julia_threads)+ \" \"\n        preview_baysor_options = r\"preview -c \" +str(self._baysor_options)\n        command = julia_threading + str(self._baysor_path) + \" \" + preview_baysor_options + \" \" +\\\n            str(baysor_input_path) + \" -o \" + str(baysor_output_path)\n\n        try:\n            result = subprocess.run(command, shell=True, check=True)\n            print(\"Baysor finished with return code:\", result.returncode)\n        except subprocess.CalledProcessError as e:\n            print(\"Baysor failed with:\", e)\n\n        # first try to run Baysor assuming that prior segmentations are present               \n        try:\n            run_baysor_options = r\"run -p -c \" +str(self._baysor_options)\n            command = julia_threading + str(self._baysor_path) + \" \" + run_baysor_options + \" \" +\\\n                str(baysor_input_path) + \" -o \" + str(baysor_output_path) + \\\n                \" --polygon-format GeometryCollectionLegacy --count-matrix-format tsv :cell_id\"\n            result = subprocess.run(command, shell=True, check=True)\n            print(\"Baysor finished with return code:\", result.returncode)\n        except subprocess.CalledProcessError:\n            # then fall back and run without prior segmentations.\n            # IMPORTANT: the .toml file has to be defined correctly for this to work!\n            try:\n                run_baysor_options = r\"run -p -c \" +str(self._baysor_options)\n                command = julia_threading + str(self._baysor_path) + \" \" + run_baysor_options + \" \" +\\\n                    str(baysor_input_path) + \" -o \" + str(baysor_output_path) + \" --count-matrix-format tsv\"\n                result = subprocess.run(command, shell=True, check=True)\n                print(\"Baysor finished with return code:\", result.returncode)\n            except subprocess.CalledProcessError as e:\n                print(\"Baysor failed with:\", e)\n\n    def reformat_baysor_3D_oultines(self):\n        \"\"\"Reformat baysor 3D json file into ImageJ ROIs.\"\"\"\n        import re\n\n        # Load the JSON file\n        baysor_output_path = self._segmentation_root_path / Path(\"baysor\")\n        baysor_segmentation = baysor_output_path / Path(r\"segmentation_polygons_3d.json\")\n        with open(baysor_segmentation, 'r') as file:\n            data = json.load(file)\n\n\n        # Dictionary to group polygons by cell ID\n        cell_polygons = defaultdict(list)\n\n        def parse_z_range(z_range):\n            cleaned_range = re.sub(r\"[^\\d.,-]\", \"\", z_range)  # Remove non-numeric, non-period, non-comma, non-dash characters\n            return map(float, cleaned_range.split(\",\"))\n\n        # Iterate through each z-plane and corresponding polygons\n        for z_range, details in data.items():\n            z_start, z_end = parse_z_range(z_range)\n\n            for geometry in details[\"geometries\"]:\n                coordinates = geometry[\"coordinates\"][0]  # Assuming the outer ring of the polygon\n                cell_id = geometry[\"cell\"]  # Get the cell ID\n\n                # Store the polygon with its z-range\n                cell_polygons[cell_id].append({\n                    \"z_start\": z_start,\n                    \"z_end\": z_end,\n                    \"coordinates\": coordinates\n                })\n\n        rois = []\n\n        # Process each cell ID to create 3D ROIs\n        for cell_id, polygons in cell_polygons.items():\n            for idx, polygon in enumerate(polygons):\n                x_coords = [point[0] for point in polygon[\"coordinates\"]]\n                y_coords = [point[1] for point in polygon[\"coordinates\"]]\n\n\n                z_start = polygon[\"z_start\"]\n                z_end = polygon[\"z_end\"]\n\n                try:\n                    # Create an ImageJRoi object for the polygon using frompoints\n                    coords = list(zip(x_coords, y_coords))  # List of (x, y) tuples\n                    roi = ImagejRoi.frompoints(coords)\n                    roi.roitype = ROI_TYPE.POLYGON  # Set the ROI type to Polygon\n                    roi.coordinates = coords  # Explicitly assign coordinates to the ROI\n                    roi.name = f\"cell_{str(cell_id)}_zstart_{str(z_start)}_zend_{str(z_end)}\"  # Ensure unique name\n                    rois.append(roi)\n                except Exception as e:\n                    print(f\"Error while creating ROI for cell ID {cell_id}: {e}\")\n\n        # Write all ROIs to a ZIP file   \n        output_file = baysor_output_path / Path(r\"3d_cell_rois.zip\")\n        roiwrite(output_file, rois,mode='w')\n\n    def load_global_baysor_filtered_spots(\n        self,\n    ) -&gt; Optional[pd.DataFrame]:\n        \"\"\"Load Baysor re-assigned decoded RNA.\n\n        Assumes Baysor has been run.\n\n        Returns\n        -------\n        baysor_filtered_genes : Optional[pd.DataFrame]\n            Baysor re-assigned decoded RNA.\n        \"\"\"\n\n        current_baysor_spots_path = (\n            self._segmentation_root_path\n            / Path(\"baysor\")\n            / Path(\"segmentation.csv\")\n        )\n\n        if not current_baysor_spots_path.exists():\n            print(\"Baysor filtered genes not found.\")\n            return None\n        else:\n            baysor_filtered_genes = self._load_from_csv(current_baysor_spots_path)\n            return baysor_filtered_genes\n\n    def load_global_baysor_outlines(\n        self,\n    ) -&gt; Optional[dict]:\n        \"\"\"Load Baysor cell outlines.\n\n        Assumes Baysor has been run.\n\n        Returns\n        -------\n        baysor_outlines : Optional[dict]\n            Baysor cell outlines.\n        \"\"\"\n\n        current_baysor_outlines_path = (\n            self._segmentation_root_path \n            / Path(\"baysor\") \n            / Path(r\"3d_cell_rois.zip\")\n        )\n\n        if not current_baysor_outlines_path.exists():\n            print(\"Baysor outlines not found.\")\n            return None\n        else:\n            baysor_rois = roiread(current_baysor_outlines_path)\n            return baysor_rois\n\n    @staticmethod\n    def _roi_to_shapely(roi):\n        return Polygon(roi.subpixel_coordinates[:, ::-1])\n\n    def reprocess_and_save_filtered_spots_with_baysor_outlines(self):\n        \"\"\"Reprocess filtered spots using baysor cell outlines, then save.\n\n        Loads the 3D cell outlines from Baysor, checks all points to see what \n        (if any) cell outline that the spot falls within, and then saves the\n        data back to the datastore.\n        \"\"\"\n        from rtree import index\n        import re\n\n        rois = self.load_global_baysor_outlines()\n        filtered_spots_df = self.load_global_filtered_decoded_spots()\n\n        parsed_spots_df = filtered_spots_df[\n                [\n                    \"gene_id\",\n                    \"global_z\",\n                    \"global_y\",\n                    \"global_x\",\n                    \"cell_id\",\n                    \"tile_idx\",\n                ]\n        ].copy()\n        parsed_spots_df.rename(\n            columns={\n                \"global_x\": \"x\",\n                \"global_y\": \"y\",\n                \"global_z\": \"z\",\n                \"gene_id\" : \"gene\",\n                \"cell_id\" : \"cell\",\n            },\n            inplace=True,\n        )\n        parsed_spots_df[\"transcript_id\"] = pd.util.hash_pandas_object(\n            parsed_spots_df, index=False\n        )\n\n        parsed_spots_df[\"assignment_confidence\"] = 1.0\n\n        # Create spatial index for ROIs\n        roi_index = index.Index()\n        roi_map = {}  # Map index IDs to ROIs\n\n        for idx, roi in enumerate(rois):\n            # Ensure roi.coordinates contains the polygon points\n            coords = roi.coordinates()\n\n            # Insert the polygon bounds into the spatial index\n            polygon = Polygon(coords)\n            roi_index.insert(idx, polygon.bounds)  # Use polygon bounds for indexing\n            roi_map[idx] = roi\n\n        # Function to check a single point\n        def point_in_roi(row):\n            point = Point(row[\"x\"], row[\"y\"])\n            candidate_indices = list(roi_index.intersection(point.bounds))  # Search spatial index\n            for idx in candidate_indices:\n                roi = roi_map[idx]\n                match = re.search(r\"zstart_([-\\d.]+)_zend_([-\\d.]+)\", roi.name)\n                if match:\n                    z_start = float(match.group(1))\n                    z_end = float(match.group(2))\n                    if z_start &lt;= row[\"z\"] &lt;= z_end:\n                        polygon = Polygon(roi.coordinates())\n                        if polygon.contains(point):\n                            return str(roi.name.split(\"_\")[1]) \n            return -1\n\n        # Apply optimized spatial lookup\n        parsed_spots_df[\"cell\"] = parsed_spots_df.apply(point_in_roi, axis=1)\n        parsed_spots_df = parsed_spots_df.loc[parsed_spots_df[\"cell\"] != -1]\n\n        current_global_filtered_decoded_path = (\n            self._datastore_path \n            / Path(\"all_tiles_filtered_decoded_features\")\n            / Path(\"refined_transcripts.parquet\")\n        )\n\n        self._save_to_parquet(parsed_spots_df, current_global_filtered_decoded_path)\n\n    def save_mtx(self, spots_source: str = \"\"):\n        \"\"\"Save mtx file for downstream analysis. Assumes Baysor has been run.\n\n        Parameters\n        ----------\n        spots_source: str, default \"baysor\"\n            source of spots. \"baysor\" or \"resegmented\".\n        \"\"\"\n\n        from merfish3danalysis.utils.dataio import create_mtx\n\n        if spots_source == \"baysor\":\n            spots_path = self._datastore_path / Path(\"segmentation\") / Path(\"baysor\") / Path(\"segmentation.csv\")\n        elif spots_source == \"resegmented\":\n            spots_path = (\n                self._datastore_path \n                / Path(\"all_tiles_filtered_decoded_features\")\n                / Path(\"refined_transcripts.parquet\")\n            )\n\n        mtx_output_path = self._datastore_path / Path(\"mtx_output\")\n\n        create_mtx(\n            spots_path=spots_path,\n            output_dir_path=mtx_output_path,\n        )\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.baysor_options","title":"<code>baysor_options</code>  <code>property</code> <code>writable</code>","text":"<p>Baysor options</p> <p>Returns:</p> Name Type Description <code>baysor_options</code> <code>Union[Path, str]</code> <p>Baysor options.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.baysor_path","title":"<code>baysor_path</code>  <code>property</code> <code>writable</code>","text":"<p>Baysor path</p> <p>Returns:</p> Name Type Description <code>baysor_path</code> <code>Union[Path, str]</code> <p>Baysor path.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.binning","title":"<code>binning</code>  <code>property</code> <code>writable</code>","text":"<p>Camera binning.</p> <p>Returns:</p> Name Type Description <code>binning</code> <code>int</code> <p>Camera binning.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.bit_ids","title":"<code>bit_ids</code>  <code>property</code>","text":"<p>Bit IDs.</p> <p>Returns:</p> Name Type Description <code>bit_ids</code> <code>Collection[str]</code> <p>Bit IDs.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.camera_model","title":"<code>camera_model</code>  <code>property</code> <code>writable</code>","text":"<p>Camera model.</p> <p>Returns:</p> Name Type Description <code>camera_model</code> <code>Optional[str]</code> <p>Camera model.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.channel_psfs","title":"<code>channel_psfs</code>  <code>property</code> <code>writable</code>","text":"<p>Channel point spread functions (PSF).</p> Return <p>channel_psfs : ArrayLike     Channel point spread functions (PSF).</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.channel_shading_maps","title":"<code>channel_shading_maps</code>  <code>property</code> <code>writable</code>","text":"<p>Channel shaiding images.</p> <p>Returns:</p> Name Type Description <code>channel_shading_maps</code> <code>ArrayLike</code> <p>Channel shading images.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.channels_in_data","title":"<code>channels_in_data</code>  <code>property</code> <code>writable</code>","text":"<p>Channel indices.</p> <p>Returns:</p> Name Type Description <code>channels_in_data</code> <code>Collection[int]</code> <p>Channel indices.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.codebook","title":"<code>codebook</code>  <code>property</code> <code>writable</code>","text":"<p>Codebook.</p> <p>Returns:</p> Name Type Description <code>codebook</code> <code>DataFrame</code> <p>Codebook.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.datastore_state","title":"<code>datastore_state</code>  <code>property</code> <code>writable</code>","text":"<p>Datastore state.</p> <p>Returns:</p> Name Type Description <code>datastore_state</code> <code>Optional[dict]</code> <p>Datastore state.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.e_per_ADU","title":"<code>e_per_ADU</code>  <code>property</code> <code>writable</code>","text":"<p>Electrons per camera ADU.</p> <p>Returns:</p> Name Type Description <code>e_per_ADU</code> <code>float</code> <p>Electrons per camera ADU.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.experiment_order","title":"<code>experiment_order</code>  <code>property</code> <code>writable</code>","text":"<p>Round and bit order.</p> <p>Returns:</p> Name Type Description <code>experiment_order</code> <code>DataFrame</code> <p>Round and bit order.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.global_background_vector","title":"<code>global_background_vector</code>  <code>property</code> <code>writable</code>","text":"<p>Global background vector.</p> <p>Returns:</p> Name Type Description <code>global_background_vector</code> <code>ArrayLike</code> <p>Global background vector.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.global_normalization_vector","title":"<code>global_normalization_vector</code>  <code>property</code> <code>writable</code>","text":"<p>Global normalization vector.</p> <p>Returns:</p> Name Type Description <code>global_normalization_vector</code> <code>ArrayLike</code> <p>Global normalization vector.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.iterative_background_vector","title":"<code>iterative_background_vector</code>  <code>property</code> <code>writable</code>","text":"<p>Iterative background vector.</p> <p>Returns:</p> Name Type Description <code>iterative_background_vector</code> <code>ArrayLike</code> <p>Iterative background vector.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.iterative_normalization_vector","title":"<code>iterative_normalization_vector</code>  <code>property</code> <code>writable</code>","text":"<p>Iterative normalization vector.</p> <p>Returns:</p> Name Type Description <code>iterative_normalization_vector</code> <code>ArrayLike</code> <p>Iterative normalization vector.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.julia_threads","title":"<code>julia_threads</code>  <code>property</code> <code>writable</code>","text":"<p>Julia thread number</p> <p>Returns:</p> Name Type Description <code>julia_threads</code> <code>int</code> <p>Julia thread number.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.microscope_type","title":"<code>microscope_type</code>  <code>property</code> <code>writable</code>","text":"<p>Microscope type.</p> <p>Returns:</p> Name Type Description <code>microscope_type</code> <code>Optional[str]</code> <p>Microscope type.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.na","title":"<code>na</code>  <code>property</code> <code>writable</code>","text":"<p>Detection objective numerical aperture (NA).</p> <p>Returns:</p> Name Type Description <code>na</code> <code>float</code> <p>Detection objective numerical aperture (NA).</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.noise_map","title":"<code>noise_map</code>  <code>property</code> <code>writable</code>","text":"<p>Camera noise image.</p> <p>Returns:</p> Name Type Description <code>noise_map</code> <code>ArrayLike</code> <p>Camera noise image.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.num_bits","title":"<code>num_bits</code>  <code>property</code>","text":"<p>Number of bits.</p> <p>Returns:</p> Name Type Description <code>num_bits</code> <code>int</code> <p>Number of bits.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.num_rounds","title":"<code>num_rounds</code>  <code>property</code> <code>writable</code>","text":"<p>Number of rounds.</p> <p>Returns:</p> Name Type Description <code>num_rounds</code> <code>int</code> <p>Number of rounds.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.num_tiles","title":"<code>num_tiles</code>  <code>property</code> <code>writable</code>","text":"<p>Number of tiles.</p> <p>Returns:</p> Name Type Description <code>num_tiles</code> <code>int</code> <p>Number of tiles.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.ri","title":"<code>ri</code>  <code>property</code> <code>writable</code>","text":"<p>Detection objective refractive index (RI).</p> <p>Returns:</p> Name Type Description <code>ri</code> <code>float</code> <p>Detection objective refractive index (RI).</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.round_ids","title":"<code>round_ids</code>  <code>property</code>","text":"<p>Round IDs.</p> <p>Returns:</p> Name Type Description <code>round_ids</code> <code>Collection[str]</code> <p>Round IDs.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.tile_ids","title":"<code>tile_ids</code>  <code>property</code>","text":"<p>Tile IDs.</p> <p>Returns:</p> Name Type Description <code>tile_ids</code> <code>Collection[str]</code> <p>Tile IDs.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.tile_overlap","title":"<code>tile_overlap</code>  <code>property</code> <code>writable</code>","text":"<p>XY tile overlap.</p> <p>Returns:</p> Name Type Description <code>tile_overlap</code> <code>float</code> <p>XY tile overlap.</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.voxel_size_zyx_um","title":"<code>voxel_size_zyx_um</code>  <code>property</code> <code>writable</code>","text":"<p>Voxel size, zyx order (microns).</p> <p>Returns:</p> Name Type Description <code>voxel_size_zyx_um</code> <code>ArrayLike</code> <p>Voxel size, zyx order (microns).</p>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.initialize_tile","title":"<code>initialize_tile(tile)</code>","text":"<p>Initialize directory structure for a tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def initialize_tile(\n    self,\n    tile: Union[int, str],\n):\n    \"\"\"Initialize directory structure for a tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n    \"\"\"\n\n    if getattr(self, \"_experiment_order\", None) is None:\n        print(\"Assign experimental order before creating tiles.\")\n        return None\n\n    if getattr(self, \"_num_tiles\", None) is None:\n        print(\"Assign number of tiles before creating tiles.\")\n        return None\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tile id.\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    try:\n        polyDT_tile_path = self._polyDT_root_path / Path(tile_id)\n        polyDT_tile_path.mkdir()\n        for round_idx, round_id in enumerate(self._round_ids):\n            polyDT_round_path = polyDT_tile_path / Path(round_id + \".zarr\")\n            polyDT_round_path.mkdir()\n            polydt_round_attrs_path = polyDT_round_path / Path(\".zattrs\")\n            round_attrs = {\n                \"bit_linker\": self._experiment_order.to_numpy()[round_idx, 1:]\n                .astype(int)\n                .tolist(),\n            }\n            self._save_to_json(round_attrs, polydt_round_attrs_path)\n    except FileExistsError:\n        print(\"Error creating polyDT tile. Does it exist already?\")\n\n    try:\n        readout_tile_path = self._readouts_root_path / Path(tile_id)\n        readout_tile_path.mkdir()\n        for bit_idx, bit_id in enumerate(self._bit_ids):\n            readout_bit_path = readout_tile_path / Path(bit_id + \".zarr\")\n            readout_bit_path.mkdir()\n            readout_bit_attrs_path = readout_bit_path / Path(\".zattrs\")\n            fiducial_channel = str(self._channels_in_data[0])\n            readout_one_channel = str(self._channels_in_data[1])\n\n            if len(self._channels_in_data) == 3:\n                readout_two_channel = str(self._channels_in_data[2])\n                condition_one = self._experiment_order[readout_one_channel] == (\n                    bit_idx + 1\n                )\n                condition_two = self._experiment_order[readout_two_channel] == (\n                    bit_idx + 1\n                )\n                combined_condition = condition_one | condition_two\n\n            else:\n                combined_condition = self._experiment_order[\n                    readout_one_channel\n                ] == (bit_idx + 1)\n            matching_rows = self._experiment_order.loc[combined_condition]\n\n            bit_attrs = {\n                \"round_linker\": int(matching_rows[fiducial_channel].values[0])\n            }\n            self._save_to_json(bit_attrs, readout_bit_attrs_path)\n    except FileExistsError:\n        print(\"Error creating readout tile. Does it exist already?\")\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_codebook_parsed","title":"<code>load_codebook_parsed()</code>","text":"<p>Load and split codebook into gene_ids and codebook matrix.</p> <p>Returns:</p> Name Type Description <code>gene_ids</code> <code>Collection[str]</code> <p>Gene IDs.</p> <code>codebook_matrix</code> <code>ArrayLike</code> <p>Codebook matrix.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_codebook_parsed(\n    self,\n) -&gt; Optional[tuple[Collection[str], ArrayLike]]:\n    \"\"\"Load and split codebook into gene_ids and codebook matrix.\n\n    Returns\n    -------\n    gene_ids : Collection[str]\n        Gene IDs.\n    codebook_matrix : ArrayLike\n        Codebook matrix.\n    \"\"\"\n\n    try:\n        data = getattr(self, \"_codebook\", None)\n\n        if data is None:\n            return None\n        num_columns = len(data[0]) if data else 0\n        columns = [\"gene_id\"] + [f\"bit{i:02d}\" for i in range(1, num_columns)]\n        codebook_df = pd.DataFrame(data, columns=columns)\n\n        gene_ids = codebook_df.iloc[:, 0].tolist()\n        codebook_matrix = codebook_df.iloc[:, 1:].to_numpy().astype(int)\n        del data, codebook_df\n        return gene_ids, codebook_matrix\n    except (KeyError, ValueError, TypeError):\n        print(\"Error parsing codebook.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_coord_of_xform_px","title":"<code>load_coord_of_xform_px(tile, round, return_future=True)</code>","text":"<p>Local fidicual optical flow matrix for one round and tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Optional[Union[int, str]]</code> <p>Tile index or tile id.</p> required <code>round</code> <code>Optional[Union[int, str]]</code> <p>Round index or round id.</p> required <code>return_future</code> <code>Optional[bool]</code> <p>Return future array.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>of_xform_px</code> <code>Optional[ArrayLike]</code> <p>Local fidicual optical flow matrix for one round and tile.</p> <code>downsampling</code> <code>Optional[ArrayLike]</code> <p>Downsampling factor.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_coord_of_xform_px(\n    self,\n    tile: Optional[Union[int, str]],\n    round: Optional[Union[int, str]],\n    return_future: Optional[bool] = True,\n) -&gt; Optional[tuple[ArrayLike, ArrayLike]]:\n    \"\"\"Local fidicual optical flow matrix for one round and tile.\n\n    Parameters\n    ----------\n    tile : Optional[Union[int, str]]\n        Tile index or tile id.\n    round : Optional[Union[int, str]]\n        Round index or round id.\n    return_future : Optional[bool]\n        Return future array.\n\n    Returns\n    -------\n    of_xform_px : Optional[ArrayLike]\n        Local fidicual optical flow matrix for one round and tile.\n    downsampling : Optional[ArrayLike]\n        Downsampling factor.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(round, int):\n        if round &lt; 0:\n            print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n            return None\n        else:\n            round_id = self._round_ids[round]\n    elif isinstance(round, str):\n        if round not in self._round_ids:\n            print(\"Set valid round id\")\n            return None\n        else:\n            round_id = round\n    else:\n        print(\"'round' must be integer index or string identifier\")\n        return None\n\n    current_local_zarr_path = str(\n        self._polyDT_root_path\n        / Path(tile_id)\n        / Path(round_id + \".zarr\")\n        / Path(\"of_xform_px\")\n    )\n    zattrs_path = str(\n        self._polyDT_root_path\n        / Path(tile_id)\n        / Path(round_id + \".zarr\")\n        / Path(\".zattrs\")\n    )\n\n    if not Path(current_local_zarr_path).exists():\n        print(\"Optical flow transform mapping back to first round not found.\")\n        return None\n\n    try:\n        compressor = {\n            \"id\": \"blosc\",\n            \"cname\": \"zstd\",\n            \"clevel\": 5,\n            \"shuffle\": 2,\n        }\n        spec_of = {\n            \"driver\": \"zarr\",\n            \"kvstore\": None,\n            \"metadata\": {\"compressor\": compressor},\n            \"open\": True,\n            \"assume_metadata\": False,\n            \"create\": True,\n            \"delete_existing\": False,\n        }\n        spec_of[\"metadata\"][\"dtype\"] = \"&lt;f4\"\n        of_xform_px = self._load_from_zarr_array(\n            self._get_kvstore_key(current_local_zarr_path),\n            spec_of.copy(),\n            return_future,\n        )\n        attributes = self._load_from_json(zattrs_path)\n        downsampling = np.asarray(\n            attributes[\"opticalflow_downsampling\"], dtype=np.float32\n        )\n\n        return of_xform_px, downsampling\n    except (IOError, OSError, ZarrError) as e:\n        print(e)\n        print(\"Error loading optical flow transform.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_global_baysor_filtered_spots","title":"<code>load_global_baysor_filtered_spots()</code>","text":"<p>Load Baysor re-assigned decoded RNA.</p> <p>Assumes Baysor has been run.</p> <p>Returns:</p> Name Type Description <code>baysor_filtered_genes</code> <code>Optional[DataFrame]</code> <p>Baysor re-assigned decoded RNA.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_global_baysor_filtered_spots(\n    self,\n) -&gt; Optional[pd.DataFrame]:\n    \"\"\"Load Baysor re-assigned decoded RNA.\n\n    Assumes Baysor has been run.\n\n    Returns\n    -------\n    baysor_filtered_genes : Optional[pd.DataFrame]\n        Baysor re-assigned decoded RNA.\n    \"\"\"\n\n    current_baysor_spots_path = (\n        self._segmentation_root_path\n        / Path(\"baysor\")\n        / Path(\"segmentation.csv\")\n    )\n\n    if not current_baysor_spots_path.exists():\n        print(\"Baysor filtered genes not found.\")\n        return None\n    else:\n        baysor_filtered_genes = self._load_from_csv(current_baysor_spots_path)\n        return baysor_filtered_genes\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_global_baysor_outlines","title":"<code>load_global_baysor_outlines()</code>","text":"<p>Load Baysor cell outlines.</p> <p>Assumes Baysor has been run.</p> <p>Returns:</p> Name Type Description <code>baysor_outlines</code> <code>Optional[dict]</code> <p>Baysor cell outlines.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_global_baysor_outlines(\n    self,\n) -&gt; Optional[dict]:\n    \"\"\"Load Baysor cell outlines.\n\n    Assumes Baysor has been run.\n\n    Returns\n    -------\n    baysor_outlines : Optional[dict]\n        Baysor cell outlines.\n    \"\"\"\n\n    current_baysor_outlines_path = (\n        self._segmentation_root_path \n        / Path(\"baysor\") \n        / Path(r\"3d_cell_rois.zip\")\n    )\n\n    if not current_baysor_outlines_path.exists():\n        print(\"Baysor outlines not found.\")\n        return None\n    else:\n        baysor_rois = roiread(current_baysor_outlines_path)\n        return baysor_rois\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_global_cellpose_outlines","title":"<code>load_global_cellpose_outlines()</code>","text":"<p>Load Cellpose max projection cell outlines.</p> <p>Returns:</p> Name Type Description <code>cellpose_outlines</code> <code>Optional[dict]</code> <p>Cellpose cell mask outlines.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_global_cellpose_outlines(\n    self,\n) -&gt; Optional[dict]:\n    \"\"\"Load Cellpose max projection cell outlines.\n\n    Returns\n    -------\n    cellpose_outlines : Optional[dict]\n        Cellpose cell mask outlines.\n    \"\"\"\n\n    current_cellpose_outlines_path = (\n        self._segmentation_root_path / Path(\"cellpose\") / Path(\"cell_outlines.json\")\n    )\n\n    if not current_cellpose_outlines_path.exists():\n        print(\"Cellpose cell mask outlines not found.\")\n        return None\n    else:\n        cellpose_outlines = self._load_from_microjson(\n            current_cellpose_outlines_path\n        )\n        return cellpose_outlines\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_global_cellpose_segmentation_image","title":"<code>load_global_cellpose_segmentation_image(return_future=True)</code>","text":"<p>Load Cellpose max projection, downsampled segmentation image.</p> <p>Parameters:</p> Name Type Description Default <code>return_future</code> <code>Optional[bool]</code> <p>Return future array.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>fused_image</code> <code>Optional[ArrayLike]</code> <p>Cellpose max projection, downsampled segmentation image.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_global_cellpose_segmentation_image(\n    self,\n    return_future: Optional[bool] = True,\n) -&gt; Optional[ArrayLike]:\n    \"\"\"Load Cellpose max projection, downsampled segmentation image.\n\n    Parameters\n    ----------\n    return_future : Optional[bool]\n        Return future array.\n\n    Returns\n    -------\n    fused_image : Optional[ArrayLike]\n        Cellpose max projection, downsampled segmentation image.\n    \"\"\"\n\n    current_local_zarr_path = str(\n        self._segmentation_root_path\n        / Path(\"cellpose\")\n        / Path(\"cellpose.zarr\")\n        / Path(\"masks_polyDT_iso_zyx\")\n    )\n\n    if not current_local_zarr_path.exists():\n        print(\"Cellpose prediction on global fused image not found.\")\n        return None\n\n    try:\n        fused_image = self._load_from_zarr_array(\n            self._get_kvstore_key(current_local_zarr_path),\n            self._zarrv2_spec.copy(),\n            return_future,\n        )\n        return fused_image\n    except (IOError, OSError, ZarrError):\n        print(\"Error loading Cellpose image.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_global_coord_xforms_um","title":"<code>load_global_coord_xforms_um(tile)</code>","text":"<p>Load global registration transform for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <p>Returns:</p> Name Type Description <code>affine_zyx_um</code> <code>Optional[ArrayLike]</code> <p>Global affine registration transform for one tile.</p> <code>origin_zyx_um</code> <code>Optional[ArrayLike]</code> <p>Global origin registration transform for one tile.</p> <code>spacing_zyx_um</code> <code>Optional[ArrayLike]</code> <p>Global spacing registration transform for one tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_global_coord_xforms_um(\n    self,\n    tile: Union[int, str],\n) -&gt; Optional[tuple[ArrayLike, ArrayLike, ArrayLike]]:\n    \"\"\"Load global registration transform for one tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n\n    Returns\n    -------\n    affine_zyx_um : Optional[ArrayLike]\n        Global affine registration transform for one tile.\n    origin_zyx_um : Optional[ArrayLike]\n        Global origin registration transform for one tile.\n    spacing_zyx_um : Optional[ArrayLike]\n        Global spacing registration transform for one tile.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None, None, None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None, None, None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    try:\n        zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(self._round_ids[0] + \".zarr\")\n            / Path(\".zattrs\")\n        )\n        attributes = self._load_from_json(zattrs_path)\n        affine_zyx_um = np.asarray(attributes[\"affine_zyx_um\"], dtype=np.float32)\n        origin_zyx_um = np.asarray(attributes[\"origin_zyx_um\"], dtype=np.float32)\n        spacing_zyx_um = np.asarray(attributes[\"spacing_zyx_um\"], dtype=np.float32)\n        return (affine_zyx_um, origin_zyx_um, spacing_zyx_um)\n    except (FileNotFoundError, json.JSONDecodeError):\n        print(tile_id, self._round_ids[0])\n        print(\"Global coordinate transforms not found\")\n        return None, None, None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_global_fidicual_image","title":"<code>load_global_fidicual_image(return_future=True)</code>","text":"<p>Load downsampled, fused fidicual image.</p> <p>Parameters:</p> Name Type Description Default <code>return_future</code> <code>Optional[bool]</code> <p>Return future array.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>fused_image</code> <code>Optional[ArrayLike]</code> <p>Downsampled, fused fidicual image.</p> <code>affine_zyx_um</code> <code>Optional[ArrayLike]</code> <p>Global affine registration transform for fused image.</p> <code>origin_zyx_um</code> <code>Optional[ArrayLike]</code> <p>Global origin registration transform for fused image.</p> <code>spacing_zyx_um</code> <code>Optional[ArrayLike]</code> <p>Global spacing registration transform for fused image.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_global_fidicual_image(\n    self,\n    return_future: Optional[bool] = True,\n) -&gt; Optional[tuple[ArrayLike, ArrayLike, ArrayLike, ArrayLike]]:\n    \"\"\"Load downsampled, fused fidicual image.\n\n    Parameters\n    ----------\n    return_future : Optional[bool]\n        Return future array.\n\n    Returns\n    -------\n    fused_image : Optional[ArrayLike]\n        Downsampled, fused fidicual image.\n    affine_zyx_um : Optional[ArrayLike]\n        Global affine registration transform for fused image.\n    origin_zyx_um : Optional[ArrayLike]\n        Global origin registration transform for fused image.\n    spacing_zyx_um : Optional[ArrayLike]\n        Global spacing registration transform for fused image.\n    \"\"\"\n\n    current_local_zarr_path = str(\n        self._fused_root_path / Path(\"fused.zarr\") / Path(\"fused_polyDT_iso_zyx\")\n    )\n\n    if not Path(current_local_zarr_path).exists():\n        print(\"Globally registered, fused image not found.\")\n        return None\n\n    zattrs_path = str(current_local_zarr_path / Path(\".zattrs\"))\n\n    try:\n        fused_image = self._load_from_zarr_array(\n            self._get_kvstore_key(current_local_zarr_path),\n            self._zarrv2_spec.copy(),\n            return_future,\n        )\n        attributes = self._load_from_json(zattrs_path)\n        affine_zyx_um = np.asarray(attributes[\"affine_zyx_um\"], dtype=np.float32)\n        origin_zyx_um = np.asarray(attributes[\"origin_zyx_um\"], dtype=np.float32)\n        spacing_zyx_um = np.asarray(attributes[\"spacing_zyx_um\"], dtype=np.float32)\n        return fused_image, affine_zyx_um, origin_zyx_um, spacing_zyx_um\n    except (IOError, OSError, ZarrError):\n        print(\"Error loading globally registered, fused image.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_global_filtered_decoded_spots","title":"<code>load_global_filtered_decoded_spots()</code>","text":"<p>Load all decoded and filtered spots.</p> <p>Returns:</p> Name Type Description <code>all_tiles_filtered</code> <code>Optional[DataFrame]</code> <p>All decoded and filtered spots.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_global_filtered_decoded_spots(\n    self,\n) -&gt; Optional[pd.DataFrame]:\n    \"\"\"Load all decoded and filtered spots.\n\n    Returns\n    -------\n    all_tiles_filtered : Optional[pd.DataFrame]\n        All decoded and filtered spots.\n    \"\"\"\n\n    current_global_filtered_decoded_dir_path = self._datastore_path / Path(\n        \"all_tiles_filtered_decoded_features\"\n    )\n    current_global_filtered_decoded_path = (\n        current_global_filtered_decoded_dir_path / Path(\"decoded_features.parquet\")\n    )\n\n    if not current_global_filtered_decoded_path.exists():\n        print(\"Global, filtered, decoded spots not found.\")\n        return None\n    else:\n        all_tiles_filtered = self._load_from_parquet(\n            current_global_filtered_decoded_path\n        )\n        return all_tiles_filtered\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_local_bit_linker","title":"<code>load_local_bit_linker(tile, round)</code>","text":"<p>Load readout bits linked to fidicual round for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>round</code> <code>Union[int, str]</code> <p>Round index or round id.</p> required <p>Returns:</p> Name Type Description <code>bit_linker</code> <code>Optional[Sequence[int]]</code> <p>Readout bits linked to fidicual round for one tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_local_bit_linker(\n    self,\n    tile: Union[int, str],\n    round: Union[int, str],\n) -&gt; Optional[Sequence[int]]:\n    \"\"\"Load readout bits linked to fidicual round for one tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n    round : Union[int, str]\n        Round index or round id.\n\n    Returns\n    -------\n    bit_linker : Optional[Sequence[int]]\n        Readout bits linked to fidicual round for one tile.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id.\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(round, int):\n        if round &lt; 0:\n            print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n            return None\n        else:\n            round_id = self._round_ids[round]\n    elif isinstance(round, str):\n        if round not in self._round_ids:\n            print(\"Set valid round id.\")\n            return None\n        else:\n            round_id = round\n    else:\n        print(\"'round' must be integer index or string identifier\")\n        return None\n\n    try:\n        zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(round_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n        attributes = self._load_from_json(zattrs_path)\n        return attributes[\"bits\"][1:]\n    except (FileNotFoundError, json.JSONDecodeError):\n        print(tile_id, round_id)\n        print(\"Bit linker attribute not found.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_local_corrected_image","title":"<code>load_local_corrected_image(tile, round=None, bit=None, return_future=True)</code>","text":"<p>Load gain and offset corrected image for fiducial OR readout bit for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>round</code> <code>Optional[Union[int, str]]</code> <p>Round index or round id.</p> <code>None</code> <code>bit</code> <code>Optional[Union[int, str]]</code> <p>Bit index or bit id.</p> <code>None</code> <code>return_future</code> <code>Optional[bool]</code> <p>Return future array.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>corrected_image</code> <code>Optional[ArrayLike]</code> <p>Gain and offset corrected image for fiducial OR readout bit for one tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_local_corrected_image(\n    self,\n    tile: Union[int, str],\n    round: Optional[Union[int, str]] = None,\n    bit: Optional[Union[int, str]] = None,\n    return_future: Optional[bool] = True,\n) -&gt; Optional[ArrayLike]:\n    \"\"\"Load gain and offset corrected image for fiducial OR readout bit for one tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n    round : Optional[Union[int, str]]\n        Round index or round id.\n    bit : Optional[Union[int, str]]\n        Bit index or bit id.\n    return_future : Optional[bool]\n        Return future array.\n\n    Returns\n    -------\n    corrected_image : Optional[ArrayLike]\n        Gain and offset corrected image for fiducial OR readout bit for one tile.\n    \"\"\"\n\n    if (round is None and bit is None) or (round is not None and bit is not None):\n        print(\"Provide either 'round' or 'bit', but not both\")\n        return None\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if bit is not None:\n        if isinstance(bit, int):\n            if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                return None\n            else:\n                local_id = self._bit_ids[bit]\n        elif isinstance(bit, str):\n            if bit not in self._bit_ids:\n                print(\"Set valid bit id\")\n                return None\n            else:\n                local_id = bit\n        else:\n            print(\"'bit' must be integer index or string identifier\")\n            return None\n        current_local_zarr_path = str(\n            self._readouts_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\"corrected_data\")\n        )\n    else:\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                local_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id\")\n                return None\n            else:\n                local_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n        current_local_zarr_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\"corrected_data\")\n        )\n\n    if not Path(current_local_zarr_path).exists():\n        print(\"Corrected image not found.\")\n        return None\n\n    try:\n        spec = self._zarrv2_spec.copy()\n        spec[\"metadata\"][\"dtype\"] = \"&lt;u2\"\n        corrected_image = self._load_from_zarr_array(\n            self._get_kvstore_key(current_local_zarr_path),\n            spec,\n            return_future,\n        )\n        return corrected_image\n    except (IOError, OSError, ZarrError):\n        print(\"Error loading corrected image.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_local_decoded_spots","title":"<code>load_local_decoded_spots(tile)</code>","text":"<p>Load decoded spots and features for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <p>Returns:</p> Name Type Description <code>tile_features</code> <code>Optional[DataFrame]</code> <p>Decoded spots and features for one tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_local_decoded_spots(\n    self,\n    tile: Union[int, str],\n) -&gt; Optional[pd.DataFrame]:\n    \"\"\"Load decoded spots and features for one tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n\n    Returns\n    -------\n    tile_features : Optional[pd.DataFrame]\n        Decoded spots and features for one tile.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    current_tile_features_path = self._decoded_root_path / Path(\n        tile_id + \"_decoded_features.parquet\"\n    )\n\n    if not current_tile_features_path.exists():\n        print(\"Decoded spots not found.\")\n        return None\n    else:\n        tile_features = self._load_from_parquet(current_tile_features_path)\n        return tile_features\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_local_registered_image","title":"<code>load_local_registered_image(tile, round=None, bit=None, return_future=True)</code>","text":"<p>Local registered, deconvolved image for fidiculial OR readout bit for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>round</code> <code>Optional[Union[int, str]]</code> <p>Round index or round id.</p> <code>None</code> <code>bit</code> <code>Optional[Union[int, str]]</code> <p>Bit index or bit id.</p> <code>None</code> <code>return_future</code> <code>Optional[bool]</code> <p>Return future array.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>registered_decon_image</code> <code>Optional[ArrayLike]</code> <p>Registered, deconvolved image for fidiculial OR readout bit for one tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_local_registered_image(\n    self,\n    tile: Union[int, str],\n    round: Optional[Union[int, str]] = None,\n    bit: Optional[Union[int, str]] = None,\n    return_future: Optional[bool] = True,\n) -&gt; Optional[ArrayLike]:\n    \"\"\"Local registered, deconvolved image for fidiculial OR readout bit for one tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n    round : Optional[Union[int, str]]\n        Round index or round id.\n    bit : Optional[Union[int, str]]\n        Bit index or bit id.\n    return_future : Optional[bool]\n        Return future array.\n\n    Returns\n    -------\n    registered_decon_image : Optional[ArrayLike]\n        Registered, deconvolved image for fidiculial OR readout bit for one tile.\n    \"\"\"\n\n    if (round is None and bit is None) or (round is not None and bit is not None):\n        print(\"Provide either 'round' or 'bit', but not both\")\n        return None\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if bit is not None:\n        if isinstance(bit, int):\n            if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                return None\n            else:\n                local_id = self._bit_ids[bit]\n        elif isinstance(bit, str):\n            if bit not in self._bit_ids:\n                print(\"Set valid bit id\")\n                return None\n            else:\n                local_id = bit\n        else:\n            print(\"'bit' must be integer index or string identifier\")\n            return None\n        current_local_zarr_path = str(\n            self._readouts_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\"registered_decon_data\")\n        )\n    else:\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                local_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id\")\n                return None\n            else:\n                local_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n        current_local_zarr_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\"registered_decon_data\")\n        )\n\n    if not Path(current_local_zarr_path).exists():\n        print(\"Registered deconvolved image not found.\")\n        return None\n\n    try:\n        spec = self._zarrv2_spec.copy()\n        spec[\"metadata\"][\"dtype\"] = \"&lt;u2\"\n        registered_decon_image = self._load_from_zarr_array(\n            self._get_kvstore_key(current_local_zarr_path),\n            spec,\n            return_future,\n        )\n        return registered_decon_image\n    except (IOError, OSError, ZarrError) as e:\n        print(e)\n        print(\"Error loading registered deconvolved image.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_local_rigid_xform_xyz_px","title":"<code>load_local_rigid_xform_xyz_px(tile, round)</code>","text":"<p>Load calculated rigid registration transform for one round and tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>round</code> <code>Union[int, str]</code> <p>Round index or round id.</p> required <p>Returns:</p> Name Type Description <code>rigid_xform_xyz_px</code> <code>Optional[ArrayLike]</code> <p>Local rigid registration transform for one round and tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_local_rigid_xform_xyz_px(\n    self,\n    tile: Union[int, str],\n    round: Union[int, str],\n) -&gt; Optional[ArrayLike]:\n    \"\"\"Load calculated rigid registration transform for one round and tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n    round : Union[int, str]\n        Round index or round id.\n\n    Returns\n    -------\n    rigid_xform_xyz_px : Optional[ArrayLike]\n        Local rigid registration transform for one round and tile.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(round, int):\n        if round &lt; 0:\n            print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n            return None\n        else:\n            round_id = self._round_ids[round]\n    elif isinstance(round, str):\n        if round not in self._round_ids:\n            print(\"Set valid round id\")\n            return None\n        else:\n            round_id = round\n    else:\n        print(\"'round' must be integer index or string identifier\")\n        return None\n    try:\n        zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(round_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n        attributes = self._load_from_json(zattrs_path)\n        rigid_xform_xyz_px = np.asarray(\n            attributes[\"rigid_xform_xyz_px\"], dtype=np.float32\n        )\n        return rigid_xform_xyz_px\n    except (FileNotFoundError, json.JSONDecodeError):\n        print(tile_id, round_id)\n        print(\"Rigid transform mapping back to first round not found.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_local_round_linker","title":"<code>load_local_round_linker(tile, bit)</code>","text":"<p>Load fidicual round linked to readout bit for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>bit</code> <code>Union[int, str]</code> <p>Bit index or bit id.</p> required <p>Returns:</p> Name Type Description <code>round_linker</code> <code>Optional[Sequence[int]]</code> <p>Fidicual round linked to readout bit for one tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_local_round_linker(\n    self,\n    tile: Union[int, str],\n    bit: Union[int, str],\n) -&gt; Optional[Sequence[int]]:\n    \"\"\"Load fidicual round linked to readout bit for one tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n    bit : Union[int, str]\n        Bit index or bit id.\n\n    Returns\n    -------\n    round_linker : Optional[Sequence[int]]\n        Fidicual round linked to readout bit for one tile.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id.\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(bit, int):\n        if bit &lt; 0 or bit &gt; len(self._bit_ids):\n            print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n            return None\n        else:\n            bit_id = self._bit_ids[bit]\n    elif isinstance(bit, str):\n        if bit not in self._bit_ids:\n            print(\"Set valid bit id.\")\n            return None\n        else:\n            bit_id = bit\n    else:\n        print(\"'bit' must be integer index or string identifier\")\n        return None\n\n    try:\n        zattrs_path = str(\n            self._readouts_root_path\n            / Path(tile_id)\n            / Path(bit_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n        attributes = self._load_from_json(zattrs_path)\n        return int(attributes[\"round_linker\"])\n    except FileNotFoundError:\n        print(tile_id, bit_id)\n        print(\"Round linker attribute not found.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_local_stage_position_zyx_um","title":"<code>load_local_stage_position_zyx_um(tile, round)</code>","text":"<p>Load tile stage position for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>round</code> <code>Union[int, str]</code> <p>Round index or round id.</p> required <p>Returns:</p> Name Type Description <code>stage_zyx_um</code> <code>Optional[ArrayLike]</code> <p>Tile stage position for one tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_local_stage_position_zyx_um(\n    self,\n    tile: Union[int, str],\n    round: Union[int, str],\n) -&gt; Optional[ArrayLike]:\n    \"\"\"Load tile stage position for one tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n    round : Union[int, str]\n        Round index or round id.\n\n    Returns\n    -------\n    stage_zyx_um : Optional[ArrayLike]\n        Tile stage position for one tile.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id.\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(round, int):\n        if round &lt; 0:\n            print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n            return None\n        else:\n            round_id = self._round_ids[round]\n    elif isinstance(round, str):\n        if round not in self._round_ids:\n            print(\"Set valid round id.\")\n            return None\n        else:\n            round_id = round\n    else:\n        print(\"'round' must be integer index or string identifier\")\n        return None\n\n    try:\n        zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(round_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n        attributes = self._load_from_json(zattrs_path)\n        return np.asarray(attributes[\"stage_zyx_um\"], dtype=np.float32)\n    except FileNotFoundError:\n        print(tile_id, round_id)\n        print(\"Stage position attribute not found.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_local_ufish_image","title":"<code>load_local_ufish_image(tile, bit, return_future=True)</code>","text":"<p>Load readout bit U-FISH prediction image for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>bit</code> <code>Union[int, str]</code> <p>Bit index or bit id.</p> required <code>return_future</code> <code>Optional[bool]</code> <code>True</code> <p>Returns:</p> Name Type Description <code>registered_ufish_image</code> <code>Optional[ArrayLike]</code> <p>U-FISH prediction image for one tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_local_ufish_image(\n    self,\n    tile: Union[int, str],\n    bit: Union[int, str],\n    return_future: Optional[bool] = True,\n) -&gt; Optional[ArrayLike]:\n    \"\"\"Load readout bit U-FISH prediction image for one tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n    bit : Union[int, str]\n        Bit index or bit id.\n    return_future : Optional[bool]\n\n    Returns\n    -------\n    registered_ufish_image : Optional[ArrayLike]\n        U-FISH prediction image for one tile.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(bit, int):\n        if bit &lt; 0 or bit &gt; len(self._bit_ids):\n            print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n            return None\n        else:\n            bit_id = self._bit_ids[bit]\n    elif isinstance(bit, str):\n        if bit not in self._bit_ids:\n            print(\"Set valid bit id\")\n            return None\n        else:\n            bit_id = bit\n    else:\n        print(\"'bit' must be integer index or string identifier\")\n        return None\n\n    current_local_zarr_path = str(\n        self._readouts_root_path\n        / Path(tile_id)\n        / Path(bit_id + \".zarr\")\n        / Path(\"registered_ufish_data\")\n    )\n\n    if not Path(current_local_zarr_path).exists():\n        print(\"U-FISH prediction image not found.\")\n        return None\n\n    try:\n        spec = self._zarrv2_spec.copy()\n        spec[\"metadata\"][\"dtype\"] = \"&lt;f4\"\n        registered_ufish_image = self._load_from_zarr_array(\n            self._get_kvstore_key(current_local_zarr_path),\n            spec,\n            return_future,\n        )\n        return registered_ufish_image\n    except (IOError, OSError, ZarrError) as e:\n        print(e)\n        print(\"Error loading U-FISH image.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_local_ufish_spots","title":"<code>load_local_ufish_spots(tile, bit)</code>","text":"<p>Load U-FISH spot localizations and features for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>bit</code> <code>Union[int, str]</code> <p>Bit index or bit id.</p> required <p>Returns:</p> Name Type Description <code>ufish_localizations</code> <code>Optional[DataFrame]</code> <p>U-FISH localizations and features for one tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_local_ufish_spots(\n    self,\n    tile: Union[int, str],\n    bit: Union[int, str],\n) -&gt; Optional[pd.DataFrame]:\n    \"\"\"Load U-FISH spot localizations and features for one tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n    bit : Union[int, str]\n        Bit index or bit id.\n\n    Returns\n    -------\n    ufish_localizations : Optional[pd.DataFrame]\n        U-FISH localizations and features for one tile.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(bit, int):\n        if bit &lt; 0 or bit &gt; len(self._bit_ids):\n            print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n            return None\n        else:\n            bit_id = self._bit_ids[bit]\n    elif isinstance(bit, str):\n        if bit not in self._bit_ids:\n            print(\"Set valid bit id\")\n            return None\n        else:\n            bit_id = bit\n    else:\n        print(\"'bit' must be integer index or string identifier\")\n        return None\n\n    current_ufish_localizations_path = (\n        self._ufish_localizations_root_path\n        / Path(tile_id)\n        / Path(bit_id + \".parquet\")\n    )\n\n    if not current_ufish_localizations_path.exists():\n        print(\"U-FISH localizations not found.\")\n        return None\n    else:\n        ufish_localizations = self._load_from_parquet(\n            current_ufish_localizations_path\n        )\n        return ufish_localizations\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.load_local_wavelengths_um","title":"<code>load_local_wavelengths_um(tile, round=None, bit=None)</code>","text":"<p>Load wavelengths for fidicual OR readout bit for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>round</code> <code>Optional[Union[int, str]]</code> <p>Round index or round id.</p> <code>None</code> <code>bit</code> <code>Optional[Union[int, str]]</code> <p>Bit index or bit id.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>wavelengths_um</code> <code>Optional[tuple[float, float]]</code> <p>Wavelengths for fidicual OR readout bit for one tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def load_local_wavelengths_um(\n    self,\n    tile: Union[int, str],\n    round: Optional[Union[int, str]] = None,\n    bit: Optional[Union[int, str]] = None,\n) -&gt; Optional[tuple[float, float]]:\n    \"\"\"Load wavelengths for fidicual OR readout bit for one tile.\n\n    Parameters\n    ----------\n    tile : Union[int, str]\n        Tile index or tile id.\n    round : Optional[Union[int, str]]   \n        Round index or round id.\n    bit : Optional[Union[int, str]]\n        Bit index or bit id.\n\n    Returns\n    -------\n    wavelengths_um : Optional[tuple[float, float]]\n        Wavelengths for fidicual OR readout bit for one tile.\n    \"\"\"\n\n    if (round is None and bit is None) or (round is not None and bit is not None):\n        print(\"Provide either 'round' or 'bit', but not both\")\n        return None\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if bit is not None:\n        if isinstance(bit, int):\n            if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                return None\n            else:\n                local_id = self._bit_ids[bit]\n        elif isinstance(bit, str):\n            if bit not in self._bit_ids:\n                print(\"Set valid bit id\")\n                return None\n            else:\n                local_id = bit\n        else:\n            print(\"'bit' must be integer index or string identifier\")\n            return None\n        zattrs_path = str(\n            self._readouts_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n    else:\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                local_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id\")\n                return None\n            else:\n                local_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n        zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n\n    try:\n        attributes = self._load_from_json(zattrs_path)\n        ex_wavelength_um = attributes[\"excitation_um\"]\n        em_wavelength_um = attributes[\"emission_um\"]\n        return (ex_wavelength_um, em_wavelength_um)\n    except KeyError:\n        print(\"Wavelength attributes not found.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.reformat_baysor_3D_oultines","title":"<code>reformat_baysor_3D_oultines()</code>","text":"<p>Reformat baysor 3D json file into ImageJ ROIs.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def reformat_baysor_3D_oultines(self):\n    \"\"\"Reformat baysor 3D json file into ImageJ ROIs.\"\"\"\n    import re\n\n    # Load the JSON file\n    baysor_output_path = self._segmentation_root_path / Path(\"baysor\")\n    baysor_segmentation = baysor_output_path / Path(r\"segmentation_polygons_3d.json\")\n    with open(baysor_segmentation, 'r') as file:\n        data = json.load(file)\n\n\n    # Dictionary to group polygons by cell ID\n    cell_polygons = defaultdict(list)\n\n    def parse_z_range(z_range):\n        cleaned_range = re.sub(r\"[^\\d.,-]\", \"\", z_range)  # Remove non-numeric, non-period, non-comma, non-dash characters\n        return map(float, cleaned_range.split(\",\"))\n\n    # Iterate through each z-plane and corresponding polygons\n    for z_range, details in data.items():\n        z_start, z_end = parse_z_range(z_range)\n\n        for geometry in details[\"geometries\"]:\n            coordinates = geometry[\"coordinates\"][0]  # Assuming the outer ring of the polygon\n            cell_id = geometry[\"cell\"]  # Get the cell ID\n\n            # Store the polygon with its z-range\n            cell_polygons[cell_id].append({\n                \"z_start\": z_start,\n                \"z_end\": z_end,\n                \"coordinates\": coordinates\n            })\n\n    rois = []\n\n    # Process each cell ID to create 3D ROIs\n    for cell_id, polygons in cell_polygons.items():\n        for idx, polygon in enumerate(polygons):\n            x_coords = [point[0] for point in polygon[\"coordinates\"]]\n            y_coords = [point[1] for point in polygon[\"coordinates\"]]\n\n\n            z_start = polygon[\"z_start\"]\n            z_end = polygon[\"z_end\"]\n\n            try:\n                # Create an ImageJRoi object for the polygon using frompoints\n                coords = list(zip(x_coords, y_coords))  # List of (x, y) tuples\n                roi = ImagejRoi.frompoints(coords)\n                roi.roitype = ROI_TYPE.POLYGON  # Set the ROI type to Polygon\n                roi.coordinates = coords  # Explicitly assign coordinates to the ROI\n                roi.name = f\"cell_{str(cell_id)}_zstart_{str(z_start)}_zend_{str(z_end)}\"  # Ensure unique name\n                rois.append(roi)\n            except Exception as e:\n                print(f\"Error while creating ROI for cell ID {cell_id}: {e}\")\n\n    # Write all ROIs to a ZIP file   \n    output_file = baysor_output_path / Path(r\"3d_cell_rois.zip\")\n    roiwrite(output_file, rois,mode='w')\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.reprocess_and_save_filtered_spots_with_baysor_outlines","title":"<code>reprocess_and_save_filtered_spots_with_baysor_outlines()</code>","text":"<p>Reprocess filtered spots using baysor cell outlines, then save.</p> <p>Loads the 3D cell outlines from Baysor, checks all points to see what  (if any) cell outline that the spot falls within, and then saves the data back to the datastore.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def reprocess_and_save_filtered_spots_with_baysor_outlines(self):\n    \"\"\"Reprocess filtered spots using baysor cell outlines, then save.\n\n    Loads the 3D cell outlines from Baysor, checks all points to see what \n    (if any) cell outline that the spot falls within, and then saves the\n    data back to the datastore.\n    \"\"\"\n    from rtree import index\n    import re\n\n    rois = self.load_global_baysor_outlines()\n    filtered_spots_df = self.load_global_filtered_decoded_spots()\n\n    parsed_spots_df = filtered_spots_df[\n            [\n                \"gene_id\",\n                \"global_z\",\n                \"global_y\",\n                \"global_x\",\n                \"cell_id\",\n                \"tile_idx\",\n            ]\n    ].copy()\n    parsed_spots_df.rename(\n        columns={\n            \"global_x\": \"x\",\n            \"global_y\": \"y\",\n            \"global_z\": \"z\",\n            \"gene_id\" : \"gene\",\n            \"cell_id\" : \"cell\",\n        },\n        inplace=True,\n    )\n    parsed_spots_df[\"transcript_id\"] = pd.util.hash_pandas_object(\n        parsed_spots_df, index=False\n    )\n\n    parsed_spots_df[\"assignment_confidence\"] = 1.0\n\n    # Create spatial index for ROIs\n    roi_index = index.Index()\n    roi_map = {}  # Map index IDs to ROIs\n\n    for idx, roi in enumerate(rois):\n        # Ensure roi.coordinates contains the polygon points\n        coords = roi.coordinates()\n\n        # Insert the polygon bounds into the spatial index\n        polygon = Polygon(coords)\n        roi_index.insert(idx, polygon.bounds)  # Use polygon bounds for indexing\n        roi_map[idx] = roi\n\n    # Function to check a single point\n    def point_in_roi(row):\n        point = Point(row[\"x\"], row[\"y\"])\n        candidate_indices = list(roi_index.intersection(point.bounds))  # Search spatial index\n        for idx in candidate_indices:\n            roi = roi_map[idx]\n            match = re.search(r\"zstart_([-\\d.]+)_zend_([-\\d.]+)\", roi.name)\n            if match:\n                z_start = float(match.group(1))\n                z_end = float(match.group(2))\n                if z_start &lt;= row[\"z\"] &lt;= z_end:\n                    polygon = Polygon(roi.coordinates())\n                    if polygon.contains(point):\n                        return str(roi.name.split(\"_\")[1]) \n        return -1\n\n    # Apply optimized spatial lookup\n    parsed_spots_df[\"cell\"] = parsed_spots_df.apply(point_in_roi, axis=1)\n    parsed_spots_df = parsed_spots_df.loc[parsed_spots_df[\"cell\"] != -1]\n\n    current_global_filtered_decoded_path = (\n        self._datastore_path \n        / Path(\"all_tiles_filtered_decoded_features\")\n        / Path(\"refined_transcripts.parquet\")\n    )\n\n    self._save_to_parquet(parsed_spots_df, current_global_filtered_decoded_path)\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.run_baysor","title":"<code>run_baysor()</code>","text":"<p>Run Baysor\"</p> <p>Assumes that spots are prepped for Baysor and the Baysor path and options are set. Reformats ROIs into ImageJ style ROIs for later use.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def run_baysor(self):\n    \"\"\"Run Baysor\"\n\n    Assumes that spots are prepped for Baysor and the Baysor path and options are set.\n    Reformats ROIs into ImageJ style ROIs for later use.\n    \"\"\"\n\n    import subprocess\n\n    baysor_input_path = self._datastore_path / Path(\"all_tiles_filtered_decoded_features\") / Path(\"transcripts.parquet\")\n    baysor_output_path = self._segmentation_root_path / Path(\"baysor\")\n    baysor_output_path.mkdir(exist_ok=True)\n\n    julia_threading = r\"JULIA_NUM_THREADS=\"+str(self._julia_threads)+ \" \"\n    preview_baysor_options = r\"preview -c \" +str(self._baysor_options)\n    command = julia_threading + str(self._baysor_path) + \" \" + preview_baysor_options + \" \" +\\\n        str(baysor_input_path) + \" -o \" + str(baysor_output_path)\n\n    try:\n        result = subprocess.run(command, shell=True, check=True)\n        print(\"Baysor finished with return code:\", result.returncode)\n    except subprocess.CalledProcessError as e:\n        print(\"Baysor failed with:\", e)\n\n    # first try to run Baysor assuming that prior segmentations are present               \n    try:\n        run_baysor_options = r\"run -p -c \" +str(self._baysor_options)\n        command = julia_threading + str(self._baysor_path) + \" \" + run_baysor_options + \" \" +\\\n            str(baysor_input_path) + \" -o \" + str(baysor_output_path) + \\\n            \" --polygon-format GeometryCollectionLegacy --count-matrix-format tsv :cell_id\"\n        result = subprocess.run(command, shell=True, check=True)\n        print(\"Baysor finished with return code:\", result.returncode)\n    except subprocess.CalledProcessError:\n        # then fall back and run without prior segmentations.\n        # IMPORTANT: the .toml file has to be defined correctly for this to work!\n        try:\n            run_baysor_options = r\"run -p -c \" +str(self._baysor_options)\n            command = julia_threading + str(self._baysor_path) + \" \" + run_baysor_options + \" \" +\\\n                str(baysor_input_path) + \" -o \" + str(baysor_output_path) + \" --count-matrix-format tsv\"\n            result = subprocess.run(command, shell=True, check=True)\n            print(\"Baysor finished with return code:\", result.returncode)\n        except subprocess.CalledProcessError as e:\n            print(\"Baysor failed with:\", e)\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_coord_of_xform_px","title":"<code>save_coord_of_xform_px(of_xform_px, tile, downsampling, round, return_future=False)</code>","text":"<p>Save fidicual optical flow matrix for one round and tile.</p> <p>Parameters:</p> Name Type Description Default <code>of_xform_px</code> <code>ArrayLike</code> <p>Local fidicual optical flow matrix for one round and tile.</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>downsampling</code> <code>Sequence[float]</code> <p>Downsampling factor.</p> required <code>round</code> <code>Union[int, str]</code> <p>Round index or round id.</p> required <code>return_future</code> <code>Optional[bool]</code> <p>Return future array.</p> <code>False</code> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_coord_of_xform_px(\n    self,\n    of_xform_px: ArrayLike,\n    tile: Union[int, str],\n    downsampling: Sequence[float],\n    round: Union[int, str],\n    return_future: Optional[bool] = False,\n):\n    \"\"\"Save fidicual optical flow matrix for one round and tile.\n\n    Parameters\n    ----------\n    of_xform_px : ArrayLike\n        Local fidicual optical flow matrix for one round and tile.\n    tile : Union[int, str]\n        Tile index or tile id.\n    downsampling : Sequence[float]\n        Downsampling factor.\n    round : Union[int, str] \n        Round index or round id.\n    return_future : Optional[bool]\n        Return future array.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(round, int):\n        if round &lt; 0:\n            print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n            return None\n        else:\n            local_id = self._round_ids[round]\n    elif isinstance(round, str):\n        if round not in self._round_ids:\n            print(\"Set valid round id\")\n            return None\n        else:\n            local_id = round\n    else:\n        print(\"'round' must be integer index or string identifier\")\n        return None\n    current_local_zarr_path = str(\n        self._polyDT_root_path\n        / Path(tile_id)\n        / Path(local_id + \".zarr\")\n        / Path(\"of_xform_px\")\n    )\n    current_local_zattrs_path = str(\n        self._polyDT_root_path\n        / Path(tile_id)\n        / Path(local_id + \".zarr\")\n        / Path(\".zattrs\")\n    )\n\n    try:\n        compressor = {\n            \"id\": \"blosc\",\n            \"cname\": \"zstd\",\n            \"clevel\": 5,\n            \"shuffle\": 2,\n        }\n        spec_of = {\n            \"driver\": \"zarr\",\n            \"kvstore\": None,\n            \"metadata\": {\"compressor\": compressor},\n            \"open\": True,\n            \"assume_metadata\": False,\n            \"create\": True,\n            \"delete_existing\": False,\n        }\n        self._save_to_zarr_array(\n            of_xform_px,\n            self._get_kvstore_key(current_local_zarr_path),\n            spec_of.copy(),\n            return_future,\n        )\n        attributes = self._load_from_json(current_local_zattrs_path)\n        attributes[\"opticalflow_downsampling\"] = downsampling\n        self._save_to_json(attributes, current_local_zattrs_path)\n    except (IOError, OSError, TimeoutError):\n        print(\"Error saving optical flow transform.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_global_cellpose_segmentation_image","title":"<code>save_global_cellpose_segmentation_image(cellpose_image, downsampling, return_future=False)</code>","text":"<p>Save Cellpose max projection, downsampled segmentation image.</p> <p>Parameters:</p> Name Type Description Default <code>cellpose_image</code> <code>ArrayLike</code> <p>Cellpose max projection, downsampled segmentation image.</p> required <code>downsampling</code> <code>Sequence[float]</code> <p>Downsample factors.</p> required <code>return_future</code> <code>Optional[bool]</code> <p>Return future array.</p> <code>False</code> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_global_cellpose_segmentation_image(\n    self,\n    cellpose_image: ArrayLike,\n    downsampling: Sequence[float],\n    return_future: Optional[bool] = False,\n):\n    \"\"\"Save Cellpose max projection, downsampled segmentation image.\n\n    Parameters\n    ----------\n    cellpose_image : ArrayLike\n        Cellpose max projection, downsampled segmentation image.\n    downsampling : Sequence[float]\n        Downsample factors.\n    return_future : Optional[bool]\n        Return future array.\n    \"\"\"\n\n    current_local_zarr_path = str(\n        self._segmentation_root_path\n        / Path(\"cellpose\")\n        / Path(\"cellpose.zarr\")\n        / Path(\"masks_polyDT_iso_zyx\")\n    )\n    current_local_zattrs_path = str(\n        self._segmentation_root_path\n        / Path(\"cellpose\")\n        / Path(\"cellpose.zarr\")\n        / Path(\"masks_polyDT_iso_zyx\")\n        / Path(\".zattrs\")\n    )\n\n    attributes = {\"downsampling\": downsampling}\n\n    try:\n        self._save_to_zarr_array(\n            cellpose_image,\n            self._get_kvstore_key(current_local_zarr_path),\n            self._zarrv2_spec.copy(),\n            return_future,\n        )\n        self._save_to_json(attributes, current_local_zattrs_path)\n    except (IOError, OSError, TimeoutError):\n        print(\"Error saving Cellpose image.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_global_coord_xforms_um","title":"<code>save_global_coord_xforms_um(affine_zyx_um, origin_zyx_um, spacing_zyx_um, tile)</code>","text":"<p>Save global registration transform for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>affine_zyx_um</code> <code>ArrayLike</code> <p>Global affine registration transform for one tile.</p> required <code>origin_zyx_um</code> <code>ArrayLike</code> <p>Global origin registration transform for one tile.</p> required <code>spacing_zyx_um</code> <code>ArrayLike</code> <p>Global spacing registration transform for one tile.</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_global_coord_xforms_um(\n    self,\n    affine_zyx_um: ArrayLike,\n    origin_zyx_um: ArrayLike,\n    spacing_zyx_um: ArrayLike,\n    tile: Union[int, str],\n) -&gt; None:\n    \"\"\"Save global registration transform for one tile.\n\n    Parameters\n    ----------\n    affine_zyx_um : ArrayLike\n        Global affine registration transform for one tile.\n    origin_zyx_um : ArrayLike\n        Global origin registration transform for one tile.\n    spacing_zyx_um : ArrayLike\n        Global spacing registration transform for one tile.\n    tile : Union[int, str]\n        Tile index or tile id.\n    \"\"\"\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    try:\n        zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(self._round_ids[0] + \".zarr\")\n            / Path(\".zattrs\")\n        )\n        attributes = self._load_from_json(zattrs_path)\n        attributes[\"affine_zyx_um\"] = affine_zyx_um.tolist()\n        attributes[\"origin_zyx_um\"] = origin_zyx_um.tolist()\n        attributes[\"spacing_zyx_um\"] = spacing_zyx_um.tolist()\n        self._save_to_json(attributes, zattrs_path)\n    except (FileNotFoundError, json.JSONDecodeError) as e:\n        print(e)\n        print(\"Could not save global coordinate transforms.\")\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_global_fidicual_image","title":"<code>save_global_fidicual_image(fused_image, affine_zyx_um, origin_zyx_um, spacing_zyx_um, fusion_type='polyDT', return_future=False)</code>","text":"<p>Save downsampled, fused fidicual image.</p> <p>Parameters:</p> Name Type Description Default <code>fused_image</code> <code>ArrayLike</code> <p>Downsampled, fused fidicual image.</p> required <code>affine_zyx_um</code> <code>ArrayLike</code> <p>Global affine registration transform for fused image.</p> required <code>origin_zyx_um</code> <code>ArrayLike</code> <p>Global origin registration transform for fused image.</p> required <code>spacing_zyx_um</code> <code>ArrayLike</code> <p>Global spacing registration transform for fused image.</p> required <code>fusion_type</code> <code>str</code> <p>Type of fusion (polyDT or all_channels).</p> <code>'polyDT'</code> <code>return_future</code> <code>Optional[bool]</code> <p>Return future array.</p> <code>False</code> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_global_fidicual_image(\n    self,\n    fused_image: ArrayLike,\n    affine_zyx_um: ArrayLike,\n    origin_zyx_um: ArrayLike,\n    spacing_zyx_um: ArrayLike,\n    fusion_type: str = \"polyDT\",\n    return_future: Optional[bool] = False,\n):\n    \"\"\"Save downsampled, fused fidicual image.\n\n    Parameters\n    ----------\n    fused_image : ArrayLike\n        Downsampled, fused fidicual image.\n    affine_zyx_um : ArrayLike\n        Global affine registration transform for fused image.\n    origin_zyx_um : ArrayLike\n        Global origin registration transform for fused image.\n    spacing_zyx_um : ArrayLike\n        Global spacing registration transform for fused image.\n    fusion_type : str\n        Type of fusion (polyDT or all_channels).\n    return_future : Optional[bool]\n        Return future array.\n    \"\"\"\n\n    if fusion_type == \"polyDT\":\n        filename = \"fused_polyDT_iso_zyx\"\n    else:\n        filename = \"fused_all_channels_zyx\"\n    current_local_zarr_path = str(\n        self._fused_root_path / Path(\"fused.zarr\") / Path(filename)\n    )\n    current_local_zattrs_path = str(\n        self._fused_root_path\n        / Path(\"fused.zarr\")\n        / Path(filename)\n        / Path(\".zattrs\")\n    )\n\n    attributes = {\n        \"affine_zyx_um\": affine_zyx_um.tolist(),\n        \"origin_zyx_um\": origin_zyx_um.tolist(),\n        \"spacing_zyx_um\": spacing_zyx_um.tolist(),\n    }\n    try:\n        self._save_to_zarr_array(\n            fused_image.astype(np.uint16),\n            self._get_kvstore_key(current_local_zarr_path),\n            self._zarrv2_spec.copy(),\n            return_future,\n        )\n        self._save_to_json(attributes, current_local_zattrs_path)\n    except (IOError, OSError, TimeoutError):\n        print(\"Error saving fused image.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_global_filtered_decoded_spots","title":"<code>save_global_filtered_decoded_spots(filtered_decoded_df)</code>","text":"<p>Save all decoded and filtered spots.</p> <p>Parameters:</p> Name Type Description Default <code>filtered_decoded_df</code> <code>DataFrame</code> <p>All decoded and filtered spots.</p> required Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_global_filtered_decoded_spots(\n    self,\n    filtered_decoded_df: pd.DataFrame,\n):\n    \"\"\"Save all decoded and filtered spots.\n\n    Parameters\n    ----------\n    filtered_decoded_df : pd.DataFrame\n        All decoded and filtered spots.\n    \"\"\"\n\n    current_global_filtered_decoded_dir_path = self._datastore_path / Path(\n        \"all_tiles_filtered_decoded_features\"\n    )\n\n    if not current_global_filtered_decoded_dir_path.exists():\n        current_global_filtered_decoded_dir_path.mkdir()\n\n    current_global_filtered_decoded_path = (\n        current_global_filtered_decoded_dir_path / Path(\"decoded_features.parquet\")\n    )\n\n    self._save_to_parquet(filtered_decoded_df, current_global_filtered_decoded_path)\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_local_bit_linker","title":"<code>save_local_bit_linker(bit_linker, tile, round)</code>","text":"<p>Save readout bits linked to fidicual round for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>bit_linker</code> <code>Sequence[int]</code> <p>Readout bits linked to fidicual round for one tile.</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>round</code> <code>Union[int, str]</code> <p>Round index or round id.</p> required Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_local_bit_linker(\n    self,\n    bit_linker: Sequence[int],\n    tile: Union[int, str],\n    round: Union[int, str],\n):\n    \"\"\"Save readout bits linked to fidicual round for one tile.\n\n    Parameters\n    ----------\n    bit_linker : Sequence[int]\n        Readout bits linked to fidicual round for one tile.\n    tile : Union[int, str]\n        Tile index or tile id.\n    round : Union[int, str]\n        Round index or round id.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id.\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(round, int):\n        if round &lt; 0:\n            print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n            return None\n        else:\n            round_id = self._round_ids[round]\n    elif isinstance(round, str):\n        if round not in self._round_ids:\n            print(\"Set valid round id.\")\n            return None\n        else:\n            round_id = round\n    else:\n        print(\"'round' must be integer index or string identifier\")\n        return None\n\n    try:\n        zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(round_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n        attributes = self._load_from_json(zattrs_path)\n        attributes[\"bits\"] = bit_linker\n        self._save_to_json(attributes, zattrs_path)\n    except (FileNotFoundError, json.JSONDecodeError):\n        print(tile_id, round_id)\n        print(\"Error writing bit linker attribute.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_local_corrected_image","title":"<code>save_local_corrected_image(image, tile, gain_correction=True, hotpixel_correction=True, shading_correction=False, psf_idx=0, round=None, bit=None, return_future=False)</code>","text":"<p>Save gain and offset corrected image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ArrayLike</code> <p>Local corrected image.</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>gain_correction</code> <code>bool</code> <p>Gain correction applied (True) or not (False).</p> <code>True</code> <code>hotpixel_correction</code> <code>bool</code> <p>Hotpixel correction applied (True) or not (False).</p> <code>True</code> <code>shading_correction</code> <code>bool</code> <p>Shading correction applied (True) or not (False).</p> <code>False</code> <code>psf_idx</code> <code>int</code> <p>PSF index.</p> <code>0</code> <code>round</code> <code>Optional[Union[int, str]]</code> <p>Round index or round id.</p> <code>None</code> <code>bit</code> <code>Optional[Union[int, str]]</code> <p>Bit index or bit id.</p> <code>None</code> <code>return_future</code> <code>Optional[bool]</code> <p>Return future array.</p> <code>False</code> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code> def save_local_corrected_image(\n     self,\n     image: ArrayLike,\n     tile: Union[int, str],\n     gain_correction: bool = True,\n     hotpixel_correction: bool = True,\n     shading_correction: bool = False,\n     psf_idx: int = 0,\n     round: Optional[Union[int, str]] = None,\n     bit: Optional[Union[int, str]] = None,\n     return_future: Optional[bool] = False,\n ):\n     \"\"\"Save gain and offset corrected image.\n\n     Parameters\n     ----------\n     image : ArrayLike\n         Local corrected image.\n     tile : Union[int, str]\n         Tile index or tile id.\n     gain_correction : bool\n         Gain correction applied (True) or not (False).\n     hotpixel_correction : bool\n         Hotpixel correction applied (True) or not (False).\n     shading_correction : bool\n         Shading correction applied (True) or not (False).\n     psf_idx : int\n         PSF index.\n     round : Optional[Union[int, str]]\n         Round index or round id.\n     bit : Optional[Union[int, str]]\n         Bit index or bit id.\n     return_future : Optional[bool]\n         Return future array.\n\"\"\"\n\n     if (round is None and bit is None) or (round is not None and bit is not None):\n         print(\"Provide either 'round' or 'bit', but not both\")\n         return None\n\n     if isinstance(tile, int):\n         if tile &lt; 0 or tile &gt; self._num_tiles:\n             print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n             return None\n         else:\n             tile_id = self._tile_ids[tile]\n     elif isinstance(tile, str):\n         if tile not in self._tile_ids:\n             print(\"set valid tiled id\")\n             return None\n         else:\n             tile_id = tile\n     else:\n         print(\"'tile' must be integer index or string identifier\")\n         return None\n\n     if bit is not None:\n         if isinstance(bit, int):\n             if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                 print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                 return None\n             else:\n                 local_id = self._bit_ids[bit]\n         elif isinstance(bit, str):\n             if bit not in self._bit_ids:\n                 print(\"Set valid bit id\")\n                 return None\n             else:\n                 local_id = bit\n         else:\n             print(\"'bit' must be integer index or string identifier\")\n             return None\n         current_local_zarr_path = str(\n             self._readouts_root_path\n             / Path(tile_id)\n             / Path(local_id + \".zarr\")\n             / Path(\"corrected_data\")\n         )\n         current_local_zattrs_path = str(\n             self._readouts_root_path\n             / Path(tile_id)\n             / Path(local_id + \".zarr\")\n             / Path(\".zattrs\")\n         )\n     else:\n         if isinstance(round, int):\n             if round &lt; 0:\n                 print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                 return None\n             else:\n                 local_id = self._round_ids[round]\n         elif isinstance(round, str):\n             if round not in self._round_ids:\n                 print(\"Set valid round id\")\n                 return None\n             else:\n                 local_id = round\n         else:\n             print(\"'round' must be integer index or string identifier\")\n             return None\n         current_local_zarr_path = str(\n             self._polyDT_root_path\n             / Path(tile_id)\n             / Path(local_id + \".zarr\")\n             / Path(\"corrected_data\")\n         )\n         current_local_zattrs_path = str(\n             self._polyDT_root_path\n             / Path(tile_id)\n             / Path(local_id + \".zarr\")\n             / Path(\".zattrs\")\n         )\n\n     try:\n         self._save_to_zarr_array(\n             image,\n             self._get_kvstore_key(current_local_zarr_path),\n             self._zarrv2_spec,\n             return_future,\n         )\n         attributes = self._load_from_json(current_local_zattrs_path)\n         attributes[\"gain_correction\"] = (gain_correction,)\n         attributes[\"hotpixel_correction\"] = (hotpixel_correction,)\n         attributes[\"shading_correction\"] = (shading_correction,)\n         attributes[\"psf_idx\"] = psf_idx\n         self._save_to_json(attributes, current_local_zattrs_path)\n     except (IOError, OSError, TimeoutError) as e:\n         print(e)\n         print(\"Error saving corrected image.\")\n         return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_local_decoded_spots","title":"<code>save_local_decoded_spots(features_df, tile)</code>","text":"<p>Save decoded spots and features for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>features_df</code> <code>DataFrame</code> <p>Decoded spots and features for one tile.</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_local_decoded_spots(\n    self,\n    features_df: pd.DataFrame,\n    tile: Union[int, str],\n) -&gt; None:\n    \"\"\"Save decoded spots and features for one tile.\n\n    Parameters\n    ----------\n    features_df : pd.DataFrame\n        Decoded spots and features for one tile.\n    tile : Union[int, str]\n        Tile index or tile id.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    current_tile_features_path = self._decoded_root_path / Path(\n        tile_id + \"_decoded_features.parquet\"\n    )\n\n    self._save_to_parquet(features_df, current_tile_features_path)\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_local_registered_image","title":"<code>save_local_registered_image(registered_image, tile, deconvolution=True, round=None, bit=None, return_future=False)</code>","text":"<p>Save registered, deconvolved image.</p> <p>Parameters:</p> Name Type Description Default <code>registered_image</code> <code>ArrayLike</code> <p>Registered, deconvolved image.</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>deconvolution</code> <code>bool</code> <p>Deconvolution applied (True) or not (False).</p> <code>True</code> <code>round</code> <code>Optional[Union[int, str]]</code> <p>Round index or round id.</p> <code>None</code> <code>bit</code> <code>Optional[Union[int, str]]</code> <p>Bit index or bit id.</p> <code>None</code> <code>return_future</code> <code>Optional[bool]</code> <p>Return future array.</p> <code>False</code> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_local_registered_image(\n    self,\n    registered_image: ArrayLike,\n    tile: Union[int, str],\n    deconvolution: bool = True,\n    round: Optional[Union[int, str]] = None,\n    bit: Optional[Union[int, str]] = None,\n    return_future: Optional[bool] = False,\n):\n    \"\"\"Save registered, deconvolved image.\n\n    Parameters\n    ----------\n    registered_image : ArrayLike\n        Registered, deconvolved image.\n    tile : Union[int, str]\n        Tile index or tile id.\n    deconvolution : bool\n        Deconvolution applied (True) or not (False).\n    round : Optional[Union[int, str]]\n        Round index or round id.\n    bit : Optional[Union[int, str]]\n        Bit index or bit id.\n    return_future : Optional[bool]\n        Return future array.\n    \"\"\"\n\n    if (round is None and bit is None) or (round is not None and bit is not None):\n        print(\"Provide either 'round' or 'bit', but not both\")\n        return None\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if bit is not None:\n        if isinstance(bit, int):\n            if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                return None\n            else:\n                local_id = self._bit_ids[bit]\n        elif isinstance(bit, str):\n            if bit not in self._bit_ids:\n                print(\"Set valid bit id\")\n                return None\n            else:\n                local_id = bit\n        else:\n            print(\"'bit' must be integer index or string identifier\")\n            return None\n        current_local_zarr_path = str(\n            self._readouts_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\"registered_decon_data\")\n        )\n        current_local_zattrs_path = str(\n            self._readouts_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n    else:\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                local_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id\")\n                return None\n            else:\n                local_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n        current_local_zarr_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\"registered_decon_data\")\n        )\n        current_local_zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n\n    try:\n        spec = self._zarrv2_spec.copy()\n        spec[\"metadata\"][\"dtype\"] = \"&lt;u2\"\n        self._save_to_zarr_array(\n            registered_image,\n            self._get_kvstore_key(current_local_zarr_path),\n            spec,\n            return_future,\n        )\n        attributes = self._load_from_json(current_local_zattrs_path)\n        attributes[\"deconvolution\"] = deconvolution\n        self._save_to_json(attributes, current_local_zattrs_path)\n    except (IOError, OSError, TimeoutError):\n        print(\"Error saving corrected image.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_local_rigid_xform_xyz_px","title":"<code>save_local_rigid_xform_xyz_px(rigid_xform_xyz_px, tile, round)</code>","text":"<p>Save calculated rigid registration transform for one round and tile.</p> <p>Parameters:</p> Name Type Description Default <code>rigid_xform_xyz_px</code> <code>ArrayLike</code> <p>Local rigid registration transform for one round and tile.</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>round</code> <code>Union[int, str]</code> <p>Round index or round id.</p> required <p>Returns:</p> Name Type Description <code>rigid_xform_xyz_px</code> <code>Optional[ArrayLike]</code> <p>Local rigid registration transform for one round and tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_local_rigid_xform_xyz_px(\n    self,\n    rigid_xform_xyz_px: ArrayLike,\n    tile: Union[int, str],\n    round: Union[int, str],\n) -&gt; Optional[ArrayLike]:\n    \"\"\"Save calculated rigid registration transform for one round and tile.\n\n    Parameters\n    ----------\n    rigid_xform_xyz_px : ArrayLike\n        Local rigid registration transform for one round and tile.\n    tile : Union[int, str]\n        Tile index or tile id.\n    round : Union[int, str]\n        Round index or round id.\n\n    Returns\n    -------\n    rigid_xform_xyz_px : Optional[ArrayLike]\n        Local rigid registration transform for one round and tile.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(round, int):\n        if round &lt; 0:\n            print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n            return None\n        else:\n            round_id = self._round_ids[round]\n    elif isinstance(round, str):\n        if round not in self._round_ids:\n            print(\"Set valid round id\")\n            return None\n        else:\n            round_id = round\n    else:\n        print(\"'round' must be integer index or string identifier\")\n        return None\n    try:\n        zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(round_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n        attributes = self._load_from_json(zattrs_path)\n        attributes[\"rigid_xform_xyz_px\"] = rigid_xform_xyz_px.tolist()\n        self._save_to_json(attributes, zattrs_path)\n    except (FileNotFoundError, json.JSONDecodeError):\n        print(\"Error writing rigid transform attribute.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_local_round_linker","title":"<code>save_local_round_linker(round_linker, tile, bit)</code>","text":"<p>Save fidicual round linker attribute to readout bit for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>round_linker</code> <code>int</code> <p>Fidicual round linked to readout bit for one tile.</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>bit</code> <code>Union[int, str]</code> <p>Bit index or bit id.</p> required Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_local_round_linker(\n    self,\n    round_linker: int,\n    tile: Union[int, str],\n    bit: Union[int, str],\n):\n    \"\"\"Save fidicual round linker attribute to readout bit for one tile.\n\n    Parameters\n    ----------\n    round_linker : int\n        Fidicual round linked to readout bit for one tile.\n    tile : Union[int, str]\n        Tile index or tile id.\n    bit : Union[int, str]\n        Bit index or bit id.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id.\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(bit, int):\n        if bit &lt; 0 or bit &gt; len(self._bit_ids):\n            print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n            return None\n        else:\n            bit_id = self._bit_ids[bit]\n    elif isinstance(bit, str):\n        if bit not in self._bit_ids:\n            print(\"Set valid bit id.\")\n            return None\n        else:\n            bit_id = bit\n    else:\n        print(\"'bit' must be integer index or string identifier\")\n        return None\n\n    try:\n        zattrs_path = str(\n            self._readouts_root_path\n            / Path(tile_id)\n            / Path(bit_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n        attributes = self._load_from_json(zattrs_path)\n        attributes[\"round\"] = int(round_linker)\n        self._save_to_json(attributes, zattrs_path)\n    except (FileNotFoundError, json.JSONDecodeError):\n        print(tile_id, bit_id)\n        print(\"Error writing round linker attribute.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_local_stage_position_zyx_um","title":"<code>save_local_stage_position_zyx_um(stage_zyx_um, tile, round)</code>","text":"<p>Save tile stage position for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>stage_zyx_um</code> <code>ArrayLike</code> <p>Tile stage position for one tile.</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>round</code> <code>Union[int, str]</code> <p>Round index or round id.</p> required <p>Returns:</p> Name Type Description <code>stage_zyx_um</code> <code>Optional[ArrayLike]</code> <p>Tile stage position for one tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_local_stage_position_zyx_um(\n    self,\n    stage_zyx_um: ArrayLike,\n    tile: Union[int, str],\n    round: Union[int, str],\n):\n    \"\"\"Save tile stage position for one tile.\n\n    Parameters\n    ----------\n    stage_zyx_um : ArrayLike\n        Tile stage position for one tile.\n    tile : Union[int, str]\n        Tile index or tile id.\n    round : Union[int, str]\n        Round index or round id.\n\n    Returns\n    -------\n    stage_zyx_um : Optional[ArrayLike]\n        Tile stage position for one tile.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id.\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(round, int):\n        if round &lt; 0:\n            print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n            return None\n        else:\n            round_id = self._round_ids[round]\n    elif isinstance(round, str):\n        if round not in self._round_ids:\n            print(\"Set valid round id.\")\n            return None\n        else:\n            round_id = round\n    else:\n        print(\"'round' must be integer index or string identifier\")\n        return None\n\n    try:\n        zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(round_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n        attributes = self._load_from_json(zattrs_path)\n        attributes[\"stage_zyx_um\"] = stage_zyx_um.tolist()\n        self._save_to_json(attributes, zattrs_path)\n    except (FileNotFoundError, json.JSONDecodeError):\n        print(tile_id, round_id)\n        print(\"Error writing stage position attribute.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_local_ufish_image","title":"<code>save_local_ufish_image(ufish_image, tile, bit, return_future=False)</code>","text":"<p>Save U-FISH prediction image.</p> <p>Parameters:</p> Name Type Description Default <code>ufish_image</code> <code>ArrayLike</code> <p>U-FISH prediction image.</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>bit</code> <code>Union[int, str]</code> <p>Bit index or bit id.</p> required <code>return_future</code> <code>Optional[bool]</code> <p>Return future array.</p> <code>False</code> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_local_ufish_image(\n    self,\n    ufish_image: ArrayLike,\n    tile: Union[int, str],\n    bit: Union[int, str],\n    return_future: Optional[bool] = False,\n):\n    \"\"\"Save U-FISH prediction image.\n\n    Parameters\n    ----------\n    ufish_image : ArrayLike\n        U-FISH prediction image.\n    tile : Union[int, str]\n        Tile index or tile id.\n    bit : Union[int, str]\n        Bit index or bit id.\n    return_future : Optional[bool]\n        Return future array.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if bit is not None:\n        if isinstance(bit, int):\n            if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                return None\n            else:\n                local_id = self._bit_ids[bit]\n        elif isinstance(bit, str):\n            if bit not in self._bit_ids:\n                print(\"Set valid bit id\")\n                return None\n            else:\n                local_id = bit\n        else:\n            print(\"'bit' must be integer index or string identifier\")\n            return None\n        current_local_zarr_path = str(\n            self._readouts_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\"registered_ufish_data\")\n        )\n\n    try:\n        self._save_to_zarr_array(\n            ufish_image,\n            self._get_kvstore_key(current_local_zarr_path),\n            self._zarrv2_spec.copy(),\n            return_future,\n        )\n    except (IOError, OSError, ZarrError) as e:\n        print(e)\n        print(\"Error saving U-Fish image.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_local_ufish_spots","title":"<code>save_local_ufish_spots(spot_df, tile, bit)</code>","text":"<p>Save U-FISH localizations and features.</p> <p>Parameters:</p> Name Type Description Default <code>spot_df</code> <code>DataFrame</code> <p>U-FISH localizations and features.</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>bit</code> <code>Union[int, str]</code> <p>Bit index or bit id.</p> required Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_local_ufish_spots(\n    self,\n    spot_df: pd.DataFrame,\n    tile: Union[int, str],\n    bit: Union[int, str],\n):\n    \"\"\"Save U-FISH localizations and features.\n\n    Parameters\n    ----------\n    spot_df : pd.DataFrame\n        U-FISH localizations and features.\n    tile : Union[int, str]\n        Tile index or tile id.\n    bit : Union[int, str]\n        Bit index or bit id.\n    \"\"\"\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if isinstance(bit, int):\n        if bit &lt; 0 or bit &gt; len(self._bit_ids):\n            print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n            return None\n        else:\n            bit_id = self._bit_ids[bit]\n    elif isinstance(bit, str):\n        if bit not in self._bit_ids:\n            print(\"Set valid bit id\")\n            return None\n        else:\n            bit_id = bit\n    else:\n        print(\"'bit' must be integer index or string identifier\")\n        return None\n\n    if not (self._ufish_localizations_root_path / Path(tile_id)).exists():\n        (self._ufish_localizations_root_path / Path(tile_id)).mkdir()\n\n    current_ufish_localizations_path = (\n        self._ufish_localizations_root_path\n        / Path(tile_id)\n        / Path(bit_id + \".parquet\")\n    )\n\n    try:\n        self._save_to_parquet(spot_df, current_ufish_localizations_path)\n    except (IOError, OSError) as e:\n        print(e)\n        print(\"Error saving U-FISH localizations.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_local_wavelengths_um","title":"<code>save_local_wavelengths_um(wavelengths_um, tile, round=None, bit=None)</code>","text":"<p>Save wavelengths for fidicual OR readout bit for one tile.</p> <p>Parameters:</p> Name Type Description Default <code>wavelengths_um</code> <code>tuple[float, float]</code> <p>Wavelengths for fidicual OR readout bit for one tile.</p> required <code>tile</code> <code>Union[int, str]</code> <p>Tile index or tile id.</p> required <code>round</code> <code>Optional[Union[int, str]]</code> <p>Round index or round id.</p> <code>None</code> <code>bit</code> <code>Optional[Union[int, str]]</code> <p>Bit index or bit id.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>wavelengths_um</code> <code>Optional[tuple[float, float]]</code> <p>Wavelengths for fidicual OR readout bit for one tile.</p> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_local_wavelengths_um(\n    self,\n    wavelengths_um: tuple[float, float],\n    tile: Union[int, str],\n    round: Optional[Union[int, str]] = None,\n    bit: Optional[Union[int, str]] = None,\n) -&gt; Optional[tuple[float, float]]:\n    \"\"\"Save wavelengths for fidicual OR readout bit for one tile.\n\n    Parameters\n    ----------\n    wavelengths_um : tuple[float, float]\n        Wavelengths for fidicual OR readout bit for one tile.\n    tile : Union[int, str]\n        Tile index or tile id.\n    round : Optional[Union[int, str]]\n        Round index or round id.\n    bit : Optional[Union[int, str]]\n        Bit index or bit id.\n\n    Returns\n    -------\n    wavelengths_um : Optional[tuple[float, float]]\n        Wavelengths for fidicual OR readout bit for one tile.\n    \"\"\"\n\n    if (round is None and bit is None) or (round is not None and bit is not None):\n        print(\"Provide either 'round' or 'bit', but not both\")\n        return None\n\n    if isinstance(tile, int):\n        if tile &lt; 0 or tile &gt; self._num_tiles:\n            print(\"Set tile index &gt;=0 and &lt;=\" + str(self._num_tiles))\n            return None\n        else:\n            tile_id = self._tile_ids[tile]\n    elif isinstance(tile, str):\n        if tile not in self._tile_ids:\n            print(\"set valid tiled id\")\n            return None\n        else:\n            tile_id = tile\n    else:\n        print(\"'tile' must be integer index or string identifier\")\n        return None\n\n    if bit is not None:\n        if isinstance(bit, int):\n            if bit &lt; 0 or bit &gt; len(self._bit_ids):\n                print(\"Set bit index &gt;=0 and &lt;=\" + str(len(self._bit_ids)))\n                return None\n            else:\n                local_id = self._bit_ids[bit]\n        elif isinstance(bit, str):\n            if bit not in self._bit_ids:\n                print(\"Set valid bit id\")\n                return None\n            else:\n                local_id = bit\n        else:\n            print(\"'bit' must be integer index or string identifier\")\n            return None\n        zattrs_path = str(\n            self._readouts_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n    else:\n        if isinstance(round, int):\n            if round &lt; 0:\n                print(\"Set round index &gt;=0 and &lt;\" + str(self._num_rounds))\n                return None\n            else:\n                local_id = self._round_ids[round]\n        elif isinstance(round, str):\n            if round not in self._round_ids:\n                print(\"Set valid round id\")\n                return None\n            else:\n                local_id = round\n        else:\n            print(\"'round' must be integer index or string identifier\")\n            return None\n        zattrs_path = str(\n            self._polyDT_root_path\n            / Path(tile_id)\n            / Path(local_id + \".zarr\")\n            / Path(\".zattrs\")\n        )\n\n    try:\n        attributes = self._load_from_json(zattrs_path)\n        attributes[\"excitation_um\"] = float(wavelengths_um[0])\n        attributes[\"emission_um\"] = float(wavelengths_um[1])\n        self._save_to_json(attributes, zattrs_path)\n    except (FileNotFoundError, json.JSONDecodeError):\n        print(\"Error writing wavelength attributes.\")\n        return None\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_mtx","title":"<code>save_mtx(spots_source='')</code>","text":"<p>Save mtx file for downstream analysis. Assumes Baysor has been run.</p> <p>Parameters:</p> Name Type Description Default <code>spots_source</code> <code>str</code> <p>source of spots. \"baysor\" or \"resegmented\".</p> <code>''</code> Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_mtx(self, spots_source: str = \"\"):\n    \"\"\"Save mtx file for downstream analysis. Assumes Baysor has been run.\n\n    Parameters\n    ----------\n    spots_source: str, default \"baysor\"\n        source of spots. \"baysor\" or \"resegmented\".\n    \"\"\"\n\n    from merfish3danalysis.utils.dataio import create_mtx\n\n    if spots_source == \"baysor\":\n        spots_path = self._datastore_path / Path(\"segmentation\") / Path(\"baysor\") / Path(\"segmentation.csv\")\n    elif spots_source == \"resegmented\":\n        spots_path = (\n            self._datastore_path \n            / Path(\"all_tiles_filtered_decoded_features\")\n            / Path(\"refined_transcripts.parquet\")\n        )\n\n    mtx_output_path = self._datastore_path / Path(\"mtx_output\")\n\n    create_mtx(\n        spots_path=spots_path,\n        output_dir_path=mtx_output_path,\n    )\n</code></pre>"},{"location":"reference/classes/qi2labDataStore/#merfish3danalysis.qi2labDataStore.qi2labDataStore.save_spots_prepped_for_baysor","title":"<code>save_spots_prepped_for_baysor(prepped_for_baysor_df)</code>","text":"<p>Save spots prepped for Baysor.</p> <p>Parameters:</p> Name Type Description Default <code>prepped_for_baysor_df</code> <code>DataFrame</code> <p>Spots prepped for Baysor.</p> required Source code in <code>src/merfish3danalysis/qi2labDataStore.py</code> <pre><code>def save_spots_prepped_for_baysor(self, prepped_for_baysor_df: pd.DataFrame):\n    \"\"\"Save spots prepped for Baysor.\n\n    Parameters\n    ----------\n    prepped_for_baysor_df : pd.DataFrame\n        Spots prepped for Baysor.\n    \"\"\"\n\n    current_global_filtered_decoded_dir_path = self._datastore_path / Path(\n        \"all_tiles_filtered_decoded_features\"\n    )\n\n    if not current_global_filtered_decoded_dir_path.exists():\n        current_global_filtered_decoded_dir_path.mkdir()\n\n    current_global_filtered_decoded_path = (\n        current_global_filtered_decoded_dir_path / Path(\"transcripts.parquet\")\n    )\n\n    self._save_to_parquet(prepped_for_baysor_df, current_global_filtered_decoded_path)\n</code></pre>"},{"location":"reference/modules/dataio/","title":"Data I/O Module","text":"<p>Data I/O functions for qi2lab 3D MERFISH.</p> <p>This module provides utilities for reading and writing data in various formats used by qi2lab 3D MERFISH datasets.</p> History: <ul> <li>2024/12: Refactored repo structure.</li> <li>2024/12: Updated docstrings.</li> <li>2024/07: Removed native NDTiff reading package; integrated tifffile/zarr.                Reduced dask dependencies.</li> </ul>"},{"location":"reference/modules/dataio/#merfish3danalysis.utils.dataio.create_mtx","title":"<code>create_mtx(spots_path, output_dir_path, confidence_cutoff=0.7)</code>","text":"<p>Create a sparse matrix in MTX format from Baysor output.</p> <p>Parameters:</p> Name Type Description Default <code>spots_path</code> <code>Union[Path, str]</code> <p>Path to spots file</p> required <code>output_dir_path</code> <code>Union[Path, str]</code> <p>Path to output directory</p> required <code>confidence_cutoff</code> <code>float</code> <p>Confidence cutoff for transcript assignment</p> <code>0.7</code> Source code in <code>src/merfish3danalysis/utils/dataio.py</code> <pre><code>def create_mtx(\n    spots_path: Union[Path,str], \n    output_dir_path: Union[Path,str], \n    confidence_cutoff: float = 0.7\n):\n    \"\"\"Create a sparse matrix in MTX format from Baysor output.\n\n    Parameters\n    ----------\n    spots_path: Union[Path,str]\n        Path to spots file\n    output_dir_path: Union[Path,str]\n        Path to output directory\n    confidence_cutoff: float\n        Confidence cutoff for transcript assignment\n    \"\"\"\n\n    # Read 5 columns from transcripts Parquet file\n    if spots_path.suffix == \".csv\":\n        transcripts_df = pd.read_csv(spots_path,\n                                    usecols=[\"gene\",\n                                            \"cell\",\n                                            \"assignment_confidence\"])\n        transcripts_df['cell'] = transcripts_df['cell'].replace('', pd.NA).dropna().str.split('-').str[1]\n    else:\n        transcripts_df = pd.read_parquet(spots_path,\n                            columns=[\"gene\",\n                                    \"cell\",\n                                    \"assignment_confidence\"])\n\n\n    transcripts_df['cell'] = pd.to_numeric(transcripts_df['cell'], errors='coerce').fillna(0).astype(int)\n\n    # Find distinct set of features.\n    features = transcripts_df[\"gene\"].dropna().unique()\n\n    # Create lookup dictionary\n    feature_to_index = dict()\n    for index, val in enumerate(features):\n        feature_to_index[str(val)] = index\n\n    # Find distinct set of cells. Discard the first entry which is 0 (non-cell)\n    cells = transcripts_df[\"cell\"].dropna().unique()\n    cells = cells[cells != 0]\n\n    # Create a cells x features data frame, initialized with 0\n    matrix = pd.DataFrame(0, index=range(len(features)), columns=cells, dtype=np.int32)\n\n    # Iterate through all transcripts\n    for index, row in transcripts_df.iterrows():\n        feature = str(row['gene'])\n        cell = row['cell']\n        conf = row['assignment_confidence']\n\n        # Ignore transcript below user-specified cutoff\n        if conf &lt; confidence_cutoff:\n            continue\n\n        # If cell is not 0 at this point, it means the transcript is associated with a cell\n        if cell != 0:\n            # Increment count in feature-cell matrix\n            matrix.at[feature_to_index[feature], cell] += 1\n\n    # Call a helper function to create Seurat and Scanpy compatible MTX output\n    write_sparse_mtx(output_dir_path, matrix, cells, features)\n</code></pre>"},{"location":"reference/modules/dataio/#merfish3danalysis.utils.dataio.read_config_file","title":"<code>read_config_file(config_path)</code>","text":"<p>Read config data from csv file. </p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>Union[Path, str]</code> <p>Location of configuration file</p> required <p>Returns:</p> Name Type Description <code>dict_from_csv</code> <code>dict</code> <p>instrument configuration metadata</p> Source code in <code>src/merfish3danalysis/utils/dataio.py</code> <pre><code>def read_config_file(config_path: Union[Path,str]) -&gt; dict:\n    \"\"\"Read config data from csv file. \n\n    Parameters\n    ----------\n    config_path: Path\n        Location of configuration file\n\n    Returns\n    -------\n    dict_from_csv: dict\n        instrument configuration metadata\n    \"\"\"\n\n    dict_from_csv = pd.read_csv(config_path, header=None, index_col=0).squeeze(\"columns\").to_dict()\n\n    return dict_from_csv\n</code></pre>"},{"location":"reference/modules/dataio/#merfish3danalysis.utils.dataio.read_fluidics_program","title":"<code>read_fluidics_program(program_path)</code>","text":"<p>Read fluidics program from CSV file as pandas dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>program_path</code> <code>Union[Path, str]</code> <p>location of fluidics program</p> required <p>Returns:</p> Name Type Description <code>df_fluidics</code> <code>Dataframe</code> <p>dataframe containing fluidics program</p> Source code in <code>src/merfish3danalysis/utils/dataio.py</code> <pre><code>def read_fluidics_program(program_path: Union[Path,str]) -&gt; pd.DataFrame:\n    \"\"\"Read fluidics program from CSV file as pandas dataframe.\n\n    Parameters\n    ----------\n    program_path: Path\n        location of fluidics program\n\n    Returns\n    -------\n    df_fluidics: Dataframe\n        dataframe containing fluidics program \n    \"\"\"\n\n    try:                \n        df_fluidics = pd.read_csv(program_path)            \n        df_fluidics = df_fluidics[[\"round\", \"source\", \"time\", \"pump\"]]\n        df_fluidics.dropna(axis=0, how='any', inplace=True)\n        df_fluidics[\"round\"] = df_fluidics[\"round\"].astype(int)\n        df_fluidics[\"pump\"] = df_fluidics[\"pump\"].astype(int)\n\n        print(\"Fluidics program loaded\")\n    except Exception as e:\n        raise Exception(\"Error in loading fluidics file:\\n\", e)\n\n    return df_fluidics\n</code></pre>"},{"location":"reference/modules/dataio/#merfish3danalysis.utils.dataio.read_metadatafile","title":"<code>read_metadatafile(fname)</code>","text":"<p>Read metadata from csv file. </p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>Union[str, Path]</code> <p>filename</p> required <p>Returns:</p> Name Type Description <code>metadata</code> <code>Dict</code> <p>metadata dictionary</p> Source code in <code>src/merfish3danalysis/utils/dataio.py</code> <pre><code>def read_metadatafile(fname: Union[str,Path]) -&gt; dict:\n    \"\"\"Read metadata from csv file. \n\n    Parameters\n    ----------\n    fname: Union[str,Path]\n        filename\n\n    Returns\n    -------\n    metadata: Dict\n        metadata dictionary\n    \"\"\"\n\n    scan_data_raw_lines = []\n\n    with open(fname, \"r\") as f:\n        for line in f:\n            scan_data_raw_lines.append(line.replace(\"\\n\", \"\"))\n\n    titles = scan_data_raw_lines[0].split(\",\")\n\n    # convert values to appropriate datatypes\n    vals = scan_data_raw_lines[1].split(\",\")\n    for ii in range(len(vals)):\n        if re.fullmatch(r\"\\d+\", vals[ii]):\n            vals[ii] = int(vals[ii])\n        elif re.fullmatch(r\"\\d*.\\d+\", vals[ii]):\n            vals[ii] = float(vals[ii])\n        elif vals[ii].lower() == \"False\".lower():\n            vals[ii] = False\n        elif vals[ii].lower() == \"True\".lower():\n            vals[ii] = True\n        else:\n            # otherwise, leave as string\n            pass\n\n    # convert to dictionary\n    metadata = {}\n    for t, v in zip(titles, vals):\n        metadata[t] = v\n\n    return metadata\n</code></pre>"},{"location":"reference/modules/dataio/#merfish3danalysis.utils.dataio.return_data_zarr","title":"<code>return_data_zarr(dataset_path, ch_idx, ch_idx_offset=0)</code>","text":"<p>Return NDTIFF data as a numpy array via tiffile.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>Union[Path, str]</code> <p>pycromanager dataset object</p> required <code>ch_idx</code> <code>int</code> <p>channel index in ZarrTiffStore file</p> required <code>ch_idx_offset</code> <code>Optional[int]</code> <p>channel index offset for unused phase channels</p> <code>0</code> <p>Returns:</p> Name Type Description <code>data</code> <code>ArrayLike</code> <p>data stack</p> Source code in <code>src/merfish3danalysis/utils/dataio.py</code> <pre><code>def return_data_zarr(dataset_path: Union[Path,str],\n                     ch_idx : int,\n                     ch_idx_offset: Optional[int] = 0) -&gt; ArrayLike:\n    \"\"\"Return NDTIFF data as a numpy array via tiffile.\n\n    Parameters\n    ----------\n    dataset_path: Dataset\n        pycromanager dataset object\n    ch_idx: int\n        channel index in ZarrTiffStore file\n    ch_idx_offset: int\n        channel index offset for unused phase channels\n\n    Returns\n    -------\n    data: ArrayLike\n        data stack\n    \"\"\"\n\n    ndtiff_zarr_store = imread(dataset_path, mode='r+', aszarr=True)\n    ndtiff_zarr = zarr.open(ndtiff_zarr_store, mode='r+')\n    first_dim = str(ndtiff_zarr.attrs['_ARRAY_DIMENSIONS'][0])\n\n    if first_dim == 'C':\n        data = np.asarray(ndtiff_zarr[ch_idx-ch_idx_offset, :],dtype=np.uint16)\n    else:\n        data = np.asarray(ndtiff_zarr[:,ch_idx-ch_idx_offset,:],dtype=np.uint16)\n    del ndtiff_zarr_store, ndtiff_zarr\n\n    return np.squeeze(data)\n</code></pre>"},{"location":"reference/modules/dataio/#merfish3danalysis.utils.dataio.time_stamp","title":"<code>time_stamp()</code>","text":"<p>Generate timestamp string.</p> <p>Returns:</p> Name Type Description <code>timestamp</code> <code>str</code> <p>timestamp formatted as string</p> Source code in <code>src/merfish3danalysis/utils/dataio.py</code> <pre><code>def time_stamp():\n    \"\"\"Generate timestamp string.\n\n    Returns\n    -------\n    timestamp: str\n        timestamp formatted as string\n    \"\"\"\n\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"reference/modules/dataio/#merfish3danalysis.utils.dataio.write_metadata","title":"<code>write_metadata(data_dict, save_path)</code>","text":"<p>Write dictionary as CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>dict</code> <p>metadata dictionary</p> required <code>save_path</code> <code>Union[str, Path]</code> <p>path for file</p> required Source code in <code>src/merfish3danalysis/utils/dataio.py</code> <pre><code>def write_metadata(\n    data_dict: dict, \n    save_path: Union[str,Path]\n):\n    \"\"\"Write dictionary as CSV file.\n\n    Parameters\n    ----------\n    data_dict: dict\n        metadata dictionary\n    save_path: Union[str,Path]\n        path for file\n    \"\"\"\n\n    pd.DataFrame([data_dict]).to_csv(save_path)\n</code></pre>"},{"location":"reference/modules/dataio/#merfish3danalysis.utils.dataio.write_sparse_mtx","title":"<code>write_sparse_mtx(output_dir_path, matrix, cells, features)</code>","text":"<p>Write sparse matrix in MTX format.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir_path</code> <code>Union[Path, str]</code> <p>Path to output directory</p> required <code>matrix</code> <code>ArrayLike</code> <p>Sparse matrix</p> required <code>cells</code> <code>Sequence[str]</code> <p>Cell names</p> required <code>features</code> <code>Sequence[str]</code> <p>Feature names</p> required Source code in <code>src/merfish3danalysis/utils/dataio.py</code> <pre><code>def write_sparse_mtx(output_dir_path : Union[Path,str], \n                     matrix: ArrayLike, \n                     cells: Sequence[str], \n                     features: Sequence[str]):\n    \"\"\"Write sparse matrix in MTX format.\n\n    Parameters\n    ----------\n    output_dir_path: Union[Path,str]\n        Path to output directory\n    matrix: ArrayLike\n        Sparse matrix\n    cells: Sequence[str]\n        Cell names\n    features: Sequence[str]\n        Feature names\n    \"\"\"\n\n    sparse_mat = sparse.coo_matrix(matrix.values)\n    sio.mmwrite(str(output_dir_path / \"matrix.mtx\"), sparse_mat)\n    write_tsv(output_dir_path / \"barcodes.tsv\", [\"cell_\" + str(cell) for cell in cells])\n    write_tsv(output_dir_path / \"features.tsv\", [[str(f), str(f), \"Blank Codeword\" if str(f).startswith(\"Blank\") else \"Gene Expression\"] for f in features])\n    subprocess.run(f\"gzip -f {str(output_dir_path)}/*\", shell=True)\n</code></pre>"},{"location":"reference/modules/dataio/#merfish3danalysis.utils.dataio.write_tsv","title":"<code>write_tsv(filename, data)</code>","text":"<p>Write data to TSV file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>Union[str, Path]</code> <p>Filename</p> required <code>data</code> <code>Sequence[Union[str, Sequence[str]]]</code> <p>Data to write</p> required Source code in <code>src/merfish3danalysis/utils/dataio.py</code> <pre><code>def write_tsv(\n    filename: Union[str, Path], \n    data: Sequence[Union[str, Sequence[str]]]\n):\n    \"\"\"Write data to TSV file.\n\n    Parameters\n    ----------\n    filename: Union[str, Path]\n        Filename\n    data: Sequence[Union[str, Sequence[str]]]\n        Data to write\n    \"\"\"\n\n    with open(filename, 'w', newline='') as tsvfile:\n        writer = csv.writer(tsvfile, delimiter='\\t', lineterminator='\\n')\n        for item in data:\n            writer.writerow([item] if isinstance(item, str) else item)\n</code></pre>"},{"location":"reference/modules/imageprocessing/","title":"Image Processing Module","text":"<p>Image processing functions for qi2lab 3D MERFISH.</p> <p>This module includes various utilities for image processing, such as downsampling, padding, and chunked GPU-based deconvolution.</p> History: <ul> <li>2024/12: Refactored repo structure.</li> <li>2024/07: Added numba-accelerated downsampling, padding helper functions,                and chunked GPU deconvolution.</li> </ul>"},{"location":"reference/modules/imageprocessing/#merfish3danalysis.utils.imageprocessing.chunked_cudadecon","title":"<code>chunked_cudadecon(image, psf, image_voxel_zyx_um, psf_voxel_zyx_um, wavelength_um, na, ri, n_iters=10, background=100.0)</code>","text":"<p>Chunked and padded GPU deconvolution using pycudadecon.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ArrayLike</code> <p>3D image to deconvolve</p> required <code>psf</code> <code>ArrayLike</code> <p>3D psf</p> required <code>image_voxel_zyx_um</code> <code>Sequence[float]</code> <p>image voxel spacing in microns</p> required <code>psf_voxel_zyx_um</code> <code>Sequence[float]</code> <p>psf voxel spacing in microns</p> required <code>wavelength_um</code> <code>float</code> <p>emission wavelength in microns</p> required <code>na</code> <code>float</code> <p>numerical aperture</p> required <code>ri</code> <code>float</code> <p>immersion media refractive index</p> required <code>n_iters</code> <code>int</code> <p>number of deconvolution iterations</p> <code>10</code> <code>background</code> <code>float</code> <p>background level to subtract</p> <code>100.0</code> <p>Returns:</p> Name Type Description <code>image_decon</code> <code>ArrayLike</code> <p>deconvolved image</p> Source code in <code>src/merfish3danalysis/utils/imageprocessing.py</code> <pre><code>def chunked_cudadecon(\n    image: ArrayLike,\n    psf: ArrayLike,\n    image_voxel_zyx_um: Sequence[float],\n    psf_voxel_zyx_um: Sequence[float],\n    wavelength_um: float,\n    na: float,\n    ri: float,\n    n_iters: int = 10,\n    background: float = 100.0,\n) -&gt; ArrayLike:\n    \"\"\"Chunked and padded GPU deconvolution using pycudadecon.\n\n    Parameters\n    ----------\n    image: ArrayLike\n        3D image to deconvolve\n    psf: ArrayLike\n        3D psf\n    image_voxel_zyx_um: Sequence[float]\n        image voxel spacing in microns\n    psf_voxel_zyx_um: Sequence[float]\n        psf voxel spacing in microns\n    wavelength_um: float\n        emission wavelength in microns\n    na: float\n        numerical aperture\n    ri: float\n        immersion media refractive index\n    n_iters: int\n        number of deconvolution iterations\n    background: float\n        background level to subtract\n\n    Returns\n    -------\n    image_decon: ArrayLike\n        deconvolved image\n\n    \"\"\"\n    original_print = builtins.print\n\n    image_padded, pad_z_before, pad_z_after = pad_z(image)\n\n    image_decon_padded = np.zeros_like(image_padded)\n\n    slices = Slicer(\n        image_padded,\n        crop_size=(image_padded.shape[0], 1600, 1600),\n        overlap=(0, 32, 32),\n        batch_size=1,\n        pad=True,\n    )\n\n    for crop, source, destination in slices:\n        builtins.print= no_op\n        image_decon_padded[destination] = decon(\n            images=crop,\n            psf=psf,\n            dzpsf=float(psf_voxel_zyx_um[0]),\n            dxpsf=float(psf_voxel_zyx_um[1]),\n            dzdata=float(image_voxel_zyx_um[0]),\n            dxdata=float(image_voxel_zyx_um[1]),\n            wavelength=int(wavelength_um * 1000),\n            na=float(na),\n            nimm=float(ri),\n            n_iters=int(n_iters),\n            cleanup_otf=True,\n            napodize=15,\n            background=float(background),\n        )[source]\n        builtins.print = original_print\n\n    image_decon = remove_padding_z(image_decon_padded, pad_z_before, pad_z_after)\n\n    del image_padded, image_decon_padded\n    gc.collect()\n\n    return image_decon\n</code></pre>"},{"location":"reference/modules/imageprocessing/#merfish3danalysis.utils.imageprocessing.correct_shading","title":"<code>correct_shading(noise_map, darkfield_image, shading_image, data)</code>","text":"<p>Perform illumination shading correction.</p> <p>I_corrected = (I_raw - I_dark) / (I_bright - I_dark). Here, we assume I_bright is not normalized or background corrected.</p> <p>Parameters:</p> Name Type Description Default <code>noise_map</code> <code>ArrayLike</code> <p>darkfield image collected at long exposure time to get hot pixels</p> required <code>darkfield_image</code> <code>ArrayLike</code> <p>darkfield image collected at data's exposure time</p> required <code>shading_image</code> <code>ArrayLike</code> <p>illumination shading correction</p> required <code>data</code> <code>ArrayLike</code> <p>ND data [broadcast_dim,z,y,x]</p> required <p>Returns:</p> Name Type Description <code>data</code> <code>ArrayLike</code> <p>shading corrected data</p> Source code in <code>src/merfish3danalysis/utils/imageprocessing.py</code> <pre><code>def correct_shading(\n    noise_map: ArrayLike,\n    darkfield_image: ArrayLike, \n    shading_image: ArrayLike, \n    data: ArrayLike\n) -&gt; ArrayLike:\n    \"\"\"Perform illumination shading correction.\n\n    I_corrected = (I_raw - I_dark) / (I_bright - I_dark).\n    Here, we assume I_bright is not normalized or background corrected.\n\n    Parameters\n    ----------\n    noise_map: ArrayLike\n        darkfield image collected at long exposure time to get hot pixels\n    darkfield_image: ArrayLike\n        darkfield image collected at data's exposure time\n    shading_image: ArrayLike\n        illumination shading correction\n    data: ArrayLike\n        ND data [broadcast_dim,z,y,x]\n\n    Returns\n    -------\n    data: ArrayLike\n        shading corrected data\n    \"\"\"\n\n    darkfield_image = xp.squeeze(xp.asarray(darkfield_image, dtype=xp.float32))\n    shading_image = xp.squeeze(xp.asarray(shading_image, dtype=xp.float32))\n    noise_map = xp.squeeze(xp.asarray(shading_image, dtype=xp.float32))\n    data = xp.asarray(data, astype=xp.float32)\n\n    shading_image = replace_hot_pixels(noise_map, (shading_image - darkfield_image))\n    shading_image = xp.asarray(shading_image, dtype=xp.float32)\n    shading_image = shading_image / xp.max(shading_image, axis=(0, 1))\n\n    data = replace_hot_pixels(noise_map, (data - darkfield_image))\n    data = xp.asarray(data, dtype=xp.float32)\n\n    for z_idx in range(data.shape[0]):\n        data[z_idx, :] = data[z_idx, :] / shading_image\n\n    if CUPY_AVIALABLE:\n        data = xp.asnumpy(data).astype(np.uint16)\n        gc.collect()\n        cp.clear_memo()\n        cp._default_memory_pool.free_all_blocks()\n    else:\n        data = data.astype(np.uint16)\n\n    return data\n</code></pre>"},{"location":"reference/modules/imageprocessing/#merfish3danalysis.utils.imageprocessing.downsample_axis","title":"<code>downsample_axis(image, level=2, axis=0)</code>","text":"<p>Numba accelerated downsampling for 3D images along a specified axis.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ArrayLike</code> <p>3D image to be downsampled.</p> required <code>level</code> <code>int</code> <p>Amount of downsampling.</p> <code>2</code> <code>axis</code> <code>int</code> <p>Axis along which to downsample (0, 1, or 2).</p> <code>0</code> <p>Returns:</p> Name Type Description <code>downsampled_image</code> <code>ArrayLike</code> <p>3D downsampled image.</p> Source code in <code>src/merfish3danalysis/utils/imageprocessing.py</code> <pre><code>@njit(parallel=True)\ndef downsample_axis(\n    image: ArrayLike, \n    level: int = 2, \n    axis: int = 0\n) -&gt; ArrayLike:\n    \"\"\"Numba accelerated downsampling for 3D images along a specified axis.\n\n    Parameters\n    ----------\n    image: ArrayLike\n        3D image to be downsampled.\n    level: int\n        Amount of downsampling.\n    axis: int\n        Axis along which to downsample (0, 1, or 2).\n\n    Returns\n    -------\n    downsampled_image: ArrayLike\n        3D downsampled image.\n\n    \"\"\"\n    if axis == 0:\n        new_length = image.shape[0] // level + (1 if image.shape[0] % level != 0 else 0)\n        downsampled_image = np.zeros(\n            (new_length, image.shape[1], image.shape[2]), dtype=image.dtype\n        )\n\n        for y in prange(image.shape[1]):\n            for x in range(image.shape[2]):\n                for z in range(new_length):\n                    sum_value = 0.0\n                    count = 0\n                    for j in range(level):\n                        original_index = z * level + j\n                        if original_index &lt; image.shape[0]:\n                            sum_value += image[original_index, y, x]\n                            count += 1\n                    if count &gt; 0:\n                        downsampled_image[z, y, x] = sum_value / count\n\n    elif axis == 1:\n        new_length = image.shape[1] // level + (1 if image.shape[1] % level != 0 else 0)\n        downsampled_image = np.zeros(\n            (image.shape[0], new_length, image.shape[2]), dtype=image.dtype\n        )\n\n        for z in prange(image.shape[0]):\n            for x in range(image.shape[2]):\n                for y in range(new_length):\n                    sum_value = 0.0\n                    count = 0\n                    for j in range(level):\n                        original_index = y * level + j\n                        if original_index &lt; image.shape[1]:\n                            sum_value += image[z, original_index, x]\n                            count += 1\n                    if count &gt; 0:\n                        downsampled_image[z, y, x] = sum_value / count\n\n    elif axis == 2:\n        new_length = image.shape[2] // level + (1 if image.shape[2] % level != 0 else 0)\n        downsampled_image = np.zeros(\n            (image.shape[0], image.shape[1], new_length), dtype=image.dtype\n        )\n\n        for z in prange(image.shape[0]):\n            for y in range(image.shape[1]):\n                for x in range(new_length):\n                    sum_value = 0.0\n                    count = 0\n                    for j in range(level):\n                        original_index = x * level + j\n                        if original_index &lt; image.shape[2]:\n                            sum_value += image[z, y, original_index]\n                            count += 1\n                    if count &gt; 0:\n                        downsampled_image[z, y, x] = sum_value / count\n\n    return downsampled_image\n</code></pre>"},{"location":"reference/modules/imageprocessing/#merfish3danalysis.utils.imageprocessing.downsample_image_isotropic","title":"<code>downsample_image_isotropic(image, level=2)</code>","text":"<p>Numba accelerated isotropic downsampling</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ArrayLike</code> <p>3D image to be downsampled</p> required <code>level</code> <code>int</code> <p>isotropic downsampling level</p> <code>2</code> <p>Returns:</p> Name Type Description <code>downsampled_image</code> <code>ArrayLike</code> <p>downsampled 3D image</p> Source code in <code>src/merfish3danalysis/utils/imageprocessing.py</code> <pre><code>def downsample_image_isotropic(image: ArrayLike, level: int = 2) -&gt; ArrayLike:\n    \"\"\"Numba accelerated isotropic downsampling\n\n    Parameters\n    ----------\n    image: ArrayLike\n        3D image to be downsampled\n    level: int\n        isotropic downsampling level\n\n    Returns\n    -------\n    downsampled_image: ArrayLike\n        downsampled 3D image\n    \"\"\"\n\n    downsampled_image = downsample_axis(\n        downsample_axis(downsample_axis(image, level, 0), level, 1), level, 2\n    )\n\n    return downsampled_image\n</code></pre>"},{"location":"reference/modules/imageprocessing/#merfish3danalysis.utils.imageprocessing.next_multiple_of_32","title":"<code>next_multiple_of_32(x)</code>","text":"<p>Calculate next multiple of 32 for the given integer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>int</code> <p>value to check.</p> required <p>Returns:</p> Name Type Description <code>next_32_x</code> <code>int</code> <p>next multiple of 32 above x.</p> Source code in <code>src/merfish3danalysis/utils/imageprocessing.py</code> <pre><code>def next_multiple_of_32(x: int) -&gt; int:\n    \"\"\"Calculate next multiple of 32 for the given integer.\n\n    Parameters\n    ----------\n    x: int\n        value to check.\n\n    Returns\n    -------\n    next_32_x: int\n        next multiple of 32 above x.\n    \"\"\"\n\n    next_32_x = int(np.ceil((x + 31) / 32)) * 32\n\n    return next_32_x\n</code></pre>"},{"location":"reference/modules/imageprocessing/#merfish3danalysis.utils.imageprocessing.no_op","title":"<code>no_op(*args, **kwargs)</code>","text":"<p>Function to monkey patch print to suppress output.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>positional arguments</p> <code>()</code> <code>kwargs</code> <p>keyword arguments</p> <code>{}</code> Source code in <code>src/merfish3danalysis/utils/imageprocessing.py</code> <pre><code>def no_op(*args, **kwargs):\n    \"\"\"Function to monkey patch print to suppress output.\n\n    Parameters\n    ----------\n    args: Any\n        positional arguments\n    kwargs: Any\n        keyword arguments\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/modules/imageprocessing/#merfish3danalysis.utils.imageprocessing.pad_z","title":"<code>pad_z(image)</code>","text":"<p>Pad z-axis of 3D array by 32 (zyx order).</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ArrayLike</code> <p>3D image to pad.</p> required <p>Returns:</p> Name Type Description <code>padded_image</code> <code>ArrayLike</code> <p>padded 3D image</p> <code>pad_z_before</code> <code>int</code> <p>amount of padding at beginning</p> <code>pad_z_after</code> <code>int</code> <p>amount of padding at end</p> Source code in <code>src/merfish3danalysis/utils/imageprocessing.py</code> <pre><code>def pad_z(image: ArrayLike) -&gt; Tuple[ArrayLike, int, int]:\n    \"\"\"Pad z-axis of 3D array by 32 (zyx order).\n\n    Parameters\n    ----------\n    image: ArrayLike\n        3D image to pad.\n\n\n    Returns\n    -------\n    padded_image: ArrayLike\n        padded 3D image\n    pad_z_before: int\n        amount of padding at beginning\n    pad_z_after: int\n        amount of padding at end\n    \"\"\"\n\n    z, y, x = image.shape\n\n    new_z = next_multiple_of_32(z)\n    pad_z = new_z - z\n\n    # Distribute padding evenly on both sides\n    pad_z_before = pad_z // 2\n    pad_z_after = pad_z - pad_z_before\n\n    # Padding configuration for numpy.pad\n    pad_width = ((pad_z_before, pad_z_after), (0, 0), (0, 0))\n\n    padded_image = np.pad(image, pad_width, mode=\"reflect\")\n\n    return padded_image, pad_z_before, pad_z_after\n</code></pre>"},{"location":"reference/modules/imageprocessing/#merfish3danalysis.utils.imageprocessing.remove_padding_z","title":"<code>remove_padding_z(padded_image, pad_z_before, pad_z_after)</code>","text":"<p>Removing z-axis padding of 3D array (zyx order).</p> <p>Parameters:</p> Name Type Description Default <code>padded_image</code> <code>ArrayLike</code> <p>padded 3D image</p> required <code>pad_z_before</code> <code>int</code> <p>amount of padding at beginning</p> required <code>pad_z_after</code> <code>int</code> <p>amount of padding at end</p> required <p>Returns:</p> Name Type Description <code>image</code> <code>ArrayLike</code> <p>unpadded 3D image</p> Source code in <code>src/merfish3danalysis/utils/imageprocessing.py</code> <pre><code>def remove_padding_z(\n    padded_image: ArrayLike, \n    pad_z_before: int, \n    pad_z_after: int\n) -&gt; ArrayLike:\n    \"\"\"Removing z-axis padding of 3D array (zyx order).\n\n    Parameters\n    ----------\n    padded_image: ArrayLike\n        padded 3D image\n    pad_z_before: int\n        amount of padding at beginning\n    pad_z_after: int\n        amount of padding at end\n\n\n    Returns\n    -------\n    image: ArrayLike\n        unpadded 3D image\n    \"\"\"\n\n    image = padded_image[pad_z_before:-pad_z_after, :]\n\n    return image\n</code></pre>"},{"location":"reference/modules/imageprocessing/#merfish3danalysis.utils.imageprocessing.replace_hot_pixels","title":"<code>replace_hot_pixels(noise_map, data, threshold=375.0)</code>","text":"<p>Replace hot pixels with median values surrounding them.</p> <p>Parameters:</p> Name Type Description Default <code>noise_map</code> <code>ArrayLike</code> <p>darkfield image collected at long exposure time to get hot pixels</p> required <code>data</code> <code>ArrayLike</code> <p>ND data [broadcast_dim,z,y,x]</p> required <p>Returns:</p> Name Type Description <code>data</code> <code>ArrayLike</code> <p>hotpixel corrected data</p> Source code in <code>src/merfish3danalysis/utils/imageprocessing.py</code> <pre><code>def replace_hot_pixels(\n    noise_map: ArrayLike, \n    data: ArrayLike, \n    threshold: float = 375.0\n) -&gt; ArrayLike:\n    \"\"\"Replace hot pixels with median values surrounding them.\n\n    Parameters\n    ----------\n    noise_map: ArrayLike\n        darkfield image collected at long exposure time to get hot pixels\n    data: ArrayLike\n        ND data [broadcast_dim,z,y,x]\n\n    Returns\n    -------\n    data: ArrayLike\n        hotpixel corrected data\n    \"\"\"\n\n    data = xp.asarray(data, dtype=xp.float32)\n    noise_map = xp.asarray(noise_map, dtype=xp.float32)\n\n    # threshold darkfield_image to generate bad pixel matrix\n    hot_pixels = xp.squeeze(xp.asarray(noise_map))\n    hot_pixels[hot_pixels &lt;= threshold] = 0\n    hot_pixels[hot_pixels &gt; threshold] = 1\n    hot_pixels = hot_pixels.astype(xp.float32)\n    inverted_hot_pixels = xp.ones_like(hot_pixels) - hot_pixels.copy()\n\n    data = xp.asarray(data, dtype=xp.float32)\n    for z_idx in range(data.shape[0]):\n        median = ndimage.median_filter(data[z_idx, :, :], size=3)\n        data[z_idx, :] = inverted_hot_pixels * data[z_idx, :] + hot_pixels * median\n\n    data[data &lt; 0] = 0\n\n    if CUPY_AVIALABLE:\n        data = xp.asnumpy(data).astype(np.uint16)\n        gc.collect()\n        cp.clear_memo()\n        cp._default_memory_pool.free_all_blocks()\n    else:\n        data = data.astype(np.uint16)\n\n    return data\n</code></pre>"},{"location":"reference/modules/opmtools/","title":"OPM Tools Module","text":"<p>OPM-specific data handling tools.</p> <p>This module provides tools and utilities specifically for handling orthogonal plane microscopy (OPM) data.</p> History: <ul> <li>2024/12: Refactored repo structure.</li> <li>2024/07: Initial commit.</li> </ul>"},{"location":"reference/modules/opmtools/#merfish3danalysis.utils.opmtools.chunk_indices","title":"<code>chunk_indices(length, chunk_size)</code>","text":"<p>Calculate indices for evenly distributed chunks.</p> <p>Parameters:</p> Name Type Description Default <code>length</code> <code>int</code> <p>axis array length</p> required <code>chunk_size</code> <code>int</code> <p>size of chunks</p> required <p>Returns:</p> Name Type Description <code>indices</code> <code>Sequence[int, ...]</code> <p>chunk indices</p> Source code in <code>src/merfish3danalysis/utils/opmtools.py</code> <pre><code>def chunk_indices(length: int, chunk_size: int) -&gt; Sequence[int]:\n    \"\"\"Calculate indices for evenly distributed chunks.\n\n    Parameters\n    ----------\n    length: int\n        axis array length\n    chunk_size: int\n        size of chunks\n\n    Returns\n    -------\n    indices: Sequence[int,...]\n        chunk indices\n    \"\"\"\n\n    indices = []\n    for i in range(0, length - chunk_size, chunk_size):\n        indices.append((i, i + chunk_size))\n    if length % chunk_size != 0:\n        indices.append((length - chunk_size, length))\n    return indices\n</code></pre>"},{"location":"reference/modules/opmtools/#merfish3danalysis.utils.opmtools.chunked_orthogonal_deskew","title":"<code>chunked_orthogonal_deskew(oblique_image, psf_data, chunk_size=15000, overlap_size=550, scan_crop=700, camera_bkd=100, camera_cf=0.24, camera_qe=0.9, z_downsample_level=2, perform_decon=True, decon_iterations=10, decon_chunks=1024)</code>","text":"<p>Chunked orthogonal deskew of oblique data.</p> <p>Optionally performs deconvolution on each chunk.</p> <p>Parameters:</p> Name Type Description Default <code>oblique_image</code> <code>ArrayLike</code> <p>oblique image stack</p> required <code>psf_data</code> <code>ArrayLike</code> <p>PSF data for deconvolution</p> required <code>chunk_size</code> <code>int</code> <p>size of chunks</p> <code>15000</code> <code>overlap_size</code> <code>int</code> <p>overlap size</p> <code>550</code> <code>scan_crop</code> <code>int</code> <p>crop size</p> <code>700</code> <code>camera_bkd</code> <code>int</code> <p>camera background</p> <code>100</code> <code>camera_cf</code> <code>float</code> <p>camera conversion factor</p> <code>0.24</code> <code>camera_qe</code> <code>float</code> <p>camera quantum efficiency</p> <code>0.9</code> <code>z_downsample_level</code> <p>z downsample level</p> <code>2</code> <code>perform_decon</code> <code>bool</code> <p>perform deconvolution</p> <code>True</code> <code>decon_iterations</code> <code>int</code> <p>number of deconvolution iterations</p> <code>10</code> <code>decon_chunks</code> <code>int</code> <p>deconvolution chunk size</p> <code>1024</code> <p>Returns:</p> Name Type Description <code>deskewed_image</code> <code>ArrayLike</code> <p>deskewed image stack</p> Source code in <code>src/merfish3danalysis/utils/opmtools.py</code> <pre><code>def chunked_orthogonal_deskew(\n    oblique_image: ArrayLike,\n    psf_data: ArrayLike,\n    chunk_size: int = 15000,\n    overlap_size: int = 550,\n    scan_crop: int = 700,\n    camera_bkd: int = 100,\n    camera_cf: float = 0.24,\n    camera_qe: float = 0.9,\n    z_downsample_level=2,\n    perform_decon: bool = True,\n    decon_iterations: int = 10,\n    decon_chunks: int = 1024,\n) -&gt; ArrayLike:\n    \"\"\"Chunked orthogonal deskew of oblique data.\n\n    Optionally performs deconvolution on each chunk.\n\n    Parameters\n    ----------\n    oblique_image: ArrayLike\n        oblique image stack\n    psf_data: ArrayLike\n        PSF data for deconvolution\n    chunk_size: int\n        size of chunks\n    overlap_size: int\n        overlap size\n    scan_crop: int\n        crop size\n    camera_bkd: int\n        camera background\n    camera_cf: float\n        camera conversion factor\n    camera_qe: float\n        camera quantum efficiency\n    z_downsample_level: int\n        z downsample level\n    perform_decon: bool\n        perform deconvolution\n    decon_iterations: int\n        number of deconvolution iterations\n    decon_chunks: int\n        deconvolution chunk size\n\n    Returns\n    -------\n    deskewed_image: ArrayLike\n        deskewed image stack\n    \"\"\"\n\n    output_shape = deskew_shape_estimator(oblique_image.shape)\n    output_shape[0] = output_shape[0] // z_downsample_level\n    output_shape[1] = output_shape[1] - scan_crop\n    deskewed_image = np.zeros(output_shape, dtype=np.uint16)\n\n    if chunk_size &lt; output_shape[1]:\n        idxs = chunk_indices(output_shape[1], chunk_size)\n    else:\n        idxs = [(0, output_shape[1])]\n        overlap_size = 0\n\n    for idx in tqdm(idxs):\n        if idx[0] &gt; 0:\n            tile_px_start = idx[0] - overlap_size\n            crop_start = True\n        else:\n            tile_px_start = idx[0]\n            crop_start = False\n\n        if idx[1] &lt; output_shape[1]:\n            tile_px_end = idx[1] + overlap_size\n            crop_end = True\n        else:\n            if overlap_size == 0:\n                tile_px_end = idx[1] + scan_crop\n                crop_end = False\n            else:\n                tile_px_end = idx[1]\n                crop_end = False\n\n        xp, yp, sp_start = lab2cam(\n            oblique_image.shape[2], tile_px_start, 0, 30.0 * np.pi / 180.0\n        )\n\n        xp, yp, sp_stop = lab2cam(\n            oblique_image.shape[2], tile_px_end, 0, 30.0 * np.pi / 180.0\n        )\n        scan_px_start = np.maximum(0, np.int64(np.ceil(sp_start * (0.115 / 0.4))))\n        scan_px_stop = np.minimum(\n            oblique_image.shape[0], np.int64(np.ceil(sp_stop * (0.115 / 0.4)))\n        )\n\n        raw_data = np.array(oblique_image[scan_px_start:scan_px_stop, :]).astype(\n            np.float32\n        )\n        raw_data = raw_data - camera_bkd\n        raw_data[raw_data &lt; 0.0] = 0.0\n        raw_data = ((raw_data * camera_cf) / camera_qe).astype(np.uint16)\n\n        if perform_decon:\n            from pycudadecon import decon\n\n            data_padded, pad_z_before, pad_z_after = pad_z(raw_data)\n\n            del raw_data\n            gc.collect()\n            data_decon_padded = np.zeros_like(data_padded)\n\n            slices = Slicer(\n                data_padded,\n                crop_size=(decon_chunks, 384, 1200),\n                overlap=(32, 32, 32),\n                batch_size=1,\n                pad=True,\n            )\n\n            for crop, source, destination in slices:\n                data_decon_padded[destination] = decon(\n                    images=crop,\n                    psf=psf_data,\n                    otf_bgrd=0.0,\n                    background=0.0,\n                    dzpsf=0.400,\n                    dxpsf=0.115,\n                    dzdata=0.400,\n                    dxdata=0.115,\n                    wavelength=670,\n                    na=1.3,\n                    nimm=1.4,\n                    n_iters=decon_iterations,\n                    napodize=30,\n                    skewed_decon=True,\n                    cleanup_otf=True,\n                )[source]\n\n            data_decon = remove_padding_z(data_decon_padded, pad_z_before, pad_z_after)\n            del data_padded, data_decon_padded\n            gc.collect()\n\n            temp_deskew = deskew(data_decon).astype(np.uint16)\n\n        else:\n            temp_deskew = deskew(raw_data).astype(np.uint16)\n\n        if crop_start and crop_end:\n            crop_deskew = temp_deskew[:, overlap_size:-overlap_size, :]\n        elif crop_start:\n            crop_deskew = temp_deskew[:, overlap_size:-1, :]\n        elif crop_end:\n            crop_deskew = temp_deskew[:, 0:-overlap_size, :]\n        else:\n            crop_deskew = temp_deskew[:, 0:-scan_crop, :]\n\n        if crop_deskew.shape[1] &gt; (chunk_size):\n            diff = crop_deskew.shape[1] - (chunk_size)\n            crop_deskew = crop_deskew[:, :-diff, :]\n        elif crop_deskew.shape[1] &lt; (chunk_size):\n            diff = (chunk_size) - crop_deskew.shape[1]\n\n            if crop_start and crop_end:\n                crop_deskew = temp_deskew[:, overlap_size : -overlap_size + diff, :]\n            elif crop_start:\n                crop_deskew = temp_deskew[:, overlap_size - diff : -1, :]\n\n        if z_downsample_level &gt; 1:\n            deskewed_image[:, idx[0] : idx[1], :] = downsample_axis(\n                image=crop_deskew, level=z_downsample_level, axis=0\n            )\n        else:\n            deskewed_image[:, idx[0] : idx[1], :] = crop_deskew\n\n    del temp_deskew, oblique_image\n    gc.collect()\n\n    return deskewed_image\n</code></pre>"},{"location":"reference/modules/opmtools/#merfish3danalysis.utils.opmtools.deskew","title":"<code>deskew(data, theta=30.0, distance=0.4, pixel_size=0.115)</code>","text":"<p>Numba accelerated orthogonal interpolation for oblique data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>image stack of uniformly spaced OPM planes</p> required <code>theta</code> <code>float</code> <p>angle relative to coverslip</p> <code>30.0</code> <code>distance</code> <code>float</code> <p>step between image planes along coverslip</p> <code>0.4</code> <code>pixel_size</code> <code>float</code> <p>in-plane camera pixel size in OPM coordinates</p> <code>0.115</code> <p>Returns:</p> Name Type Description <code>output</code> <code>ArrayLike</code> <p>image stack of deskewed OPM planes on uniform grid</p> Source code in <code>src/merfish3danalysis/utils/opmtools.py</code> <pre><code>@njit(parallel=True)\ndef deskew(\n    data: ArrayLike,\n    theta: float = 30.0,\n    distance: float = 0.4,\n    pixel_size: float = 0.115,\n):\n    \"\"\"Numba accelerated orthogonal interpolation for oblique data.\n\n    Parameters\n    ----------\n    data: ArrayLike\n        image stack of uniformly spaced OPM planes\n    theta: float\n        angle relative to coverslip\n    distance: float\n        step between image planes along coverslip\n    pixel_size: float\n        in-plane camera pixel size in OPM coordinates\n\n    Returns\n    -------\n    output: ArrayLike\n        image stack of deskewed OPM planes on uniform grid\n    \"\"\"\n\n    # unwrap parameters\n    [num_images, ny, nx] = data.shape  # (pixels)\n\n    # change step size from physical space (nm) to camera space (pixels)\n    pixel_step = distance / pixel_size  # (pixels)\n\n    # calculate the number of pixels scanned during stage scan\n    scan_end = num_images * pixel_step  # (pixels)\n\n    # calculate properties for final image\n    final_ny = np.int64(\n        np.ceil(scan_end + ny * np.cos(theta * np.pi / 180))\n    )  # (pixels)\n    final_nz = np.int64(np.ceil(ny * np.sin(theta * np.pi / 180)))  # (pixels)\n    final_nx = np.int64(nx)  # (pixels)\n\n    # create final image\n    output = np.zeros(\n        (final_nz, final_ny, final_nx), dtype=np.float32\n    )  # (time, pixels,pixels,pixels - data is float32)\n\n    # precalculate trig functions for scan angle\n    tantheta = np.float32(np.tan(theta * np.pi / 180))  # (float32)\n    sintheta = np.float32(np.sin(theta * np.pi / 180))  # (float32)\n    costheta = np.float32(np.cos(theta * np.pi / 180))  # (float32)\n\n    # perform orthogonal interpolation\n\n    # loop through output z planes\n    # defined as parallel loop in numba\n    for z in prange(0, final_nz):\n        # calculate range of output y pixels to populate\n        y_range_min = np.minimum(0, np.int64(np.floor(np.float32(z) / tantheta)))\n        y_range_max = np.maximum(\n            final_ny, np.int64(np.ceil(scan_end + np.float32(z) / tantheta + 1))\n        )\n\n        # loop through final y pixels\n        # defined as parallel loop in numba\n        for y in prange(y_range_min, y_range_max):\n            # find the virtual tilted plane that intersects the interpolated plane\n            virtual_plane = y - z / tantheta\n\n            # find raw data planes that surround the virtual plane\n            plane_before = np.int64(np.floor(virtual_plane / pixel_step))\n            plane_after = np.int64(plane_before + 1)\n\n            # continue if raw data planes are within the data range\n            if (plane_before &gt;= 0) and (plane_after &lt; num_images):\n                # find distance of a point on the  interpolated plane to plane_before and plane_after\n                l_before = virtual_plane - plane_before * pixel_step\n                l_after = pixel_step - l_before\n\n                # determine location of a point along the interpolated plane\n                za = z / sintheta\n                virtual_pos_before = za + l_before * costheta\n                virtual_pos_after = za - l_after * costheta\n\n                # determine nearest data points to interpoloated point in raw data\n                pos_before = np.int64(np.floor(virtual_pos_before))\n                pos_after = np.int64(np.floor(virtual_pos_after))\n\n                # continue if within data bounds\n                if (\n                    (pos_before &gt;= 0)\n                    and (pos_after &gt;= 0)\n                    and (pos_before &lt; ny - 1)\n                    and (pos_after &lt; ny - 1)\n                ):\n                    # determine points surrounding interpolated point on the virtual plane\n                    dz_before = virtual_pos_before - pos_before\n                    dz_after = virtual_pos_after - pos_after\n\n                    # compute final image plane using orthogonal interpolation\n                    output[z, y, :] = (\n                        l_before * dz_after * data[plane_after, pos_after + 1, :]\n                        + l_before * (1 - dz_after) * data[plane_after, pos_after, :]\n                        + l_after * dz_before * data[plane_before, pos_before + 1, :]\n                        + l_after * (1 - dz_before) * data[plane_before, pos_before, :]\n                    ) / pixel_step\n\n    return output\n</code></pre>"},{"location":"reference/modules/opmtools/#merfish3danalysis.utils.opmtools.deskew_shape_estimator","title":"<code>deskew_shape_estimator(input_shape, theta=30.0, distance=0.4, pixel_size=0.115)</code>","text":"<p>Generate shape of orthogonal interpolation output array.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>Sequence[int]</code> <p>shape of oblique array</p> required <code>theta</code> <code>float</code> <p>angle relative to coverslip</p> <code>30.0</code> <code>distance</code> <code>float</code> <p>step between image planes along coverslip</p> <code>0.4</code> <code>pixel_size</code> <code>float</code> <p>in-plane camera pixel size in OPM coordinates</p> <code>0.115</code> <p>Returns:</p> Name Type Description <code>output_shape</code> <code>Sequence[int]</code> <p>shape of deskewed array</p> Source code in <code>src/merfish3danalysis/utils/opmtools.py</code> <pre><code>@njit\ndef deskew_shape_estimator(\n    input_shape: Sequence[int],\n    theta: float = 30.0,\n    distance: float = 0.4,\n    pixel_size: float = 0.115,\n):\n    \"\"\"Generate shape of orthogonal interpolation output array.\n\n    Parameters\n    ----------\n    input_shape: Sequence[int]\n        shape of oblique array\n    theta: float\n        angle relative to coverslip\n    distance: float\n        step between image planes along coverslip\n    pixel_size: float\n        in-plane camera pixel size in OPM coordinates\n\n    Returns\n    -------\n    output_shape: Sequence[int]\n        shape of deskewed array\n    \"\"\"\n\n    # change step size from physical space (nm) to camera space (pixels)\n    pixel_step = distance / pixel_size  # (pixels)\n\n    # calculate the number of pixels scanned during stage scan\n    scan_end = input_shape[0] * pixel_step  # (pixels)\n\n    # calculate properties for final image\n    final_ny = np.int64(\n        np.ceil(scan_end + input_shape[1] * np.cos(theta * np.pi / 180))\n    )  # (pixels)\n    final_nz = np.int64(\n        np.ceil(input_shape[1] * np.sin(theta * np.pi / 180))\n    )  # (pixels)\n    final_nx = np.int64(input_shape[2])\n\n    return [final_nz, final_ny, final_nx]\n</code></pre>"},{"location":"reference/modules/opmtools/#merfish3danalysis.utils.opmtools.lab2cam","title":"<code>lab2cam(x, y, z, theta=30.0 * np.pi / 180.0)</code>","text":"<p>Convert xyz coordinates to camera coordinates sytem, x', y', and stage position.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>int</code> <p>coverslip x coordinate</p> required <code>y</code> <code>int</code> <p>coverslip y coordinate</p> required <code>z</code> <code>int</code> <p>coverslip z coordinate</p> required <code>theta</code> <code>float</code> <p>OPM angle in radians</p> <code>30.0 * pi / 180.0</code> <p>Returns:</p> Name Type Description <code>xp</code> <code>int</code> <p>xp coordinate</p> <code>yp</code> <code>int</code> <p>yp coordinate</p> <code>stage_pos</code> <code>int</code> <p>distance of leading edge of camera frame from the y-axis</p> Source code in <code>src/merfish3danalysis/utils/opmtools.py</code> <pre><code>def lab2cam(\n    x: int, y: int, z: int, theta: float = 30.0 * (np.pi / 180.0)\n) -&gt; Tuple[int, int, int]:\n    \"\"\"Convert xyz coordinates to camera coordinates sytem, x', y', and stage position.\n\n    Parameters\n    ----------\n    x: int\n        coverslip x coordinate\n    y: int\n        coverslip y coordinate\n    z: int\n        coverslip z coordinate\n    theta: float\n        OPM angle in radians\n\n\n    Returns\n    -------\n    xp: int\n        xp coordinate\n    yp: int\n        yp coordinate\n    stage_pos: int\n        distance of leading edge of camera frame from the y-axis\n    \"\"\"\n\n    xp = x\n    stage_pos = y - z / np.tan(theta)\n    yp = z / np.sin(theta)\n    return xp, yp, stage_pos\n</code></pre>"},{"location":"reference/modules/registration/","title":"Image Registration Module","text":"<p>Image registration functions using cucim, scikit-image, and SimpleITK.</p> <p>This module contains functions for image registration leveraging tools like cucim, scikit-image, and SimpleITK, optimized for use with qi2lab 3D MERFISH data.</p> History: <ul> <li>2024/12: Refactored repo structure.</li> <li>2024/07: Prepared to remove all Dask usage and integrate functions                into the DataRegistration class as static methods.</li> <li>2024/01: Updated for qi2lab MERFISH file format v0.1.</li> <li>2023/07: Initial commit.</li> </ul>"},{"location":"reference/modules/registration/#merfish3danalysis.utils.registration.apply_transform","title":"<code>apply_transform(image1, image2, transform)</code>","text":"<p>Apply simpleITK transform</p> <p>Parameters:</p> Name Type Description Default <code>image1</code> <code>ArrayLike</code> <p>reference image</p> required <code>image2</code> <code>ArrayLike</code> <p>moving image</p> required <code>transform</code> <code>Transform</code> <p>simpleITK transform object</p> required <p>Returns:</p> Name Type Description <code>resampled_image</code> <code>ArrayLike</code> <p>transformed moving image</p> Source code in <code>src/merfish3danalysis/utils/registration.py</code> <pre><code>def apply_transform(image1: ArrayLike, \n                    image2: ArrayLike,\n                    transform: sitk.Transform) -&gt; sitk.Image:\n    \"\"\"\n    Apply simpleITK transform\n\n    Parameters\n    ----------\n    image1: ArrayLike\n        reference image\n    image2: ArrayLike\n        moving image\n    transform: sitk.Transform\n        simpleITK transform object\n\n    Returns\n    -------\n    resampled_image: ArrayLike\n        transformed moving image\n    \"\"\"\n\n    image1_sitk = sitk.GetImageFromArray(image1)\n    image2_sitk = sitk.GetImageFromArray(image2)\n\n    # Resample the moving image\n    resampler = sitk.ResampleImageFilter()\n    resampler.SetReferenceImage(image1_sitk)  # The fixed image is the reference\n    resampler.SetInterpolator(sitk.sitkLinear)\n    resampler.SetDefaultPixelValue(0)\n    resampler.SetTransform(transform)  # Use the transform from the registration\n\n    # Apply the transform to the moving image\n    resampled_image = resampler.Execute(image2_sitk)\n\n    del image1_sitk, image2_sitk\n    gc.collect()\n\n    return sitk.GetArrayFromImage(resampled_image).astype(np.float32)\n</code></pre>"},{"location":"reference/modules/registration/#merfish3danalysis.utils.registration.compute_optical_flow","title":"<code>compute_optical_flow(img_ref, img_trg)</code>","text":"<p>Compute the optical flow to warp a target image to a reference image.</p> <p>Parameters:</p> Name Type Description Default <code>img_ref</code> <code>ArrayLike</code> <p>reference image</p> required <code>img_trg</code> <code>ArrayLike</code> <p>moving image</p> required <p>Returns:</p> Name Type Description <code>field</code> <code>ArrayLike</code> <p>optical flow matrix</p> Source code in <code>src/merfish3danalysis/utils/registration.py</code> <pre><code>def compute_optical_flow(img_ref: ArrayLike, \n                         img_trg: ArrayLike) -&gt; ArrayLike:\n    \"\"\"\n    Compute the optical flow to warp a target image to a reference image.\n\n    Parameters\n    ----------\n    img_ref: ArrayLike\n        reference image\n    img_trg: ArrayLike\n        moving image\n\n    Returns\n    -------\n    field: ArrayLike\n        optical flow matrix\n    \"\"\"\n\n    field = deeds.registration_fields(\n                fixed=img_ref, \n                moving=img_trg, \n                alpha=1.6, \n                levels=5, \n                verbose=False,\n                )\n    field = np.array(field)\n    return field\n</code></pre>"},{"location":"reference/modules/registration/#merfish3danalysis.utils.registration.compute_rigid_transform","title":"<code>compute_rigid_transform(image1, image2, use_mask=False, downsample_factor=4.0, projection=None)</code>","text":"<p>Calculate initial translation transform using scikit-image phase cross correlation. Create simpleITK transform using shift.</p> <p>Parameters:</p> Name Type Description Default <code>image1</code> <code>ArrayLike</code> <p>reference image</p> required <code>image2</code> <code>ArrayLike</code> <p>moving image</p> required <code>use_mask</code> <code>Optional[bool]</code> <p>use mask for middle 1/3 of image</p> <code>False</code> <code>downsample_factor</code> <code>Optional[float]</code> <p>amount of downsampling applied before calling registration</p> <code>4.0</code> <code>projection</code> <code>Optional[str]</code> <p>projection method to use</p> <code>None</code> <p>Returns:</p> Name Type Description <code>transform</code> <code>simpleITK transform</code> <p>translation transform</p> <code>shift_xyz</code> <code>Sequence[float]</code> <p>xyz shifts in pixels</p> Source code in <code>src/merfish3danalysis/utils/registration.py</code> <pre><code>def compute_rigid_transform(image1: ArrayLike, \n                            image2: ArrayLike,\n                            use_mask: Optional[bool] = False,\n                            downsample_factor: Optional[float] = 4.0,\n                            projection: Optional[str] = None) -&gt; Tuple[sitk.TranslationTransform,Sequence[float]]:\n    \"\"\"\n    Calculate initial translation transform using scikit-image\n    phase cross correlation. Create simpleITK transform using shift.\n\n    Parameters\n    ----------\n    image1: ArrayLike\n        reference image\n    image2: ArrayLike\n        moving image\n    use_mask: Optional[bool], default False\n        use mask for middle 1/3 of image\n    downsample_factor: Optional[float], default 4.0\n        amount of downsampling applied before calling registration\n    projection: Optional[str], default None\n        projection method to use\n\n    Returns\n    -------\n    transform: simpleITK transform\n        translation transform\n    shift_xyz: Sequence[float]\n        xyz shifts in pixels\n    \"\"\"\n\n    if projection is not None:\n        if projection == 'z':\n            image1 = np.squeeze(np.max(image1,axis=0))\n            image2 = np.squeeze(np.max(image2,axis=0))\n        elif projection == 'y':\n            image1 = np.squeeze(np.max(image1,axis=1))\n            image2 = np.squeeze(np.max(image2,axis=1))\n\n    if projection == 'search':\n        if CUPY_AVAILABLE and CUCIM_AVAILABLE:\n            image1_cp = cp.asarray(image1)\n            ref_slice_idx = image1_cp.shape[0]//2\n            ref_slice = image1_cp[ref_slice_idx,:,:]\n            image2_cp = cp.asarray(image2)\n            ssim = []\n            for z_idx in range(image1.shape[0]):\n                ssim_slice = structural_similarity(ref_slice.astype(cp.uint16),\n                                                   image2_cp[z_idx,:].astype(cp.uint16),\n                                                   data_range=cp.max(ref_slice)-cp.min(ref_slice))\n                ssim.append(cp.asnumpy(ssim_slice))\n\n            ssim = np.array(ssim)\n            found_shift = float(ref_slice_idx - np.argmax(ssim))\n            del image1_cp, image2_cp, ssim_slice, ssim\n        else:\n            ref_slice_idx = image1.shape[0]//2\n            ref_slice = image1[ref_slice_idx,:,:]\n            ssim = []\n            for z_idx in range(image1.shape[0]):\n                ssim_slice = structural_similarity(ref_slice.astype(np.uint16),\n                                                   image2[z_idx,:].astype(np.uint16),\n                                                   data_range=np.max(ref_slice)-np.min(ref_slice))\n                ssim.append(ssim_slice)\n\n            ssim = np.array(ssim)\n            found_shift = float(ref_slice_idx - np.argmax(ssim))\n\n    else:\n        # Perform Fourier cross-correlation\n        if CUPY_AVAILABLE and CUCIM_AVAILABLE:\n            if use_mask:\n                mask = cp.zeros_like(image1)\n                if len(mask.shape) == 2:\n                    mask[image1.shape[0]//2-image1.shape[0]//6:image1.shape[0]//2+image1.shape[0]//6,\n                        image1.shape[1]//2-image1.shape[1]//6:image1.shape[1]//2+image1.shape[1]//6] = 1\n                else:\n                    mask[:,\n                        image1.shape[1]//2-image1.shape[1]//6:image1.shape[1]//2+image1.shape[1]//6,\n                        image1.shape[2]//2-image1.shape[2]//6:image1.shape[2]//2+image1.shape[2]//6] = 1\n                shift_cp, _, _ = phase_cross_correlation(reference_image=cp.asarray(image1), \n                                                        moving_image=cp.asarray(image2),\n                                                        upsample_factor=10,\n                                                        reference_mask=mask,\n                                                        disambiguate=True)\n            else:\n                shift_cp, _, _ = phase_cross_correlation(reference_image=cp.asarray(image1), \n                                                        moving_image=cp.asarray(image2),\n                                                        upsample_factor=10,\n                                                        disambiguate=True)\n            shift = cp.asnumpy(shift_cp)\n            del shift_cp\n        else:\n            if use_mask:\n                mask = np.zeros_like(image1)\n                if len(mask.shape)==1:\n                    mask[image1.shape[0]//2-image1.shape[0]//6:image1.shape[0]//2+image1.shape[0]//6,\n                        image1.shape[1]//2-image1.shape[1]//6:image1.shape[1]//2+image1.shape[1]//6] = 1\n                else:\n                    mask[:,\n                        image1.shape[1]//2-image1.shape[1]//6:image1.shape[1]//2+image1.shape[0]//6,\n                        image1.shape[2]//2-image1.shape[2]//6:image1.shape[2]//2+image1.shape[1]//6] = 1\n                shift , _, _ = phase_cross_correlation(reference_image=image1, \n                                                        moving_image=image2,\n                                                        upsample_factor=10,\n                                                        reference_mask=mask,\n                                                        disambiguate=True)\n            else:\n                shift , _, _ = phase_cross_correlation(reference_image=image1, \n                                                        moving_image=image2,\n                                                        upsample_factor=10,\n                                                        disambiguate=True)\n\n        # Convert the shift to a list of doubles\n        shift = [float(i*-1*downsample_factor) for i in shift]\n        shift_reversed = shift[::-1]\n\n    if projection is not None:\n        if projection == 'z':\n            shift_xyz = [shift_reversed[0],\n                         shift_reversed[1],\n                         0.]\n        elif projection == 'y':\n            shift_xyz = [shift_reversed[0],\n                         0.,\n                         shift_reversed[1]]\n        elif projection == 'search':\n            shift_xyz = [0.,0.,-downsample_factor*found_shift]\n    else:\n        shift_xyz = shift_reversed\n\n    # Create an affine transform with the shift from the cross-correlation\n    transform = sitk.TranslationTransform(3, shift_xyz)\n\n    gc.collect()\n    cp.get_default_memory_pool().free_all_blocks()\n\n    return transform, shift_xyz\n</code></pre>"},{"location":"reference/modules/registration/#merfish3danalysis.utils.registration.make_flow_vectors","title":"<code>make_flow_vectors(field, mask=None)</code>","text":"<p>Arrange the results of a optical flow method to display vectors in a 3D volume.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>Union[ArrayLike, list[ArrayLike]]</code> <p>Result from scikit-image or cucim ILK or TLV1 methods, or from DEEDS.</p> required <code>mask</code> <code>ArrayLike</code> <p>Boolean mask to select areas where the flow field needs to be computed.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>flow_field</code> <code>ArrayLike</code> <p>A (im_size x 2 x ndim) array indicating origin and final position of voxels.</p> Source code in <code>src/merfish3danalysis/utils/registration.py</code> <pre><code>def make_flow_vectors(field: Union[ArrayLike,list[ArrayLike]],\n                      mask: ArrayLike = None) -&gt; ArrayLike:\n    \"\"\"\n    Arrange the results of a optical flow method to display vectors in a 3D volume.\n\n    Parameters\n    ----------\n    field: ArrayLike or list[ArrayLike]\n        Result from scikit-image or cucim ILK or TLV1 methods, or from DEEDS.\n    mask: ArrayLike, default None\n        Boolean mask to select areas where the flow field needs to be computed.\n\n    Returns\n    -------\n    flow_field: ArrayLike\n        A (im_size x 2 x ndim) array indicating origin and final position of voxels.\n    \"\"\"\n\n    nz, ny, nx = field[0].shape\n\n    z_coords, y_coords, x_coords = np.meshgrid(\n        np.arange(nz), \n        np.arange(ny), \n        np.arange(nx),\n        indexing='ij',\n        )\n\n    if mask is not None:\n        origin = np.vstack([z_coords[mask], y_coords[mask], x_coords[mask]]).T\n        shift = np.vstack([field[0][mask], field[1][mask], field[2][mask]]).T \n    else:\n        origin = np.vstack([z_coords.ravel(), y_coords.ravel(), x_coords.ravel()]).T\n        shift = np.vstack([field[0].ravel(), field[1].ravel(), field[2].ravel()]).T \n\n    flow_field = np.moveaxis(np.dstack([origin, shift]), 1, 2) \n\n    return flow_field\n</code></pre>"},{"location":"reference/modules/registration/#merfish3danalysis.utils.registration.warp_coordinates","title":"<code>warp_coordinates(coordinates, tile_translation_transform, voxel_size_zyx_um, displacement_field_transform=None)</code>","text":"<p>First apply a translation transform to the coordinates, then warp them using a given displacement field.</p> <p>Parameters:</p> Name Type Description Default <code>coordinates</code> <code>ArrayLike</code> <p>List of tuples representing the coordinates. MUST be in xyz order!</p> required <code>voxel_size_zyx_um</code> <code>ArrayLike</code> <p>physical pixel spacing</p> required <code>displacement_field_transform</code> <code>Optional[Transform]</code> <p>simpleITK displacement field transform</p> <code>None</code> <p>Returns:</p> Name Type Description <code>transformed_coordinates</code> <code>ArrayLike</code> <p>List of tuples representing warped coordinates Returned in xyz order!</p> Source code in <code>src/merfish3danalysis/utils/registration.py</code> <pre><code>def warp_coordinates(coordinates: ArrayLike, \n                     tile_translation_transform: sitk.Transform,\n                     voxel_size_zyx_um: ArrayLike,\n                     displacement_field_transform: Optional[sitk.Transform] = None) -&gt; ArrayLike:\n    \"\"\"\n    First apply a translation transform to the coordinates, then warp them using a given displacement field.\n\n    Parameters\n    ----------\n    coordinates: ArrayLike\n        List of tuples representing the coordinates.\n        MUST be in xyz order!\n    voxel_size_zyx_um: ArrayLike\n        physical pixel spacing\n    displacement_field_transform: Optional[sitk DisplacementField transform], default None\n        simpleITK displacement field transform\n\n    Returns\n    -------\n    transformed_coordinates: ArrayLike\n        List of tuples representing warped coordinates\n        Returned in xyz order!\n    \"\"\"\n    voxel_size_xyz_um = voxel_size_zyx_um[::-1]   \n    coords_list = [[coord / voxel_size_xyz_um[i] for i, coord in enumerate(point)] for point in coordinates]\n\n\n    transformed_coordinates = []\n    for coord in coords_list:\n        coord_floats = tuple(map(float, coord))\n\n        # Apply the translation transform\n        translated_physical_coord = tile_translation_transform.TransformPoint(coord_floats)\n\n        # Apply the displacement field transform\n        if displacement_field_transform is not None:\n            warped_coord = displacement_field_transform.TransformPoint(translated_physical_coord)\n\n            transformed_coordinates.append(warped_coord)\n        else:\n            transformed_coordinates.append(translated_physical_coord)\n\n    transformed_physical_coords = [[coord * voxel_size_xyz_um[i] for i, coord in enumerate(point)] for point in transformed_coordinates]\n\n    return np.array(transformed_physical_coords)\n</code></pre>"}]}